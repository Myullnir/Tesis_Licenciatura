Código para generar .e a partir de los .c. Importante para el Prosem.c

"gcc -Wall -O3 -o miprog.e miprog.c -lm"

Para correr el programa el código es: "./miprog.e"

---------------------------------------------------------------------------------------

19/11/2020

Hoy voy a intentar revisar entre todos mis archivos viejos para organizarme todo lo que voy
a necesitar para empezar a trabajar. Tengo que revisar los .c, los .h, ver cómo funciona
el make file, cómo funciona el Ejecutar.sh, el Instanciar.sh.


---------------------------------------------------------------------------------------

25/11/2020

Voy a empezar a programar algunas boludeces, así ya voy de a poco recuperando el tacto con
esto. Después debería armarme un diagrama del orden de cosas para programar, así como la
jerarquía en que voy a plantear las funciones. Quizás convenga redefinir los nombres de
los archivos tipo avanzar, general y esos.

-----------------------------------------------------------------------------------------

26/11/2020

Luego de batallar con arrancar, ya estamos empezando. Me armé un diagrama de trabajo y una
lista de las cosas que tengo que poner en el programa. El diagrama lo tengo en una hoja
escrito a mano. Podría pasarlo a un power Point, ya veré si vale la pena.
Modifiqué el struct base, el llamado Red, para que tenga los elementos básicos que va a
necesitar el modelo. Tiene número de agentes, de tópicos, matriz de superposición, matriz de
adyacencia, vectores de opinión y una serie de parámetros más.
La próxima arrancamos con los elementos en inicializar. La función de Visualizar queda.
Creo que voy a borrar CCP y la otra la voy a modificar para que me arme la red. Arranquemos
con una inicialización fija, después planteamos que el sistema vaya variando los valores
de la matriz de superposición.

-----------------------------------------------------------------------------------------

27/11/2020

Bien, ya modifiqué el archivo de inicializar. Saqué el CCP que colocaba condiciones de
contorno cerradas. Esas no las voy a necesitar, porque al final mi sistema es una red
finita con N nodos, no hay condiciones de contorno que cumplir. Luego, el GenerarR lo adapte
para que me inicialice los vectores de opiniones. Lo probé, parece funcionar perfecto. Por
lo menos asigna números con fracción a todas las coordenadas correctamente. Pareciera
respetar el intervalo de valores. Le cambié el nombre al GenerarR y lo llamé GenerarOpi.

También modifiqué el programa de Visualizar, para que al final reciba un número que indique
qué vector es el que planeo mirar. Las opciones son "Vectores de Opiniones", "Matriz de
Superposición" y "Matriz de Adyacencia".

Además separé el struct original de Red en dos structs. El struct Red que contiene la matriz
de Superposición, la matriz de Adyacencia y la lista de vectores de opinión. El segundo
struct es Parametros, que contiene todos los parámetros del modelo.

Lo siguiente es armar dos funciones que me inicialicen la matriz de Superposición y la de
Adyacencia. De paso, se me acaba de ocurrir. Debería probar el numerar vectores con la 
notación usual de fila y columna "[i,j]", trabajándolos como matrices, en vez de trabajarlos
como vectores de una sola fila y muchas columnas, donde hago cosas como "[i*N+j]" para
designar los casilleros del vector.
Cuestión, cuando tenga todas las funciones de inicialización las voy a empaquetar en una
sola función, para que quede todo más prolijo. O quizás vea de encerrarlo en una pestaña.

Corte a las 9:40, me pareció que no valía matarse ahora. La dificultad ahora es cómo
escribir una matriz simétrica, considerando que yo las tengo armadas como una tira y no
como una matriz. Estoy seguro que alguna vuelta sencilla tiene que haber. Una propuesta que
tengo para los elementos diagonales y no tener que meter muchos if es armar todos los
elementos con algún criterio, (Armar una matriz random en cada instanciación no suena como
una buena idea.) y DESPUÉS reemplazar todos los elementos de la diagonal por 1.

-----------------------------------------------------------------------------------------

28/11/2020

Armé las funciones que estuve mencionando antes. Ya tengo GenerarOpi que me arma la lista
de vectores de Opinión de todos los agentes. También está GenerarAng, que me crea la mitad
de la matriz de Superposición. La otra mitad la voy a construir una vez que tenga armada
una función que me simetrice matrices. Esa función la voy a meter dentro del conjunto de
funciones generales, podría servir en un futuro. La situación es la misma con GenerarAdy. Me
genera la mitad de la matriz de Adyacencia, la otra la voy a construir por simetría.

Estuve probando el Visualizar. Algo raro le está pasando, pero empecemos aclarando que me
visualiza perfecto las matrices de Adyacencia  y Superposición. Pero por alguna razón viene
tirando un error con la matriz de Opiniones. Creo que el error es puramente en la 
visualización, que no tiene nada que ver con lo que realmente hay en la matriz. Me percaté
de esto para empezar porque observaba que los elementos de la última fila de esta matriz
cuando formaba el sistema con seis agentes eran todos cero. Algo estadísticamente improbable.
Cuestión, que dependiendo de la cantidad de filas los elementos de las últimas filas se ven
peor o mejor. Noté que si cambiaba a muchas filas y columnas, de repente todo se emparejaba.
También descarté que el problema sea el casteo o el armado del Vector. El casteo no es porque
pasé el vector a Double, que es la salida del Random, y sigue funcionando mal. El armado del
vector no es porque probé ponerle enteros, y sigue dando el error de que al visualizarlo me
muestra algo que no tiene sentido, como ceros en las últimas filas o números absurdamente
largos e inexplicables. Y estos números largos eran siempre los mismos, no cambiaban en un solo
número. Algo raro pasa ahi y todavía no descubrí qué es lo que está pasando. Para comprobar que
el vector está bien rellenado podrías armar una sumatoria con sus números y ver que den algo
correcto.

Ahora voy a armar una carpeta en Github para cargar todo esto. Yo diría que mañana vayas
haciendo pruebas con lo de sumatorias para verificar que el vector se llena de manera correcta.
Luego, empezá a revisar la visualización para ver qué clase de error puede estar teniendo. Al
final del día igual no es vital, pero bueno. Visualizar las redes puede facilitar el encontrar
errores.

-------------------------------------------------------------------------------------------

30/11/2020

Encontré el error. Al final el problema efectivamente estaba en la visualización. La culpa era
mía, obviamente. Yo le calculé mal el movimiento a lo largo de las filas. En pocas palabras, el
sujeto estaba yendo a mirar en posiciones de memoria que no le pertenecían al vector. Eso hizo
que visualizara cosas raras. Porque eran espacios de memoria que estaban ocupados por cosas
raras. De paso, le agregué a la visualización que se muestren sólo dos decimales después de
la coma. Para que sea más sencillo de ver.

Lo pensé mejor, voy a escribir el código de simetrización dentro de las mismas funciones de 
Generar*. Queda para el futuro armar funciones que tomen punteros, me midan el tamaño de los
punteros y desde ahí me simetricen la matriz. Igual, el proceso de simetrización es una línea.
Genial, ya se me simetrizan ambas matrices.

Bueno, armé dos funciones en los archivos de avanzar. Pero todavía no puedo decir que funcionen
bien. Probé hacer las cuentas. En el caso de un nodo sin conexión a nada, dan bárbaro las
cuentas. En el caso de que se conecte con alguien, ya no da tan bien. Todavía no descubro
porqué.

También me ocurrió algo inentendible para mi. En el struct me surgió un error incomprensible.
Agregué el diferencial de tiempo al struct, y lo puse junto a los otros float, por una cuestión
de orden, para agrupar los mismos tipos de datos. Cuestión que por alguna razón que no
comprendo, la posición de este elemento en el struct jodía todo el programa. Pero al cambiarlo
de lugar y llevarlo al fondo del struct, todo se organizaba y funcionaba bárbaro. Cosa que no
tiene sentido, porque el struct llama a sus elementos por nombre, no por posición. El orden
no debería importar. No sé porqué, pero alto error raro.

-------------------------------------------------------------------------------------------

05/12/2020

Por lo que leí en internet, no hay una forma en C de poder leer el tamaño del array al cual 
apunta el puntero. Pero lo que podemos hacer es un truco, y esto implica un cambio en la
forma de programar de ahora para SIEMPRE. Lo que vamos a hacer es que de ahora en más, al 
principio de cada vector los primeros dos números van a estar reservados para el tamaño
de la matriz. (Si es un vector, igual lo consideraré como matriz). De esta manera, siempre
podré tener acceso al tamaño de un array. Esto va a ser un cambio que voy a implementar mañana.

Ahora me voy a poner a jugar con los structs y cosas en Prosem.c. Quiero ver si puedo armar
funciones a las que les llegan punteros y de esa manera en vez de tener que pasar todo el struct,
le paso sólo el puntero correcto. ¿Porqué querrías volver a trabajar sólo con punteros, en vez de
usar el struct que es mejor? El tema es que los nombres de los elementos de los structs va
variando con cada trabajo, entonces una función como la de simetrizar matrices no puede trabajar
con recibir el struct, porque lo que yo voy a querer es que funcione para cada matriz que 
le paso independiente del nombre, pero al usar el struct tengo que especificar el nombre
del objeto, y eso me obligaría a crear una matriz de Simetrización para cada nuevo trabajo.

Veamos si podemos hacer algo con esto. Ok, probando en Prosem, cambiar el orden de los atributos
del struct Parametros no parece crear problemas. Lo cual es lo razonable, porque no los llamé
por ningún orden, los llamé por nombre. El motivo del error en la función main original 
sigue sin ser descubierto, mucho menos solucionado.

Ahora probemos a partir de una matriz inicial el pasarle a una función de visualización básica
el que me visualice mi matriz a partir de un puntero. Armé una visualización tranca haciendo
uso de esa idea de poner el tamaño de la matriz al principio. Está bueno el truco este.
Alguien mencionó también dejar un número para identificar el tipo de dato. Esa para el
futuro queda. No quiero sumar números al pedo, para no volverme loco en la definición
de valores y cosas.

Armé la función de visualización. Efectivamente, si defino el input como un puntero, y le
paso el puntero del struct entonces lo trabaja perfectamente. En este caso le pasé el
puntero red.Ady, es decir el que apunta a la matriz de Adyacencia de mi sistema, y me
lo graficó correctamente.

------------------------------------------------------------------------------------------

06/12/2020

Hoy lo que voy a hacer es un reformateo a todo incorporando nuevas prácticas de programación.
Estas consisten en:

.) Agregar una letra al nombre de una variable que defina el tipo de variable que es.
Esto lo separo del nombre de la función con un guión bajo. Los espacios en los
nombres los voy a separar con guiones bajos también. Es importante recordar
entonces que las primeras letras SIEMPRE indican el tipo de variable.

.) Voy a dejar espacios libres entre secciones de código, separando la parte de
inicializar, desarrollo, registro de datos y demás. También vamos a poner títulos.

.) En las primeras dos coordenadas de cada vector voy a colocar el tamaño del vector.
Siendo la primer coordenada las Filas y la segunda las Columnas.

Si sobra tiempo, armaré una función que haga un RK4. Aunque primero necesito una que
calcule correctamente el campo que define mi ecuación diferencial. Y estas funciones
necesitan ser probadas primero en Prosem para poder hacer las cuentas a mano y ver
que efectivamente hacen lo esperado.

##### Ya reformatee general.c y general.h ##########

##### Ya reformatee inicializar.c e inicializar.h ##########

Llegué al punto donde la función hace lo que ya hacía antes de empezar a formattear todo.
Mañana seguiré con esto. La idea es entonces primero armar la función de la ecuación
dinámica. Probarla que calcule correctamente. Hecho esto, pasamos a armar un RK4.
Es decir, mañana arrancamos laburando con el Prosem. Hecho esto, ya vamos a poder
poner el programa a evolucionar. Lo que va a quedar es registrar los datos y pasarlos
a Python.

Vamos a terminar el día de hoy subiendo todo lo correspondiente a Github. Vale
aclarar, los archivos avanzar los dejé afuera de la carpeta src, porque sino
al compilar el make all me los intenta levantar, y a esos todavía no los corregí.
Más que nada porque las funciones de la dinámica no están terminadas.

-----------------------------------------------------------------------------------------

07/12/2020

Corregí los nombres una vez más, porque ahora a los punteros les agregué la letra p.

Además, ya armé la función Din1 y la probé en el archivo Prosem. Ya lo testee y funciona,
tanto en el caso en que el sujeto no conecta con nadie como en el caso en que conecta con
alguien, la cuenta que hace es correcta. Estaría bueno conseguir un programa que me permita
hacer las cuentas con mayor facilidad, porque la verdad es una paja hacerlo en Geogebra.
Creo que voy a hacer las cuentas en Octave de ahora en más. Va a ser mejor, me puede hacer
los productos de matrices y eso va a ser mucho más rápido.

Voy a subir todo a Github ahora.

------------------------------------------------------------------------------------------

08/12/2020

Ya armé las funciones Din1 y Din2. Confío en que funcionan perfecto porque revisé las
cuentas yo mismo usando Octave. También, como para dejar constancia del test realizado
es que guardé una imagen que se llama Cáculos Din2. Es una imagen que tiene los
resultados obtenidos por el programa en C y las cuentas hechas con el Octave, donde
se puede ver que las pendientes calculadas en Octave coinciden totalmente con las 
calculadas en C. Para mayor claridad paso a describir qué son las matrices definidas
en Octave:

- Opi es la matriz de vectores de opinión transpuesto. El motivo de hacerlo transpuesto
es porque de esta manera el producto con la matriz Ang me daría en cada elemento la
suma de las opiniones de un agente en cada tópico ponderada por la superposición con
el tópico en cuestión. El hecho de usar la matriz de forma diferente a como lo hago en
C no implica ningún error de cuentas. No hay que olvidar que en C yo no tengo Matrices,
solamente tengo vectores muy largos. Por tanto el producto entre matrices y vectores
siempre lo termino armando yo. Y eso ya lo había revisado de que estaba bien.

- Ang es la matriz de superposición de tópicos. Por simplicidad, tiene unos en la
diagonal y 0.5 afuera.

- Sup es el producto de las matrices Ang y Opi. Como dije, contiene en cada elemento
la sumatoria de las opiniones de un cierto agente ponderadas según la superposición
de cada opinión con el tópico en consideración.

- Pendiente es el nómbre del resultado total del miembro de la derecha de la
ecuación diferencial.

- K es el parámetro homónimo de la ecuación diferencial. Representa la influencia social.

- alfa es el parámetro homónimo de la ecuación diferencial. Representa la
controversialidad de un tópico.

Ahora el chiste va a ser implementar el RK4. Estuve pensando en que el RK4 debería
tomar aparte de los structs, el puntero sobre el cual va a trabajar, el puntero
a la función que define el campo de mi problema, (En este caso sería Din2),
y algo interesante sería que tuviera un puntero propio dentro de la función.
Ese puntero debería guardar información sobre el estado inicial del puntero 
con el que voy a trabajar. Esa información va a ser necesaria durante el trabajo
RK4, pero terminada la función, se lo puede liberar y listo.

----------------------------------------------------------------------------------------

14/12/2020

Vamos a armar una función que realice el proceso de RK4. Mi idea es escribir una
función que sea bastante general. Para eso voy a tomar como referencia una versión
que encontré por internet. Todavía no tengo bien claras las ideas de cómo voy a
hacer funcionar la versión del RK4. Mi idea es que tome los structs y le pase
eso a la función. Por eso es que voy a estar dentro del RK4 trabajando la Matriz
de Opinión, definiéndola y redefiniéndola muchas veces.
Estaba con la duda de cómo guardar o manejar los datos correctamente, pero al
final llegué a la conclusión de que lo mejor sería tener vectores, definidos
por punteros como siempre, que me guarden los valores de las pendientes que
calculo para cada una de mis variables. Luego al final, luego de haber calculado
cada una de las pendientes, entonces sí uso todas las pendientes para calcular
el siguiente paso temporal del sistema.
Lo interesante para esto es que me di cuenta que iba a tener que hacer cada
cálculo de pendientes en orden, entonces tenía que escribir las 4 sumatorias.
Pero para simplificar la escritura de todas las sumatorias, se me ocurrió
usar un array de punteros. Por eso decidí probar el cómo usarlo en Prosem.
Ahí lo hice funcionar para que me genere la visualización de mi red.
Funciona bárbaro. Tengo que recordar no borrar eso. Voy a tener que
armar un protocolo de guardado de estas cosas. Mañana o en la semana,
lo próximo a hacer es ya implementar esto. Con esto ya básicamente tendría
armada la función del RK4. La paja va a ser comprobar que las cuentas están
bien hechas. Voy a estar un rato con el Octave mirando eso de seguro.

------------------------------------------------------------------------------------------

18/12/2020

Hoy avancé con la construcción de la función RK4. Hay varias cosas para 
discutir al respecto. Pero en términos de lo que quiero que haga, ya la
armé. Todavía no la probé ni revisé errores de tipeo, así que todavía
queda mucho hasta que esté lista. Además, todadvía estoy indeciso sobre
el formato de la función. Debería analizarlo un poco más, como para estar
seguro de que la función es perfectamente generalizable. Cosa que sería
de mucha utilidad.

Por otra parte, rearmé la función de visualizar, de manera de que sólo
tome un puntero de entrada y de eso haga la visualización. Para eso,
hice tres funciones, una que toma enteros, otra que toma floats y una
que toma doubles. Mi idea sería armar una función visualizar Global
que las encierre, que reciba un char que elija la función correcta 
y de ahí visualice. Aunque ahora que lo pienso, eso seguiría
teniendo el mismo problema de que no puedo pasar el vector que 
quiero a la función. ¿Habrá una forma de solucionar esto en el
futuro?

Cree una segunda función que es Duplicar, esa función me calcula
las pendientes de una ecuación diferencial que definí para probar
el RK4. La ecuación diferencial sería x' = 2x.

Volviendo al RK4, estoy empezando a usar más los arrays. También
introduje un array de chars, que eso también es nuevo para mi.
Hay muchas cosas, como los arrays, el meter un for para el free,
o el visualizar que podrían funcionar mal. Va a ver que mirar
todo eso la próxima vez. La próxima entonces hay que hacer
las cuentas y ver que el RK4 esté calculando bien.
Y después, repensar un poco la forma de que sea lo más general
posible. Quizás hacer unos gráficos escritos.

Pregunta importante, ¿Cómo guardar las distintas versiones de
cosas probadas en el Prosem? Podríamos hacer un archivo
llamado Archivo.c. Ahí voy guardando las cosas con descripción.
Así no me van ocupando espacio innecesario en mi archivo Prosem.
Me parece una idea genial.

------------------------------------------------------------------------------------------

21/12/2020

Estuve haciendo las pruebas del RK4 para hacerlo funcionar.
Increíblemente, pero en cuanto puse la función funco bastante
fácil, no tuvo tantos errores como esperaba.

Cuestión, hice las pruebas con ecuaciones diferenciales lineales
en las cuales la variación de mi variable estaba sujeta a
un múltiplo de su valor actual. Es decir, usé las ecuaciones
x'=2x y x'=5x. En la primera anoté los k manualmente y
comprobé que los cálculos estuvieran dando bien los k y
el valor final del sistema. Además, como conozco la solución
a estas ecuaciones puedo comprobar que tan lejos está lo
calculado con respecto a lo real. Elegí los valores de
dt y la constante que definen mi ecuación diferencial
de manera que evolucionar temporalmente un paso a mi sistema
sea equivalente a tomar los datos iniciales y multiplicarlos
por e en ambas ecuaciones.

Guardé una imagen que se llama cálculos RK4 en la cual se
puede ver a la derecha los resultados obtenidos por el 
programa RK4. Luego a la izquierda uso el Octave para
manualmente hacer los cálculos que el programa RK4 debería
estar haciendo. En este caso estoy evaluando el sistema
en el cual la ecuación diferencial es x'=5x. A continuación
está el paso a paso de las cuentas que hice:

.) Calculo k1 como los valores del sistema en tiempo t
multiplicados por 5.
.) Calculo k2 como los valores del sistema en tiempo t
sumados a k1 por dt/2 y todo eso multiplicado por 5
.) Calculo k3 como los valores del sistema en tiempo t
sumados a k2 por dt/2 y todo eso multiplicado por 5
.) Calculo k4 como los valores del sistema en tiempo t
sumados a k3 por dt y todo eso multiplicado por 5
.) Calculo el final como los valores del sistema en
tiempo t más el producto de dt sobre 6 con 
k1 más dos k2 más dos k3 más k4.
.) Corroboro el resultado de tomar los valores del
sistema a tiempo t y multiplicarlos por e.

Finalmente cabe notar que hay una diferencia en la 
segunda cifra decimal, lo cual marcaría que debería tomar
un avance temporal más chico del que tomé. Pero en conclusión,
el programa funciona perfecto. Me encanta. Sólo queda
pensar si se la puede hacer un poco más genérica y listo.

Ahora voy a incorporar esto al programa principal. Todavía no
terminé esto. Mañana lo primero que tengo que hacer es corregir
los struct que aparecen en todo el código y reemplazarlos
por los s_cosas. Con el tiempo se me ocurrirán formas de
hacer que eso funcione más fácil

----------------------------------------------------------------------------------

22/12/2020

Ya implementé el RK4 en el programa, funca bárbaro. Hice
las anotaciones sobre el bloque que creo es crucial para
la generalización de esta función en futuros trabajos.

Hice pruebas, el programa funciona muy bien, no tira
errores ni nada. Por ahora lo único que probé a variar
son los valores de K y los de la matriz de Superposición.
Los resultados son los siguientes:

.) Si los valores de la matriz de Superposición son todos
positivos, entonces todos los signos de los tópicos se
alinean.
.) Si los signos de los valores de la matriz de Superposición
se alternan, entonces lo que me ocurre es que los signos
de las columnas se alternan igualmente.
.) Si reduzco el valor de K a cero, el sistema tiende a cero.
.) Si aumento el valor de K el sistema al evolucionar
simplemente tiende a crecer.
.) Con un valor de Tiempo de integración = 20, el sistema
con K=0 logra llegar a un estado donde todos los agentes
tienen opinión nula en todos los tópicos.

Ahora lo que voy a hacer es cortar acá, mostrarle esto
a Pablo mañana y de ahí decidir qué vamos a hacer.
Si yo estuviera por mi cuenta y tuviera que decidir,
¿Cuál sería mi siguiente paso?

Razonablemente, yo diría que por ahora lo que tengo
entonces son sólo dos fotos, la inicial y la final
y que este fue un análisis muy cualitativo. Para
poder empezar a hacer un análisis más cuantitativo
voy a necesitar ver lo que ocurre durante el proceso.

Por tanto los siguientes objetivos serían:
1) Implementar una forma de tomar registro de
los datos para poder guardar mis vectores de
opinión.
2) Armar un programa para cargar estos datos a
Python.
3) Hacer un análisis de los datos. Para esto hay
que pensar de qué forma deberían graficarse los datos.
Se me ocurre que el caso de K=0 se pueden graficar
curvas de variación del valor promedio de la opinión
en cada tópico y ver cómo eso cae a cero.
4) Podría graficar el caso de K != 0 considerando la
direccionalidad de los vectores. Sería una forma
cualitativa pero un poco mejor que lo que tengo
ahora de visualizar la evolución del sistema. Me armo
una especie de Histograma donde voy contando cuántas
personas apuntan en cierta dirección, y las
direcciones las defino en base al signo del tópico.
Eso me ayudaría a ver cómo se van moviendo las personas
de una opinión a otra y si la condición inicial del
sistema afecta mucho al resultado final.
5) Otra opción viable es armar un scatter de puntos
de colores, donde cada color representa una opinión
y luego ir acercando los colores cuyas opiniones son
más similares.

-------------------------------------------------------------------------------------

23/12/2020

Hoy tuvimos la reunión con Pablo. Le mostré los resultados
obtenidos. Me aconsejó lo siguiente:

1) Guardar todos los gráficos que vaya realizando en un
powerpoint. De esta manera vamos mejorando la forma de
presentar datos, así como también mi hablidad de
explicarlos y cosas.
2) Arrancar únicamente con 2 tópicos, así podemos
concentrarnos en replicar los resultados del paper de
Baumann.
3) Que en cada iteración el sistema recorrar a los N agentes
de manera ordenada, pero elija una pareja de manera aleatoria
(Esto va a resultar más complicado de lo que parece)

Por si acaso, antes de meterme con esto, resolver primero
el tema de registrar los datos y levantarlos con Python.

------------------------------------------------------------------

27/12/2020

En la carpeta de Aprendiendo C ayer armé un programa de Bash
para compilar archivos y correrlos. Eso lo importé acá,
se llama Compilar.sh. La idea es que para correrlo yo
uso en la línea de comando "./Compilar.sh Nombre_archivo".
El Nombre_archivo va sin .e, eso el programa de Compilar
lo agrega solo. Luego, el programa compila y te da la 
opción de elegir correr o no el .e. La idea es primero
revisar los errores. Si no salta ningún error, apretás
enter y se corre el programa. Si hay errores que corregir,
apretás una letra y después enter y no se corre el programa.

Hoy voy a encargarme de organizar el tema de que haya registros
de los datos. Lo que voy a hacer es armar archivos que guarden
la siguiente info:

.) Primera línea es la matriz de adyacencia.
.) Segunda línea es la matriz de Superposición
.) Tercera línea y en adelante es la matriz de Opiniones en cada
paso temporal.

Si tengo tiempo, me pondré a hacer lo que me propone Pablo sobre
variar la forma en que se elijen personas para realizar las
interacciones.

Se me está ocurriendo armar una función que reciba el puntero
al archivo y una matriz, y que copie toda la matriz. Como para
hacerlo más prolijo, por más que son sólo dos líneas.

Ok, ya logré usar los comandos para armar un archivo en donde
guardar los datos. Lo raro de esto es que por alguna razón no puedo
ponerle la extensión txt al archivo. No comprendo porqué. Igual eso
es lo de menos. Puedo fácilmente usar línea de comando o armar un
programa de Bash que se encargue de eso.

También me armé dos programas, uno para vectores doubles y otro
para vectores ints que me printea los datos. Teniendo esto, y
siendo que lo que me propuso Pablo es algo más trabajoso de hacer,
podría ponerme a trabajar en hacer un archivo de Bash que mueva 
mis archivos de mi carpeta actual a la de Programas Python.
Esto no está resultando tan fácil. Tampoco es tan importante,
lo dejaré para otro momento.

Más tarde, subiré las cosas a Github y armaré un registro de lo
que hice hoy. ¿Qué voy a hacer mañana?

.) Tengo que armar una carpeta en Drive y compartírsela a Pablo
y Sebastián.
.) Ahora que tengo un programa que guarda registro de mis datos,
voy a incorporar esto al main,
.) Cuando tenga al programa armando mis datos, voy a armar un
código de Bash que me los mueva todos a la carpeta de Datos
en la carpeta de Programas de Python.
.) Una vez que tenga los datos armados, voy a empezar a trabajar
en Python para ir viendo de levantar los datos y graficarlos.
.) Hecho todo esto, voy a empezar a trabajar en lo que dijo Pablo
sobre modificar la forma en que el sistema se itera.

--------------------------------------------------------------------------

28/12/2020

Ya agregué las partes de registro de datos al programa, funciona
bien. El tiempo que tarda me preocupa, voy a tener que trabajar
en armar las tablas previamente. Ahora voy a armar un archivo
que me mueva los datos.

Ya armé el programa que me mueve los archivos de datos a la carpeta
de Programas de Python. Despues quizás arme un archivo Bash que
lo levante, así de una al terminar de hacer todo me los mueva.
Posiblemente haga eso en el instanciar. Tipo, al final del for
muevo todo.

Antes de trabajar en Python, voy a anotar algo respecto a lo
que dijo Pablo sobre modificar la forma de iteración del
sistema. Entiendo que lo que Pablo dijo fue permitir que
todos los usuarios interactuen con todos, es decir una
matriz de adyacencia con ceros en la diagonal y unos afuera.
Se me ocurre que podría agregar un segundo agente en el struct
de la red. Luego debería supongo quitar a la matriz de adyacencia
de la ecuación y hacer que la cuenta se efectúe mirando a un
agente particular.
Ahora que lo pienso, el Din1 está calculando las tanh para TODOS
los agentes, incluso los no conectados. Eso es una pérdida de
tiempo abismal e innecesario. Podría meter un if en la función
Din1, pero creo que no sería lo óptimo. Lo mejor que se me
ocurre es armar un array de punteros a funciones. Que ese
array en la posición cero tenga una función que returnea un
cero, y en la posición 1 tenga a la función Din1. Ese array
lo creo en la función Din2, y eso sería mejor que tener un
if, porque no hay comparación involucrada. Y debería correr
más rápido el programa. Me gusta la idea. Ahora sí,
trabajemos en Python.

Ya logré armar un programa que levanta los datos de los archivos
y me genera un gráfico de la evolución de las opiniones. Lo que
grafica es el promedio del cuadrado de la opinión de cada uno 
de los agentes, y esto lo hace para todos los tópicos.
Este gráfico no sé si tendría mucho sentido para graficar
sistemas con K distinto de cero. Pero lo importante es que
el esqueleto está, modificar esto no sería tan difícil.

Lo interesante va a ser ver cómo graficar el caso de K distinto
de cero. Se me ocurre que puedo simplemente marcar con puntos o
líneas muy chicas las trayectorias en un plano 2D. Habrá que ver
cómo sale eso.

¿Con qué debería seguir? Ahora definitivamente es una buena idea
seguir con las propuestas de Pablo. Estos son los objetivos:

.) Modificar la iteración de manera de que cada agente se conecte
con otro agente aleatorio. (¿Cómo verifico que esto se cumpla bien?)
.) Modificar la función Din1 para que trabaje con un sujeto en vez
de con todos. (Asegurarse de guardar esta versión del programa
en algún lado, porque esta era la original. Esta era la buena.
Quizás simplemente lo deje comentado y listo.)
.) Corregir esa pila de cálculos innecesarios de las tanh que
después se mueren. Probar primero con un if. Y si eso no
es un gran progreso, probar la idea del puntero a dos funciones.
.) Armar las tablas de cálculo previo de las tanh y las funciones
de interpolación, para reducir el tiempo de cálculo de las tanh.
.) A futuro, cuando haya leído sobre el uso de archivos en el Tutorial
de C, ver de la posibilidad de armar una tabla una sola vez, y luego
cargarla, para deshacerme definitivamente de esa pérdida de tiempo.
.) Modificar el instanciar.sh para empezar a realizar muchas corridas
juntas. Agregarle al fondo el mover.sh. Eso es genial, me encanta.
.) Cuando esté consiguiendo cantidades ingentes de datos en la carpeta
de programas, ahí voy a tener que empezar a generalizar el funcionamiento
de mi programa de Python.

Mañana tengo que subir las nuevas versiones de programas de C y
de Python al Github.

----------------------------------------------------------------------------------

29/12/2020

Hoy lo que hice fue ver toda una sección de loops y decisiones en
el tutorial de C de AticleWorld. Aprendí un poco sobre los breaks
y el goto. Y sobre el continue.

-----------------------------------------------------------------------------------

30/12/2020

Ok, estoy pensando en hacer lo de la modificación al programa para que 
la iteración cumpla con la idea que propuso Pablo de agregar un elemento
random a la forma en que evoluciona el sistema. Se me ocurre que es
un buen momento para modificar el RK4 de manera de que se vuelva
incluso más general. Voy a sacar las iteraciones en agentes y tópicos
por fuera de la función.

Lo primero que debo hacer para hacer esto es guardar el programa que
tengo hasta ahora, para no perder nada de lo que ya funciona. Para
esto voy a empezar a usar la carpeta de Programas, ahí me voy a guardar
los archivos de esta versión del programa. Aunque voy a tener que usar
una carpeta, ya que esta versión que tengo del programa no va a funcionar
con la nueva versión del RK4, por lo que también va a necesitar su
correspondiente archivo general y demás.

Mañana arranco con las modificaciones. Si me pongo las pilas, ya mañana
tengo modificado el programa para que la iteración no sea ordenada.

------------------------------------------------------------------------------------

31/12/2020

Una idea para mis funciones es estandárizar el nombre que le doy a las
variables que registran el número de filas y columnas. A veces las llamo
i_C,i_CF, a veces i_columnas,i_filas, a veces i_Filas, i_Columnas. Tiene
que ser en lo posible siempre el mismo. Después voy a revisar todas mis
funciones y cambiarlo por el más sencillo, i_C e i_F.

Ahí probé la función RK4 modificada. Ahora efectivamente evoluciona a
una sola variable, y calcula todo correctamente. Ahora sólo queda
decidir detalles de si necesito o no armar un vector que me guarde
los datos extra para ir haciendo la comparación y poder decidir cuando
cortar el programa.

Igual lo siguiente a esto ahora es implementar esto en el main y ver
que efectivamente esto itera para cada sujeto. Seguramente armando 
una función Iteración que englobe al RK4.

Lo último que estaba haciendo es modificar la función Din1. El objetivo
es justamente modificar la selección del segundo agente de interacción
para volverla random. Estaba trabajando en eso armando una función
que toma un número y un rango de valores, crea un vector con todos
los números en el rango menos el que tomo como input. Luego genera
un número random menor al tamaño del vector, va a la casilla elegida
del vector y toma ese número. La función la estaba armando para el
caso de números ints.

Voy a tomar la siguiente regla de trabajo. Es cierto que soy muy
celoso con el tiempo de trabajo de mis funciones, pero voy a dejar
estos temas de optimización de código para más adelante. Por si acaso,
si pensás que te vas a ir olvidando de esto en el futuro, hagamos un
archivo llamado optimización donde guardemos las ideas de qué cosas
necesitan optimizarse, así cuando tengamos tiempo con el código y
no sepamos qué hacer, podemos invertir el tiempo en mejorar el código
y otro día resolvemos esos problemas grandes.

De paso, mañana cuando arranques con el programa de nuevo, acordate
de comentar la función esta de generar números aleatorios, sino te va 
a tirar altos errores.

-----------------------------------------------------------

01/01/2021

Modifiqué la función Din1 para que no tome al segundo agente como un
input, sino que el segundo agente sea un elemento dentro de los structs.
Luego armé al segundo agente dentro del struct de la red, que es i_agente2.

A la función Din2 le saqué el for que hacía que revisara todos los
agentes con los cuales el primer agente estaba conectado. Esto es para que
las interacciones sean únicamente de a pares.

Agregué en la iteración del main los elementos necesarios para definir
el segundo agente y para que el sistema itere en los tópicos y los agentes.

A la función RK4 le saqué todo el mambo de vectores k donde calculaba
el k para cada elemento del vector de opiniones y evolucionaba
todo el sistema junto, y lo reemplacé por un sólo vector el
cual contenía los valores de k0 hasta k4. El k0 es un extra
que agregué yo para generalizar el cálculo de los k y poder
hacerlos todos en un sólo loop, en vez de tener que escribir
4 acciones separadas para calcularlos.

Todavía no pasé esto al programa principal, todo esto fue en
el programa de pruebas.Así que todavía tengo eso por hacer.


Algo interesante que me llamó la atención. El sistema funcionando
de esta manera converge a un punto en el cual las opiniones
de los tópicos valen K o -K. El motivo de esto es que hay dos
términos en conflicto, el primero de -x y el segundo que tiene
K * tanh(...). El tema es el siguiente, como ahora no estoy
haciendo una sumatoria en todos los sujetos con los cuales mi
agente principal tiene conexiones, sino que hago interacciones
de a pares, mi segundo término es básicamente K*tanh(...).
Con un alfa suficientemente alto, la tanh cappea rápido,
por lo que se puede aproximar por 1. Esto me deja la
ecuación diferencial como -x+K*sgn(x). Esto es lo que lleva
a que el punto de equilibrio para todas las opiniones se
encuentre en el valor de K o -K. Antes esto no resultaba
tan evidente porque cada sujeto tenía una cantidad de
conexiones aleatoria, entonces me quedaba en el segundo
término K * sum(vecinos). Entonces cada agente tendía a
un valor que dependía de la cantidad de vecinos. 
De nuevo, como ahora las interacciones son de a pares,
todos tienen 1 solo vecino y por eso es que esto ahora
resulta tan evidente. Está copado poder observarlo.

Ya pasé todo al archivo principal, ahora funciona con este
sistema de interacción por pares. ¿Qué debería hacer mañana?

Los objetivos de cosas por hacer que charle con Pablo se 
pueden resumir en:

.) Optimizar más el código. Todo eso está anotado en el
archivo de Optimización.txt.
.) Modificar el archivo de instanciación de Bash para que
pueda ir tomando valores como input en la línea de comando
y que pueda correr muchas veces. Eso no es una gran
urgencia.
.) Mejorar el archivo de Python para que cuando esté con
grandes cantidades de datos poder levantarlas y graficarlas
mejor. Eso surgirá cuando esté graficando.

En principio, tengo una buena parte del trabajo ya hecho,
y eso que hoy es primero de Enero. Recordá que el gran
objetivo propuesto por Pablo es arrancar Febrero con los
dos gráficos de Baumann ya realizados. El de evolución
del sistema y el de los resultados según combinaciones 
de Delta y alfa.

Para cumplir esto también es importante revisar si Baumann
tiene algún código especial que use para hacer los gráficos.

Mañana voy a tomármelo libre. Lo que voy a hacer es armar
un archivo donde me anoto los objetivos Día a día.
Como para no tener que andar moviéndome para arriba cada vez
que necesito ir recordando qué hacer.

Algo que me estoy olvidando es de que debería armar un
mecanismo de corte automático del programa cuando el sistema
llega a un estado estable. Para esto, la idea es calcular como
un error cuadrático y ver la diferencia entre el vector antes
de iterar y después de iterar. Eso es lo siguiente a
implementar. Entonces mañana arranco con eso, y después lo
siguiente ya es empezar a trabajar en Python para armar
los gráficos. Ese debería ser el orden, creo yo. Bueno,
entonces el domingo arrancamos con ese mecanismo de corte.

Lo último que voy a hacer hoy es pasar todo el código del
archivo de pruebas al archivo Archivo.c, como para tener
guardada esta versión. Y debería subir todo a Github.

----------------------------------------------------------------

03/01/2021

Estuve intentando armar algo en el archivo Archivo.c de
manera de que me permita moverme rápido entre elementos.
No encontré un método útil. Después seguiré buscando,
y sino simplemente armaré una carpeta y pondré todo en
archivos separados.

Ahí corregí mis dos funciones, la de Norma y la de
Deltax, que ahora la llamo Delta_Vec_d.
Funcionan perfecto, Delta_Vec_d toma dos vectores
double, los resta y guarda la diferencia en la
tercer coordenada. Norma_d por su parte calcula
correctamente la norma del vector double que le
pases. Así que con esto estoy totalmente en condiciones
de hacer la modificación al programa del main para
que pueda registrar una condición de corte automático.
Igual, antes de armar la condición en el programa, me 
conviene primero calcular datos para un cálculo bien grande,
verificar cuanto tarda el sistema en llegar a un punto estable
y cuánto vale el error cuadrático en ese punto, como para tener
una noción de cuanto debe ser el error cuadrático. Además
ese error va a ser una función del tamaño de mi sistema, así
que estaría bueno ver si podemos hallar alguna relación
entre esas dos magnitudes.

Ahí coloqué esto en el main. Estoy teniendo un error de lo más
raro. Por alguna razón el programa no funciona, ya ni me itera
el vector de opiniones correctamente. No entiendo porqué
sólo está moviendo la primer coordenada, y además la mueve mal.
Es un total sin sentido esto.

---------------------------------------------------------------------

07/01/2021

Ahí empecé a retocar el archivo main que no funcionaba. Saqué los
elementos nuevos en los structs, saqué la función iteración del
main y todo rastro de los nuevos elementos del main. Con nuevos
elementos me refiero a los nuevos punteros y cosas que
puse en los struct que iba a necesitar.

El problema al parecer corría por el programa de iteración,
que lo reemplacé directamente por su código. Lo único que
realmente cambié es que en la iteración del for, en vez de
hacer un PRE incremento, lo pasé a un POST incremento.
Pareciera que esa fue la solución. Ojalá haya sido eso. 
Ahora voy a guardar el programa tal cual está, y voy a empezar
a agregar los elementos nuevos al struct. Espero que esta
vez no se rompa de nuevo.

Bien, ya incorporé todo y no explotó. No sé si funca,
pero por lo menos no explota. Quiero decir, llega al
resultado que llegaba antes, pero todavía no tengo
forma de saber que las funciones están haciendo su
trabajo correctamente. Por eso ahora lo que voy a hacer
es probar en Prosem si puedo abrir dos archivos en simultáneo
y escribir en ambos a la vez.

Ahí lo probé, funciona perfecto. De seguro el error estaba
en que quería usar el mismo puntero para dos archivos
distintos o algo así. Esto está buenísimo. Voy a además
empezar a usar esta notación de ahora en adelante, se siente
re natural.

Perfecto, el archivo ya funciona. Lo próximo que tengo que hacer
ahora es levantar los datos con Python, ver que está todo bien, y
empezar a hacer corridas masivas. Eso no suena muy bien. Me refiero
a correr el programa muchas veces, para muchos valores de N distintos.

Mi objetivo es ver si existe alguna variación de los valores de 
error cuadrático al ir variando el tamaño de la red. Considerando
que al ir aumentando el número de sujetos aumenta también el
número de coordenadas que tienen que restarse y luego sumar
su error cuadrático, yo pensaría que tiene que aumentar. Pero
también es cierto que el error cuadrático que estoy calculando
está normalizado según el número de agentes y tópicos, por tanto
NO debería cambiar con el cambio de número de agentes y tópicos.
Esto es muy importante para poder definir un criterio de corte del
sistema.

¿Que es lo próximo que tengo que hacer entonces?:

.) Levantar los datos en Python y ver que las funciones estén
trabajando correctamente.
.) Modificar el main para que tome datos desde la línea de comando.
.) Modificar el Bash para que las instanciaciones corran el programa
y luego manden todos los datos a la carpeta de C correspondiente.
.) Si queda tiempo y ganas, investigar un poco sobre cómo cambiar el
nombre de mis archivos para agregarles una extensión txt.

Voy a subir todo esto a GitHub

----------------------------------------------------------------------------------

08/01/2021

Ahí subí todo a Github hoy a la mañana. Por accidente apreté enter
y subí los archivos del Source sin un nombre correcto, ni hablar de
que no le puse descripción. En ese archivo se puede ver que ya cambié
todos los i_filas (y las columnas también), y les puse un único nombre
en todas las funciones, para que sea siempre i_F e i_C.
Eso era una tarea de unificación necesaria.

Ahora voy a empezar a trabajar en Python para armar una estructura que
levante los datos y los grafique. Mi objetivo ahora es mirar un poco
cuando se produce el corte en mis datos y ver si puedo armar alguna relación
entre la variable de corte, K y N.

El programa de Python me parece que está bastante prolijo y ahora está
armado para levantar los datos de los archivos y graficarlos. En particular
lo que hace la función es tomar las opiniones de todos los sujetos en
una iteración respecto a un tópico, elevarlas al cuadrado, sumarlas y
normalizarlas según la cantidad de agentes.
Además, por una cuestión de que la leyenda del gráfico no me tape todo,
antes de graficar los datos lo que hago es sumar estos valores al cuadrado
y sumados de todas las opiniones de un tópico sobre todos los tópicos.
De esa manera lo que estoy graficando es una sumatoria de las opiniones
sobre TODOS los tópicos. Repito que esto es para una simplificación de la
graficación por un tema de que al final del día yo espero que todas las
opiniones se estabilicen en algún valor, por lo que como todas convergen
a algo, la suma también va a converger. En este momento perder rastro de
los valores de cada opinión no es un problema porque lo que yo quiero medir
es más o menos cuantos pasos requiere el sistema para alcanzar un punto
de convergencia. Al parecer con 7500 paso le sobra a todos los sistemas.

Es más, cuando lo corra en C para tener bastantes más datos, lo que
podría hacer es reducir el tiempo de corrida a 10000 paso.

Bueno, ¿qué voy a hacer la próxima vez que agarre esto?:

.) Tengo que terminar de armar el cargado de datos de los errores cuadráticos.
Necesito eso para determinar correctamente un punto de corte.
.) Modificar el main para que tome datos desde la línea de comando.
.) Modificar el Bash para que las instanciaciones corran el programa
y luego manden todos los datos a la carpeta de C correspondiente.
.) Si queda tiempo y ganas, investigar un poco sobre cómo cambiar el
nombre de mis archivos para agregarles una extensión txt.

-------------------------------------------------------------------------------

09/01/2021

Ya armé el programa para que tome los archivos de errores y lo grafique.
Fue bastante fácil, total fue hacer una copia casi del código
para graficar los datos de opiniones. Lo raro es lo que estoy viendo en
los datos. Al parecer el sistema primero cae a un primer mínimo local.
Esto se encuentra cerca de las 2500 iteraciones. Hasta este punto, la
evolución parece bastante caótica porque la suma de diferencias cuadráticas
oscila fuertemente. Luego de eso el ruido se reduce de forma considerable
y las diferencias cuadradas comienzan a aumentar hasta llegar a un pico
claramente definido, apenas antes de las 5000 iteraciones. De ahí descienden
las diferencias a cero de manera progresiva y sin ningún ruido.
Es como si el sistema alcanzara un punto crítico en el cual se
decide un claro estado "ganador" que se encarga de absorber a todos los
demás estados. Me lo imagino como que se habían formado ciertas comunidades,
(Cosa rara porque no hay ningún mecanismo de formación de comunidades),
y luego el gran cambio se da porque una comunidad colapsa y es absorbida
por la otra.
Lo interesante es que jamás se me hubiera ocurrido ver esto tomando en cuenta
los gráficos de opiniones. Ese gráfico, con las pérdidas de información
que implica, pareciera sugerir un progresivo avance hacia una dirección
que ya en la iteración 5000 pareciera haberse prácticamente establecido.

Esto es raro, pero bueno, ahora voy a tomar MUCHAS mediciones más
y ver qué pasa. Mi mayor miedo es qué hacer con la leyenda. 
Quizás la ignore. Eso lo pensaré una vez que tenga mis datos.

Primero tengo entonces que modificar el archivo Bash.
También tengo que armar una carpeta separada para guardar mis nuevos
datos. No quiero borrar los que tengo hasta haberlo charlado con
Pablo.

De paso, te comento que por alguna razón no habías realmente
normalizado los errores. Sólo para que lo sepas. Igual quizás
también convenga multiplicarlo por algún número grande,
sólo para que no se vea tan chico.

Armé un nuevo archivo en Prosem para probar un poco el tema de
mandarle números al programa a través de input por línea de comando.
En el mismo archivo está explicado detalladamente cómo funcionan las
variables argc y argv, así como la función strtol que voy a usar para
el programa. Ya mañana lo puedo mandar a correr y sacar muchos
datos.

Lo que sí voy a necesitar es armar una segunda carpeta donde
guardar esos datos. Y modificar el instanciar.sh para que
mande todos los archivos a esa carpeta y no a la ya existente.

----------------------------------------------------------------------------------

10/01/2021

Cosas importantes de hoy. Ya modifiqué el instanciar para que
corra muchos N y muchos K. Es más, ya lo hice correr, así que
eso ya está hecho. Además el programa levanta todos los archivos
con nombre "Datos_*" y los mueve a la carpeta de Datos Corte
dentro de la carpeta de programas Python.

Por otra parte, al parecer el antivirus era lo que impedía
que el comando mv cambie los nombres de los archivos. Así que
modifiqué eso ahora para que los nombres de los archivos movidos
se escriban como txt.
Ahora que lo pienso, si bien puedo hacerlo, eso no necesariamente
es bueno. Creo que voy a seguir trabajando sin eso, porque al final
me va a generar más problemas.

También tuve que poner una nueva función, strtof, que me permite
convertir números en floats. Bah, yo digo que tuve, pero en
realidad lo hice por si acaso porque el número K está definido
como un float, pero en realidad no sé si me hubiera tirado un
error el usar el strtol.

Ahora debería levantar los datos con Python y ver qué me da. 
Eso lo haré mañana seguro.

--------------------------------------------------------------------------------

14/01/2021

Estoy anotando esto con un día de retraso.

Lo que voy a hacer hoy es tomar los archivos de Python y
modificarlos de manera de que me hagan un gráfico de Opiniones
y uno de Errores por cada valor de agentes N, y que en cada
gráfico entren todos los valores de K considerado. La idea es
separar un poco todas las curvas para poder entender mejor qué
está pasando.

Bien, ya logré hacer esto. De paso intenté generalizar el sistema
haciendo que primero reconozca los valores entre los cuales se
mueve N y de ahí el código grafica todo correctamente.
Ahora sería interesante lograr que me guarde los gráficos automáticamente,
así puedo mirarlos mejor y más cómodo en la pc en vez de mirarlos
en el notebook de Python.

Bueno, eso fue sencillo, lo realicé con la función plt.savefig().
Listo, ahora voy a subir estos datos al powerpoint de la tesis,
voy a subir todas las imágenes asociadas a una carpeta
que se llame: "Datos Corte", porque son los datos que
voy a usar para definir el mecanismo de corte.

Ya está todo subido y correctamente documentado en el archivo
de powerpoint de la tesis.

¿Qué es lo que sigue?

1) Debería empezar a pensar un criterio de corte y ponerlo
a prueba con los datos. La idea sería que el sistema me grafique
una línea vertical en mis plots y con eso poder comprobar cuándo
el sistema cortaría las iteraciones según ese criterio.
2) Para lo anterior estaría bueno comprobar si realmente
los valores de ErrCuad están debidamente normalizados o si
varían con los valores de N y K. Lo ideal sería que estén
correctamente normalizados. Eso me garantizaría que puedo
tomar algún valor arbitrario como criterio definitivo
y estar tranquilo de que nunca va a pasar que el sistema
nunca llegue a cumplirlo.
3) Hacer el gráfico de las opiniones para el caso de N=5 
que tiene ese error con el pico raro. Así lo podemos analizar
más en detalle.
4) ¿Podría normalizar los errores cuadráticos usando 
el máximo del error o no? El problema de eso es que
eso tiene sentido una vez que YA tenés los datos de errores
calculados, no me ayudaría a armar un criterio de
corte para el sistema mientras corre

-----------------------------------------------------------------------------------

15/01/2021

Lo primero que hice fue revisar que no faltara nada en el Github.
Luego cloné el repositorio de Github en mi carpeta de SiCoMoDa. La
idea es empezar a usar Github Desktop como una forma más sencilla,
(y parece que sinceramente lo es), de ir comitteando el progreso
de mis archivos. De esta manera no soy yo el que tiene que estar
atento a qué cosas se modificaron, sino que el programa me avisa de
qué se modificó, crea las carpetas necesarias, sube los archivos.
Es realmente más cómodo.

Estuve dándole vueltas al asunto del criterio de corte, todavía no
llegué a un criterio razonable. No me convence del todo la idea de 
pedirle al sistema que corte cuando llega a cero. ¿Porque si nunca
llega a cero que hacemos? ¿Y si cero es una aproximación?

Estoy pensando en una cota variable, que sea una fracción del máximo
de error. De esa manera puedo desentenderme del tamaño de mi sistema
y del valor de la influencia K. En este caso se me ocurre darle un 
período largo, unas 1000 iteraciones, en las cuales el sistema se
encuentre en un intervalo del 10% o del 5% del valor máximo.
La idea sería ver en los valores que tengo cuánto "tiempo" el
sistema se encuentra en ese intervalo. Si el tiempo que se encuentra
ahí es muy grande, es decir que ya en el principio cumple ese criterio,
el criterio es super holgado y no me sirve. Necesito ver que el sistema
comience a cumplir con el criterio que yo establezca en el último tramo
de su evolución.

Por otro lado, para ver si existe una correlación entre K y N y la
cantidad de pasos necesarios para que el sistema deje de evolucionar
es que armé un gráfico que me marca la cantidad de ceros que hay en
el vector de Errores. El motivo de mirar la cantidad de ceros es que en
todos los sistemas una vez que el sistema llega al cero, no vuelve
a levantar cabeza. Entonces el número de cantidad de ceros es de alguna
manera la cantidad de iteraciones demás que hizo el sistema. Si el
sistema tardara más en llegar a un equilibrio para valores de N y K
grandes, entonces yo debería ver una caída en la cantidad de ceros
a medida que K y N crecen.

Creo que estaría bueno hacer cuentas para algunos valores de N más.
Voy a hacer eso y ver si se ve algo claro.

Ok, hice unas cuantas sumas más para valores de N entre 16 y 30. Por lo
que vi no aportan MUCHA más claridad. Pareciera que efectivamente
valores bajos de K llegan a una convergencia total más rápido a medida
que N aumenta. Después "pareciera" que los valores de K más grande
tardan más en llegar a una convergencia, y los valores intermedio
"pareciera" que se organizan de manera que la cantidad de pasos
necesarios para converger totalmente disminuye a medida que disminuye
el K. Pongo comillas al pareciera porque la verdad tampoco es
algo definitivo. El K=1 oscila terriblemente, el K=3 a veces
está por debajo del K=5, a veces por encima del K=1. No es claro.

Después armé un código para marcar sobre los gráficos de Errores_Corte
unas barreras de porcentaje respecto del valor del error máximo.
La idea es visualizar cuántas iteraciones el sistema tarda en reducir
sus errores por debajo del valor máximo, siendo que la mayoría de los
comportamientos de los errores son monótonamente decrecientes y que
por tanto los máximos suelen encontrarse al principio del gráfico.
También sirve para ver cuánto vive el sistema en cierto umbral, de manera
de determinar si el hecho que el sistema atraviese esa barrera es un 
buen criterio de corte o si resulta muy holgado, o muy estricto.

Para la próxima la idea entonces es 
.) Guardar los gráficosde Umbrales_Error. Van a ser MUCHOS gráficos, 
la idea es mirar un poco a ojo que efectivamente una vez que el 
sistema cruza la barrera del 0.5%, por decir un número, se encuentra
a unas 1000 o 2000 iteraciones de terminar. Entonces le podemos
pedir que corte luego de 500 iteraciones una vez cruzado ese
umbral.

.) Para cerciorarme cuántas iteraciones hay de distancia desde que
el sistema cruza la barrera que le digo que cruce, podría hacer alguna
clase de resta entre el índice en el cual cruza la barrera
y el valor en el cual los errores se vuelven cero. Creo que este
gráfico va a ser el definitivo para elegir un criterio. Idealmente,
todos los sistemas tienen un mismo valor de cantidad de iteraciones
desde que cruzan la barrera y eso me permite asegurarme que nunca
corto muy temprano o muy tarde. Realísticamente van a ser números
con algo de suerte no muy dispares. De ahí, si quiero un criterio 
más holgado, puedo tirarme a elegir uno de los números altos,
si quiero un criterio estricto puedo tomar valores de los bajos,
si hay mucha disparidad puedo tomar un promedio, ya veré cuando
tenga el gráfico.

De paso, la idea es que ese gráfico me va a dar una idea
de cuánto tarda el sistema luego de cruzar una cierta barrera
en llegar a una convergencia TOTAL, donde ya no varía más.
Por eso si tomo alguno de los valores más grandes el criterio es
holgado, porque en ese caso lo que va a pasar es que los sistemas
van a por si acaso pasar tiempo sin evolucionar en favor de que
ningún sistema se quede sin llegar a ese estado.

-------------------------------------------------------------------------------------

17/01/2021

Por alguna razón, recién hoy se me ocurrió pensar que quizás, sólo quizás,
estaba guardando mal mis datos. Efectivamente se me estaban guardando los datos
con una precisión de seis decimales. Lo cual es una gran cagada consideranco que
estoy trabajando con doubles, es decir que el programa tiene datos con precisión
de 12 decimales. (Leí que técnicamente son 14, igual yo por si acaso le pido
sólo 12).

Así que lo que hice fue primero probar en un archivo aparte cómo lograr que se
printearan datos con precisión de más de 6 decimales, logré que se printeara
con 12.

Luego, pasé eso al main y me aseguré que los errores y los valores de opinión
tuvieran esa precisión. Aunque siendo sincero, los valores de opinión no necesitan
semejante precisión. Digo, esos números rondan los enteros, ¿Para qué carajos
quiero 12 decimales más de precisión? Pero como digo, ya están calculados,
no es que me esté ahorrando cómputo no anotándolos.

Lo otro que hice, que me siento muy bien por haberle encontrado la vuelta,
es darle un sentido a la variable de Error_Cuad. Hasta ahora era sólo un
número que en la medida que se iba a cero me marcaba que el sistema cambiaba
cada vez menos, pero ni idea de qué representaba. Lo que hice ahora es normalizarlo
usando la definición de norma para que ese número represente la VARIACIÓN PROMEDIO
DE CADA OPINIÓN. De esta manera, si ese número es 0.0001, lo que me dice es que
en promedio TODAS las opiniones variaron eso. Eso es lo que significa ese número
ahora. Por eso decidí llamarlo ahora en el código VarProm, es decir Variación Promedio.

Me doy cuenta que el nombre de los archivos sigue siendo Datos_ErrCuad_... .
Debería cambiarlo para llamarlo variación promedio y reflejar mejor el sentido
de ese valor. Dejaré eso para otro día. Lo bueno es que puedo cambiar eso
sin necesidad de cambiar el código de Python en nada.

Esto hace que mi análisis de la cantidad de pasos que el sistema necesita
para llegar a una convergencia TOTAL sea una total pelotudez. Debí darme
cuenta de la terrible gilada que estaba diciendo.

Ahora, visto y considerado esto, habría que considerar que el criterio sea
un número fijo ahora que la variable representa algo claro del sistema.
Por ejemplo, si propongo que el criterio sea 10^(-6), sería decir
que si las opiniones empiezan a variar en valores menores a la
millonésima parte, entonces considero que el sistema deja de
evolucionar y llegó al estado de equilibrio. Suena bastante
razonable, recordando que mi sistema se mueve en el orden de los
enteros. Es decir, las opiniones en promedio estarían variando 
en 6 órdenes de magnitud menos que sus valores actuales. Por tanto
esa es una variación desreciable. Si el sistema entra en esta región
y no escapa luego de un tiempo, lo podemos cortar y listo.

En lo que queda del día me voy a poner simplemente a ordenar todo,
subir todos los datos correspondientes y listo. Estoy actualizando
programas y archivos de Documentación.

Cosas pasaron y tuve que borrar los archivos de datos que tenía
en la carpeta local, por lo que aproveché para mandar a correr de
nuevo todo el programa. Así que no voy a tener tiempo de subir
las imágenes a la carpeta de Tesis, eso queda para otro día.

¿Qué es lo próximo para hacer?

.) Implementar el sistema de corte en el main. El criterio
seguro sea que la variación atraviese el piso de los
10^(-6) durante unas 500 iteraciones.
.) Mirar en el trabajo de Baumann cómo realizó las imágenes.
Empezar a intentar hacer esas imágenes.
.) Subir las imágenes que tengo al Drive para mantener
registro de todo. (Quizás arranque por esto)
.) Optimizar el código en el tema de la tanh. A esta altura
los tiempos de corrida están empezando a ser molestos, me gustaría
reducirlos.

------------------------------------------------------------------------------

19/01/2021

Ayer subí los gráficos al Drive. Al final borré algunos que tenía de
antes porque ya no eran válidos. Y subí uno de los nuevos de Variación
Promedio de las Opiniones.

Hoy ya armé una función que implementa el criterio de corte. Algo gracioso
es que al armar esta función pude claramente observar la diferencia entre
pasar una variable por copia o por referencia. Al principio estaba pasando
el parámetro de corte por copia. Eso hacía que mi sistema nunca cortase
porque el parámetro de corte nunca variaba, sino que lo que variaba era su copia.

La solución entonces fue pasar el parámetro por referencia, y se solucionó
bárbaro. Creo que ese problema no lo voy a tener en mi sistema, pero bueno,
esto muestra lo importante que es seguir con el tutorial de C. Quiero llegar
a la parte donde habla sobre structs.

Al final decidí pasar el código y no armar una función en el main. El motivo
de esto es que siento que sino estoy pasando MUCHAS veces el struct por copia.
Eso me preocupa un poco. También está el tema de que iba a tener que definir
muchos argumentos para la función, por el hecho de que iba a tener que
pasar los punteros a mis archivos en los cuales estoy anotando los datos.
Para ahorrarme eso, decidí simplemente copiar el código fuente en vez
de pasarlo en una función. En un futuro, cuando decida corregir el pase
por copia a un pase por referencia, quizás convierta todo esto a una función.
O cuando me arme la función que interpola las LUT.

Ahí lo implementé y estuve mirando los números. Tomando un criterio de corte
de 10^(-6), lo que observé es que en las últimas iteraciones el sistema
está variando en la séptima cifra decimal en valores promedio de tres o cuatro.
Es decir, ~4*10^(-7). Por tanto, el programa está cortando correctamente.

Ya subí el programa con las nuevas implementaciones a Github.

--------------------------------------------------------------------------------

21/01/2021

Ahora que el mecanismo de corte funciona, lo que voy a hacer
es crear una nueva muestra de datos y graficarlos en Python, para
ver que el código funcionó bien.

Ahí miré las imágenes, parece estar bárbaro. Cada iteración corta
luego de 1000 iteraciones de haber atravesado el piso de 10^(-6).
Y en los gráficos de las opiniones se ve que cada uno corta
a ritmos diferentes, pero siempre la curva se ve como que
deja de variar.

Ahora lo que voy a ver es si Baumann tiene algo anotado
sobre el código con el cual arma sus gráficos. Sino, tuve algunas
ideas de cómo reproducir esos gráficos por mi cuenta.
Bueno, en lo que yo leí no vi ninguna referencia a un código
o software especial usado para generar esos gráficos.
Queda entonces arremangarse y hacerlo yo mismo. Por ahora
dejemos de lado las distribuciones, arranquemos con los gráficos
que muestran como la opinión de los agentes se van
moviendo. La idea es que las opiniones armen una trayectoria
en línea gris. Es decir, voy a tener que armar vectores con 
la opinión de un agente en cada iteración, graficar eso con una
línea muy fina y gris. Luego, en el punto final, le pongo
un punto grande con un color que tenga que ver con el ángulo
que forma el vector, tomando en cuenta una distribución de todos
los colores según el ángulo que forman con la horizontal.

Para el tema de estudiar cómo varía el estado final del sistema
con delta y alfa, lo que voy a hacer es dentro del mismo programa
de C, o quizás en Python, un código que discrimine los distintos casos
en función de los signos de las opiniones finales y de sus módulos y
le ponga la etiqueta de: Consenso, Polarización y Estado Ideológico.
Luego, en Python tomo la etiqueta y a ese punto le coloco un marcador
cuadrado del tamaño correcto y con el color asociado a la etiqueta.

Ya modifiqué un poco el archivo del main para que me empiece a 
generar los datos que voy a necesitar graficar. En este caso
cambié los nomrbes de los archivos, me deshice del archivo de
Variación promedio de las Opiniones porque creo que no lo voy a necesitar
y ya modifiqué el Mover.sh. Ahora me voy a poner en Python a armar
el gráfico de estos datos.

De paso, también voy a armar una documentación en la carpeta con
imágenes de los archivos creados con el mecanismo de corte implementado.

-------------------------------------------------------------------------

27 y 28/01/2021

Esto es una entrada doble porque ayer arranqué con esto.

Mi idea es empezar a probar el tema de optimizar el código. Busco
implementar el uso de un archivo en el cual se encuentren ya calculados
los valores de la tanh. De esa manera mi intención es reducir el 
tiempo de cómputo de cada iteración y así poder correr los datos más rápido.
Porque con el tiempo actual esto resulta muy poco viable. Supongo que
podría ganar tiempo reduciendo alguna cantidad de cuentas, pero entiendo
yo que si quiero armar un gráfico similar al que hace Baumann para
determinar en el espacio de sus variables cuando el sistema llega 
a un consenso o cuando se polariza, voy a necesitar hacer un
barrido fino. Por no decir que técnicamente tendría que iterar
muchas veces y tomar un promedio, no alcanza con una sola iteración.

Reducir el tiempo de cómputo es PRIORITARIO.

Por eso partí de armar un archivo "Tabla_Valores_Prueba". En este
archivo armé una matriz de 3x3 con números en sucesión.
Probé el uso de la función fgets. Si bien está copada porque
lee una cierta cantidad de caracteres, los convierte a string
y los mete en un puntero, mi problema es que hasta donde
entiendo sólo puede levantar chars. Eso es un problema, porque
entonces no podría correctamente levantar los números doubles.

Ahora estoy pasando al uso del fscanf. Por lo que entiendo,
fscanf busca el patrón que le armás en el centro y eso se lo
pasa a las variables que le indicas. Entonces ignora totalmente
los newline y por lo que vi, ignora las tabulaciones también, porque
le estuve pidiendo que levante ints, que son los números que como
dije antes puse en la matriz, y ni registro tuvo de las tabulaciones.
Entonces se me ocurre que puedo armar un vector, o dos valores ya veré,
en los cuales guardar los números que necesito para la interpolación.

Mi idea hoy seria investigar un poco el fseek(). La idea sería poder
leer el archivo sin tener que pasar todos los datos a un puntero.
Lo cual sería una re cagada y creo yo que haría que todo este trabajo
sea reverendamente al pedo, porque el archivo va a tener muchos datos
ya de por sí. Eso es algo que también tengo que ver, si abrir un archivo
pesado requiere mucho trabajo o no.

Pero volviendo al fseek(), eso me permitiría fácilmente moverme en el
archivo. Esto además es muy importante porque el instanciar va a hacer
que el programa se cargue muchas veces, haciendo que el archivo se abra
y cierre muchas veces. Entonces, nuevamente, estaría perdiendo un
montón si tengo que cada vez cargar todo el archivo. De nuevo,
reverendamente al pedo.

Ok, el fseek() parece bastante sencillo. Además, comprobé que las 
tabulaciones ocupan una posición cada una, efectivamente. Ahora
debería comprobar que efectivamente los double ocupan un único
espacio, cosa razonable, y algo más importante, que el fscanf
avanza el indicador de posición a la posición inmediatamente
siguiente en la cual termina el patrón que busca, una vez
encontrado. Esto es importante porque si quiero moverme leyendo
desde el archivo, necesito total control de la posición del indicador.

Para esto voy a necesitar entonces las funciones ftell() y rewind().
ftell() me devuelve un int que me indica la posición del indicador,
mientras que rewind() regresa el indicador a su posición inicial. 
No sé si ese sea tan necesario o útil.

Bien, como yo supuse, el fscanf revisa el archivo hasta encontrar
la primera instancia del patrón indicado. Luego lee el patrón,
envía los elementos a las variables indicadas y luego avanza
una posición más. Cabría ver que los doubles ocupan exactamente
el mismo lugar, pero eso es casi obvio. Igual lo voy a probar.
Una vez hecho eso, lo que voy a hacer es archivar esto, armar
un archivo gigante y ver si el revisar algunos de sus elementos
me consume mucho tiempo.

Menos mal que probé si los doubles ocupaban el mismo lugar. No lo
hacen. Cada número guardado de un double tiene una posición propia.
Lo que ocurre es que el scanf sabe interpretar el número y lo lee
entero cuando le decís que tiene que levantar un %lf. La pregunta
interesante es, ¿Lo levantaría igual si el número tuviera más decimales?
¿O en ese caso habría que indicarle la cantidad de decimales? Además,
¿Qué pasa si arranca a leer número por la mitad?

.) Ya probé lo de indicar más decimales, eso no funcionó. La función
fscanf() no permite agregar el número de decimales como parámetro
a la hora de levantar datos, entiendo yo que lo que pasa es que lo levanta
todo y listo.

.) Sobre arrancar el número por la mitad, lo que hace es tomar el número
desde el cual arranca y lo lee todo hasta encontrar un punto o un final.
Vas a tener un problema en las cuentas, pero nada malo va a pasar en términos
de crasheos.

.) Si el número de decimales es mayor, el tipo lo lee correctamente. Recordá
que lo que hace fscanf al leer un "%lf" es mirar el número, ubicar el punto
y leer hasta que haya un corte del número. Entonces si el número tiene más
decimales no importa, lo va a leer hasta que se corte. Por tanto, lo que
marca el final del número es la tabulación al final del día.

Bueno, habiendo hecho todas estas pruebas estoy en condiciones de armar
una función que lea los datos que yo quiero leer. La idea de
la función es que reciba el puntero al archivo, usando fseek() ubique
los valores a interpolar y luego pase esos valores a una función de
interpolación. O que ella misma interpole. Igual quizás la interpolación
la arme aparte sólo para tenerla para futuras funciones.

Entonces, ¿Cuáles son los objetivos mañana?

.) Primero, mandar a correr el programa para crear un archivo con millones
de datos. Estaría bueno primero probar de armar un archivo de muchos datos
y ver si el programa le toma tiempo abrirlo, buscar dos números y volver.

.) Definir los intervalos en los cuales voy a calcular mi tanh. La idea
es cortar en algún punto donde pueda aproximar la tanh por 1. También
definir cada cuanto debería hacer el paso de la tanh. Para eso estaría
bueno ver una medida de cuánto varía la tanh punto a punto, porque
quizás varía cada 10^(-6) y el tipo varía en 10^(-3) cada 10^(-4).
Entonces tendría una precisión de 100 valores en los cuales yo no noto
diferencia de la función.

.) Armar una función de Interpolación de los datos.

.) Archivar la función de prueba armada hasta ahora. Ordenalo para
que se vea un poco más fácil de entender la próxima vez que quieras
mirarlo. Separa la parte escritura de la parte de lectura.

------------------------------------------------------------------------------

29/01/2021

Ya guardé los datos en el programa de Archivo.c. Ahora lo que voy a hacer
es armar la tabla de datos. Ya hice un primer archivo en el cual guardé
1 millón de datos double. Eso me tomó siete segundos. Por tanto podría guardar
10 o 100 veces esta cantidad de información y no sería mucho problema.
Sería una tarde trabajando, ningún problema. Creo, veamos si ahora
intento guardar doubles, pero que sean tanh.

Es algo totalmente inesperado, pero el programa tardó 4 segundos menos
en guardar valores de tanh. No entiendo qué pasó ahí. Pero bueno,
cosas pasan. Hagamos unas pruebas sobre tiempo de cómputo primero.

Esto no pareciera que es un problema. Estoy probando el uso de la función
tanh para realizar diez millones de cuentas y lo estoy comparando con la
realización de unas simples cuentas de multiplicación y suma. Las mismas
cuentas que más o menos haría con la función de interpolación.
No logro ver una diferencia  de tiempo en el cálculo que hace el programa. 
Digo, los dos parecen tardar cero segundos. No parece haber ninguna
diferencia, como que la tanh no le aporta mayores problemas al cálculo. 
De ser así, no tiene ningún sentido que haga todo este quilombo para cambiarla
en mi función original. Quizás vale más la pena que revise el
funcionamiento general del modelo y empiece a trabajar en el 
armado del gráfico de los estados finales del sistema en función
de las opiniones alcanzadas.

Ahí está, ahora sí pude poner a prueba esto. Casi se me cae el
mundo a pedazos. Como mi medición del tiempo no tenía una precisión
que me permitiera diferenciar cuál proceso tardaba más, lo que hice
fue meter a los dos loops en un while y ver cuál proceso lograba
en un segundo realizar más ciclos. El resultado fue que el proceso
que agregaba una tanh realizaba un 40% menos de ciclos en el mismo
intervalo de tiempo. Por tanto, hay una diferencia apreciable
y vale la pena optimizar al sistema con la implementación de un método
de interpolación de los valores de mi tanh().

Ok, momento, todavía no está todo dicho. Esto parece tener una variabilidad
de la concha de la lora. Volví a mirar al sujeto que hace las cuentas
simples, el que le ganó por un 40% en los ciclos, y en una iteración
apenas logró realizar 9 ciclos. En otra realizó 76. Creo
que voy a tener que guardar muchos valores y luego armar un histograma
y compararlos. Qué paja.

---------------------------------------------------------------------------

31/01/2021

Bueno, estuve retocando el Instanciar.sh para que mañana simplemente lo
mando a correr y eso pueda estar tranquilamente tres putas horas corriendo.
Todo para armar un cuarto de los archivos que necesito y encima con sólo
500 agentes, no con 1000. Pero bueno, es lo que hay, quiero gráficos.

Por otro lado, también armé un programa para medir los tiempos de cómputo
y comprobar que efectivamente vale la pena implementar una función de
interpolación. Con eso me guardé los datos y armé un histograma. El histograma
está guardado en la carpeta de Tesis. La idea es simplemente que hice
que el programa realice muchos cálculos, unos simples, unos con tanh.
La idea es que para medir cuál caso trabaja más rápido los hice
hacer una tarea similar y ver quién puede hacerlo más veces seguidas.
Guardé esa cantidad de veces y eso es lo que grafiqué en el histograma.

Ya archivé los datos, pero no subí nada a Github.

¿Entonces, qué debería hacer mañana?

1) Armar muchos archivos y empezar a armar los gráficos
correspondientes en Python. Va a haber muuuucho cálculo de fondo.
2) Definir los intervalos para calcular la tanh. Para esto calcular
la derivada de la función y ver cada cuanto varía. Creo que puedo
incluso hacer un barrido cada 10^(-5).
3) Armar efectivamente la función de interpolación.

------------------------------------------------------------------------

06/02/2021

Estos días estuve estudiando E4 fuerte, por eso casi ni me puse con esto.
Hoy ya me encargué de poner en todos los gráficos subidos al powerpoint
de la tesis las ecuaciones que indican los valores graficados.

Mañana debería preparar se gráfico explicativo de la forma en que se
resuelve cada iteración. También estaría bueno ver lo que me preguntó
Pablo sobre si el sistema sigue reduciendo el valor promedio de 
variación de opiniones de manera indefinida. Ya comprobé que un double
te puede guardar hasta 30 decimales sin problemas. Re flashero.

También tendría pronto que empezar a trabajar en el tema de la función de
interpolación y el armado de la tabla de valores de tanh calculadas.

------------------------------------------------------------------------

07/02/2021

Hoy terminé de subir unas imágenes de los gráficos de trayectorias de
opiniones que ayer no había subido. Ahora sí, no me queda nada por
subir al archivo de power Point. Debería empezar a trabajar en el
armado de los gráficos para explicarle a Pablo las interacciones entre
agentes. Y en el armado del archivo que guarde datos para ver que el sistema
no cae hasta el infinito.

------------------------------------------------------------------------

09/02/2021

Armé un archivo con las opiniones y otro con la variación promedio para un
número de 200 agentes, un K=5, T=2  y lo hice correr un total de 60000 pasos.
El objetivo es ver si el error cae infinitamente o en algún momento se plancha.

Ahí armé el gráfico de esto, y de paso corregí el código de Python para que
cuando arme estos gráficos, si hay un salto en el número de agentes no
guarde gráficos vacíos. Eso me pasaba porque antes me meovía entre valores
mínimos y máximos de N, por lo que no sabía que había en el medio.
Ahora lo corregí para que me arme un conjunto con todos los valores
de N para los cuales existe un archivo de datos, así sólo grafica
para valores de N existentes. Es decir, para archivos que tienen
ese valor de N.
	De paso, no corregí esto para la sección del código que arma
los gráficos sacados de la carpeta de interacción de Pares. Así que
atento a eso.

Cuestión, por lo que vi del gráfico hecho, el error no cae infinitamente.
Pareciera que llega como hasta 4*10^(-17) y después cae directamente
a cero. Y eso teniendo en cuenta que guardé hasta 20 decimales. Así
que pareciera que no cae infinitamente, lo cual es genial

------------------------------------------------------------------------

10/02/2021

Subí los gráficos que hice para el caso de 200 agentes con el objetivo
de ver si el error del sistema eventualmente caía a cero. También
lo puse en el archivo de Powerpoint. Los gráficos están en la 
carpeta de Corte, para que no quede ninguna duda de dónde están.

------------------------------------------------------------------------

16/02/2021

Al parecer en algún día antes me armé la tabla de valores de la Tanh.
El problema es que parece que eso no lo anoté acá, lo cual es una muy
mala situación. Así que anotemos directamente lo que pasó desde la
reunión con Pablo el miércoles 10.

Juraría que la Tabla_Valores_TANH la armé antes de juntarme con Pablo.
Durante la reunión con Pablo y Seba, charlamos sobre que deberíamos
entonces rearmar el programa para lograr replicar el modelo de Baumann
entendiendo que hay dos formas de realizar el trabajo.
La primera sería armando una red impulsada por actividades, con agentes
que se conectan con m otros agentes siguiendo homofilia e interactúan
con varios sujetos al mismo tiempo
La segunda es la que venía haciendo, que es pensar al modelo como un
modelo de agentes, con interacciones de a pares, quizás habría
que incorporar a eso también el tema de la homofilia y la red
impulsada por actividades.

Cuestión, decidimos trabajar sobre el primer caso que sería un poco
más exactamente lo que hizo Baumann, y luego pasaremos al caso
dos con más tiempo.

Entonces, lo que voy a tener que hacer ahora es reformar el programa
que tengo armado, porque tengo que modificar la función Din1 y
Din2 para que ahora trabajen con más gente, tengo que conectar usando
a la red de adyacencia, tengo que armar un mecanismo de homofilia y el
de variación temporal de la red según una cierta cantidad de pasos.

Hay claramente MUCHO por hacer, no parece que vaya a correr el programa
pronto, en especial considerando que voy a empezar a dar clases, por
no mencionar el final de E4.

Considerando todos los cambios que voy a tener que hacer al programa
principal, creo que es una muy buena oportunidad para modificar las
funciones de manera que no tomen al struct por copia, sino que lo tomen
por referencia. Me gustaría creer que haciendo eso, ahora esto va 
a funcionar mejor.

Lo primero que voy a hacer entonces es guardar el programa actual en la
carpeta de Grafico Baumann dentro de Programas y actualizar la documentación.
Luego, voy a empezar a hacer pruebas modificando un poco los structs
y con algunos structs en Prosem. Hecho eso, pasaré a hacer la modificación
total, desde adentro hacia afuera, arrancando por las funciones Din1 y Din2
y terminando con una nueva función Paso Temporal que se encargue de
evolucionar TODO el sistema en cada iteración. O quizás no, quizás 
mantenga el mismo formato más o menos abierto del código como
para poder intercalar más fácilmente ciertas cosas.

En el archivo de Powerpoint, acabo de agregar unas diapositivas
de separación, cosa de que quede claro y sin lugar a dudas cuáles
archivos los hice con cual código.

Luego de un poco de experimentación, ya descubrí qué era lo que
hacía que los structs funcionaran raro. El problema estaba en 
la forma de compilación de mi programa que yo hago, el hecho
de que uso el make all y que este programa es el que decide cómo
se updatean los archivos.o. En resumen, el problema que tenía
es que el archivo .o debía de alguna manera guardar registro
de la posición en el código de las variables. Lo cual no es
algo tan descabellado. Entonces, al moverlas de lugar se generaban
todos esos errores de acceder a memorias de manera equivocada.
La solución que encontré es simplemente reescribir los archivos
.o cada vez que modifique los structs. Eso se va a encargar de
armar correctamente las direcciones y todo va a estar bien.

Lo importante, ya no hay motivo para temerle a los structs,
ya encontramos el motivo de las fallas. Ahora sí, son una
herramienta bajo mi control. Hermoso

---------------------------------------------------------------------

17/02/2021

Ya hice un archivo para probar el uso de punteros a structs,
es realmente muy sencilla la cosa, así que no va a haber problema
en pasar todo el uso de structs a punteros de structs.

Entonces, lo que voy a hacer es lo siguiente:
Primero voy a cambiar todo el uso de variables de structs por
punteros de structs. Para esto voy a ir desde afuera hacia
adentro, arrancando por el main, luego voy a pasar a los demás
archivos, y una vez que haya modificado todas las funciones en
los paquetes del main, ahí si regresar al main y desde ahí
modificar el input de las funciones.

Hecho esto, recién ahí voy a empezar a agregar las funcionalidades
nuevas al programa. Esto debería terminarse en no demasiado tiempo.

Ya hice los cambios en todos los archivos, el programa funciona tal
cual como funcionaba antes, ahora sí podemos empezar a trabajar
en implementar las nuevas ideas. Están todas anotadas en objetivos,
para que no te olvides. El interpolador de la tanh por ahora
no lo voy a usar. Cuando digo cambios me refiero a que ahora
todos los programas corren con punteros a structs, no con
variables de structs.

--------------------------------------------------------------------

21/02/2021

Ya estuve revisando el tema de cómo armar la red de Erdos Renyi.
Mi interés principal hoy es organizar claramente el trabajo para
hacer mañana y el martes, cosa de llevar algo programado para el martes
para mostrarle a Pablo.

Primero, voy a querer crear redes de Erdos Renyi. Eso es bastante fácil,
ya tengo el generador de la matriz de Adyacencia. Lo único que tengo que
hacer es definir un nuevo parámetro que sea el grado medio y con ese
definir la probabilidad de interacción, una sencillez.

Luego, voy a querer que los sujetos operen sobre esa red. Así que voy a tener
que ir a revisar la función Din1 y cambiar su interacción entre sujetos de
manera que ahora interactúe con varios a la vez.

Lo siguiente, que es un poco más enrevesado, es lograr que el RK4 me
evolucione el sistema de forma sincrónica. La forma en que evoluciona
el vector actualmente me gusta. Lo que voy a tener que ver es cómo guardar
correctamente la parte evolucionada. Lo que podría hacer es intentar guardar
los datos en un vector extra, como el preOpi. Pero no me gusta la idea
de tener 15 vectores dando vueltas. Aunque creo que podría agregar un vector
de estado final y usar eso. Habrá que ver.

Con esto creo que ya el sistema podría funcionar correctamente. Y estaría
listo para hacer simulaciones. Lo de variar Alfa, Delta y N es tema de la
instanciación nomás.

------------------------------------------------------------------------

22/02/2021

Trabajando en el código, ya hice que el programa genere mi red de adyacencia
con una probabilidad que va variando según la cantidad de agentes. 
Visualicé la red y en efecto se genera de manera correcta. De paso, 
también agregué en el generador de la red una línea que pone ceros
en la diagonal. Por si acaso nomás, no estaba seguro de si se ponían
ceros al principio. CORRECCIÓN: Ya lo vi, en la inicialización del
vector ya se colocan ceros en todos lados, así que no debería haber
ningún problema con eso. Igualmente, no pierdo nada por hacerlo dos
veces. Digo, es algo bastante rápido de hacer.

Revisé la función Din1. La modifiqué para agregar el factor de 
la matriz de Adyacencia, de manera que ahora toma en cuenta los
agentes con los que está conectados. También le agregué un if
que cancele la cuenta en caso de que los agentes no estén conectados.
Porque para qué hacer toda la cuenta con la tanh si al final
el cero de la matriz de Adyacencia lo mata.
También modifiqué la función Din2, para que haga la sumatoria 
sobre los todos los agentes de manera que la tanh se calcula 
sobre toda la red y se van sumando todas las tanh que correspondan.

Ahora estoy modificando también la iteración, de manera de incluir
una nueva matriz que sea la que itere y en cada paso me guarda el nuevo
valor de opinión en la matriz de Opinión final. La idea es esta:
.) Antes de iterar, saco una foto a todo con mi vector PreOpi.
.) Inicio la función Iteración.
.) En la función iteración, antes de mandar a ejecutar el RK4,
primero copio mi foto (El vector PreOpi) en el vector que SÍ voy
a iterar, el vector ItOpi.
.) Evoluciono para un agente y un tópico al vector ItOpi.
.) Luego de evolucionar el valor de opinión de un agente respecto de
un tópico, copio ese valor en la matriz Opi.
.) Repito los últimos tres pasos hasta haber recorrido todos los agentes
y tópicos.
.) Termino la iteración, teniendo ahora en el vector Opi el estado
del siguiente paso temporal, en el cual todas las opiniones fueron
evolucionadas a partir de una foto.
.) A continuación guardo los valores que correspondan, calculo las
diferencias y vuelvo a empezar.

Al final logré modificar a la Iteración de manera de no necesitar
un tercer vector de opiniones. Para eso lo que hice fue modificar el
RK4 de manera de que la evolución de la función ahora no se guarde
en el vector que le paso al RK4, sino que ese valor sea returneado por
mi función. Luego, lo que hice fue guardar ese valor en el vector
Opi, de manera que el Opi que se construye sea mi siguiente paso temporal.
Entonces, la "foto" que le paso al RK4 de mi sistema en el paso actual
es el PreOpi. Y lo genial de esto es que el RK4 NO modifica de ninguna
forma el vector que le estás pasando, entonces ese vector puede servir
sin problemas como una foto estática del sistema.

En otros temas, descubrí un craso error en las pendientes usadas 
al calcular el RK4. No puedo creer que no hubiera notado esto antes.
Me da bastante bronca que esto haya pasado por fuera de mi radar,
esto es un error importante en las cuentas. La cosa es que por como
yo armé el RK4, tengo un vector de k, que son las pendientes intermedias
que calcula el RK4 para evolucionar el sistema. Por una cuestión de simpleza
en el código, yo hice que existan 5 k, donde la primera es siempre cero.
Por eso yo las había numerado desde k0 hasta k4 mis k, entendiendo que la
k0 es un fantasma creado por mi. El problema es que cuando calculaba el paso
siguiente del modelo, en vez de usar el conjuntos que va desde k1 hasta k4,
usaba el conjunto que va desde k0 hasta k3. Ya lo corregí, espero que
no haya más sorpresas de este estilo.

Falta ponerlo a prueba, pero la idea es que el código ya está reformado
para ponerlo a probar con lo que decía Pablo. Anotemos entonces las características
de este nuevo código:
.) Ahora el ejecutable recibe tres valores como input por línea de comando.
Estos valores, en orden, son el número de agentes N, el valor de Alfa*10
y el valor de Cosdelta*10. Los valores de Alfa y Cosdelta están multiplicados
por 10 porque no descubrí como pasar en línea de comando números fraccionarios,
así que paso números más grandes y lo que hago es en el programa de C
dividirlos para que sean lo que tienen que ser.
.) El programa genera una red de Erdós-Renyi con grado medio igual a 8.
.) Al evolucionar a los agentes, el programa mira ahora la opinión de todos
los vecinos de un agente, en vez de mirar la opinión de un sólo "vecino".
.) El programa genera archivos con nombres:
"Datos_Opiniones_alfa=$_Cdelta=$_N=$" (Los $ significan números").
.) Los parámetros son K=1, T=2, dt=0.001, Máxima Opinion inicial = 3,
Criterio de Corte = 10^(-6), Iteraciones Extra = 2000.

Hagamos una primer prueba del programa. Pareciera que funciona lo más bien,
habría que graficarlo para ver que todo está en orden. Hagámoslo hoy eso.
Así ya para mañana tengo los datos hechos, y a lo sumo dedico la mañana
a hacer el commiteo correcto de los archivos.

Para hoy entonces queda crear las carpetas para guardar los archivos de datos,
crear la carpeta para las imágenes, modificar el Mover.sh, modificar el 
Instanciar.sh y largarlo a correr, por lo menos para 10 y 100 agentes.
Es más, siendo sincero, no pongas al N como un valor de Input, sacalo
por ahora, y que el Instanciar sólo recorrar en Alfa y Delta. Porque
siendo sincero, no vas a poder calcular con tiempo las simulaciones
para N=1000. Vos y yo lo sabemos muy bien. Esto no es un tema de
pajero, es simplemente una cuestión de que no quiero comprometerme
a cálculos que después no se puedan terminar.

---------------------------------------------------------------------------

23/02/2021

Creo que hoy tampoco voy a hacer el trabajo más burocrático, seguro lo
termine haciendo después del final.

Hoy voy a intentar implementar una función que lo que haga es detectar
grupos en mi red. La idea es que pueda detectar si la red es conexa. Con
eso creo que ya voy a poder hacer correr los casos de N=10 y N=100 que me
dijo Pablo. Luego, mejoraré eso para que en caso de que la red no sea conexa
pueda artificialmente crear una red conexa. En el sentido de generar enlaces
en lugares específicos para que quede todo conexo. Más que nada
porque sino creo que hay grandes chances de que el sistema se quede mucho
tiempo intentando crear de manera aleatoria una red que justo resulte conexa.
Estoy seguro que esa probabilidad se puede calcular, pero ahora no quiero
perder tiempo en eso, pero después lo veré.

Armé la función, la probé y parece que funca bárbaro. La probé arrancando
desde cualquier nodo, con grupos que yo sé exáctamente cuáles son. Funciona
muy bien, siempre identifica correctamente el tamaño del grupo. En el futuro
me gustaría que además identificara todos los grupos que hay, no sólo el
que está revisando, pero para eso voy a tener que trabajar más el código.
Por no decir que la forma en que funciona ahora es con un montón de
ifs y eso no me gusta una mierda. Después veré de corregirlo de una manera
que resulte más fluída y razonable. Pero lo importante es que funciona y
ya podría implementarla en la función del main, con el objetivo de que compruebe
que el grupo sea de tamaño N. De esa manera, garantiza que el sistema sea conexo.

-------------------------------------------------------------------------------

24/02/2021

Como la función que mide el tamaño de la componente a la cual pertenece un nodo
parecía funcionar bastante bien, decidí poner a prueba cuánto tiempo podría tardar
el programa en crear una red conexa para los valores de N que iba a usar. Por suerte,
para N=10 y N=100 las chances de crear una red conexa son grandes, así que básicamente
en un intento lo hacía siempre. Para N=1000 ya necesitaba dos o tres intentos algunas
veces. Para N=10000 le costó muchos intentos crear una sola red conexa.
Como mi intención era medir cuanto tardaba en crear redes conexas, y como mi
ignorancia me hacía pensar que iba a costarle hacerlo, le puse un while para
que lo repitiera 10 veces confiando en que eso me iba a dar una medida de tiempo.
Lo hizo muy rápido en general como para que tardara más de un segundo.

.) Este Programa con esta descripción entonces lo tengo que pasar al Archivo
todavía.

Luego implementé la función de Tamaño_Comunidad en el programa principal
y lo estoy mandando a correr para 10 y 100 agentes. Las simulaciones para
10 agentes me están tomando entre 1 y 4 segundos, promedio 2. Las de 100
agentes están tomando entre 12 y 240 segundos, promedio 25. Considerando
que en total estoy realizando 200 simulaciones, al variar Alfa y Cdelta,
La corrida total para N=10 me toma unos siete minutos, mientras que
la corrida para N=100 me toma 1 hora y 20 minutos. Un poco más de 10 veces
el tiempo de lo que tomó para N=10, justamente. Para N=1000 entonces, me
va a tomar razonablemente unas 13 o 15 horas. Lo puedo separar en dos
tandas, y hacerlo en dos días distintos, eso no es problema. Mi problema
va a ser el levantar los datos. El jupyter Notebook estuvo teniendo problemas
la última vez con programas que pesaban más de 1 Giga. Así que voy a tener que
pensar enformas de levantar mejor esos datos, o pasarme al Python propiamente
dicho, dejar por un rato el Notebook más que nada para pruebas. No me copa
mucho eso. Eso me está pasando desde que me pasé al navegador Opera. Quizás
Opera tiene algún mecanismo de corte de memoria en cuanto ve que está
consumiendo demasiado. No hice pruebas como para saber si el problema
estaba en la lectura o en la manipulación de los datos, tendré que
revisar eso cuando tenga mis datos calculados. Hay bastante para ver
en este aspecto.

En Python simplemente copié un código ya existente para realizar estos
gráficos y lo modifiqué para que levante los archivos de la nueva carpeta
guarde los gráficos en la nueva carpeta y que corrija los nombres de
manera acorde. De paso, modifiqué la función que asigna colores, porque
me pareció que no los estaba asignando de manera correcta. Por ahora
parece que funciona, pero definitivamente tengo que revisarla de nuevo
y hacer las cuentas, ya no confío en eso. Más que nada porque al modificarla
no hice las cuentas, simplemente escalé los parámetros. Lo cual no
me da mucha confianza. Así que eso, atento, es algo a mejorar.

Todavía no hice los commits correspondientes. Es obvio que al haber
hecho una simulación masiva, obviamente usé los Instanciar.sh
y Mover.sh, por tanto esos están modificados. También me falta
crear el archivo de Documentación en la carpeta de imágenes de
Erdos-Renyi. De paso, a esta altura me parece una buena idea revisar
la carpeta original de Programas C y Programas Python, ver si
hay algo rescatable y si no borrarla.

También falta subir los gráficos al Power point y hacer las anotaciones
correspondientes sobre los parámetros del modelo y demás.

-------------------------------------------------------------------------

28/02/2021

Hoy lo que hice fue commitear cosas al Github, Agregar archivos de 
documentación en las carpetas de imágenes, Modificar el archivo
de Documentación de Programas y agregar notas en el archivo
Archivo.c. Lo que me queda para terminar la burocracia de organizar
mis cosas es agregar notas en el powerpoint.

Seguro haga eso el martes, y ya mañana voy a tener que armar
una carpeta extra donde guardar archivos en los que voy a hacer
unas pruebas con distintos valores de dt, para ver cómo quedan
los gráficos al ir aumentando el valor de dt. La idea es que
eso me permita reducir el tiempo de cómputo, el tamaño de mis
archivos y demás.

------------------------------------------------------------------------

01/03/2021

Hoy voy a arrancar armando el gráfico del alfa crítico en función del
coseno de delta. Luego de eso voy a armar una tira de datos en los que
voy a hacer cálculos mientras varío el dt.

Bien, ya tengo hecho el gráfico del Alfa, ahora vamos a subirlo
al Power Point y empezar a armar las tiradas de varios valores de 
dt.

Voy a armar las nuevas tiradas cambiando el dt. Para un cos(delta)=0.2
voy a hacer cálculos con alfa entre 0 y 0.5. Voy a usar 100 agentes
y variar el dt entre los siguientes valores:
[0.001,0.005,0.01,0.05,0.1]
Estos datos los voy a guardar en una nueva carpeta llamada "Cambios dt".

Ya estoy correctamente armando los archivos. Ahora voy a tener
que armar los gráficos asociados. Para eso voy a levantar los datos
en Python, graficar las trayectorias de opiniones, y podría además
hacer un gráfico de cantidad de iteraciones según dt y según alfa,
que permita visualmente comparar la cantidad de iteraciones de cada
combinación, como una forma de medida del tiempo ahorrado 
entre iteraciones.

Me acabo de dar cuenta que los gráficos que voy a comparar son todos
distintos, eso no me sirve. Necesito que la comparación de los dt
surja sobra las mismas redes, y sobre los mismos estados iniciales,
para ver que lleguen a los mismos valores finales. Por eso es que
voy a rearmar el main para que itere en los dt DENTRO del programa
y no en el instanciar. Eso va a hacer que mueva muchas cosas de lugar.
Igual el Erdós-Renyi lo guardé ya en programas, para evitar problemas.
Este código lo usaré para esto y luego lo guardaré en una carpeta 
aparte en Programas.

Perfecto, ahora el programa funciona generando una red de adyacencia
y una serie de opiniones iniciales, y sobre esos datos el sistema
corre evolucionando, únicamente cambiando el dt. (Técnicamente lo
que también varía es el random porque no reseteo el random, pero
creo que no vuelvo a lanzar dados en ninguna otra cuenta)

Estoy trabajando en Python. La idea es armar tres gráficos a partir
de estos datos:
.) Trayectoria de Opiniones, para ver que tan bruscos son los saltos
al evolucionar el sistema
.) Variación de las opiniones, para ver cuánto tarda cada sistema
en llegar a cero y comparar eso en un gráfico. Además para ver
que efectivamente se cumpla el criterio de llegar a una variación
menor al criterio de Corte.
.) Cantidad de iteraciones en función de dt y de Alfa, para ver 
gráficamente una medida del tiempo ahorrado dependiendo del valor
de dt.

Mañana continuar desde el gráfico de Variación de las opiniones.
Creo que ya tengo el orden del programa correctamente armado

-------------------------------------------------------------------------

07/03/2021

En algún momento en la semana resolví el tema de que el código
de Python me arme los gráficos de Trayectoria de Opiniones,
el de Variación de las opiniones y el de Cantidad de iteraciones
vs dt. Así que ya tengo los gráficos correctamente armados,
guardados en sus carpetas correspondientes y con los archivos
de Documentación pertinentes.

Estudiando los datos, pareciera que lo más razonable actualmente
es usar valores de dt = 0.1 fuera de la región de transición y
usar valores de dt = 0.01 o 0.02 en la región de transición.

Las trayectorias de opiniones no parecen en lo más mínimo afectadas
por el cambio de dt, tanto en los caminos tomados como en los
resultados finales alcanzados. Sí es importante notar que el número
de iteraciones se reduce dramáticamente, y es mi entender que de 
aumentar el dt por encima de 0.1 el número de iteraciones podría
reducirse a apenas decenas, lo cual resultaría muy poco para
poder hacer estudio de la evolución del sistema.

La idea para hacer hoy, no sé si llegaré a hacerlo todo, es
primero hacer el commit a Github con todas las actualizaciones de
archivos.

Luego, tendría que armar un nuevo programa para empezar a simular
datos con el nuevo valor de dt y barriendo valores de Alfa y 
cos(delta) como los mencionados en el pdf que le mandé a Pablo.
Estos datos los voy a tener que guardar en una nueva carpeta
y además voy a tener que hacer varias corridas para cada valor
de Alfa y cos(Delta), de manera de empezar a producir estadística.
Para arrancar, yo diría hacer entre 20 y 40 corridas. Confío que
40 puedo hacer sin problemas.

Lo siguiente será levantar estos datos de manera correcta y
graficar las trayectorias de todos en un mismo gráfico, para 
ver qué pasa en promedio. Y de ser posible, armar una función que
reconozca en función de los valores finales, qué está ocurriendo con
el sistema.
(Cuando digo graficar la trayectoria de todos en un gráfico, me
refiero a tomar todas las instanciaciones de un valor particular
de alfa y cos(delta) y esas instanciaciones guardarlas en un
solo gráfico, no digo de mezclar datos de alfas o cos(delta)
distintos)

Hecho eso, ya tendría todas mis herramientas para ir armando los
gráficos de Baumann, así que tendría prácticamente la mitad de
la tesis hecha. Digo yo, inocentemente.

La nueva fase del proyecto la voy a llamar "ER2". Los parámetros
de esta nueva fase son: N = 10,100,1000. K = 1, T = 2, <k> = 8,
dt = 0.1 para valores fuera de la región de transición
( 0 < Alfa < 0.2) y dt = 0.01 para la región de transición.
El Criterio de corte lo baje a 10^(-8), El máximo de opiniones
iniciales es 3, El cos(delta) varía entre 0 y 1 de a 0,1, la 
cantidad de iteraciones extra las bajé a 50.

Los archivos creados tienen nombres: 
"Datos_Opiniones_alfa=$_Cdelta=$_N=$_Iter=$".
Los estoy guardando en la carpeta ER2 dentro de la carpeta de
Python. Para cada terna de valores N, Alfa y Cdelta estoy 
armando 40 iteraciones cosa de armar estadística con eso
por ahora. En el futuro serán más.

Lo que estoy pensando apra ahorrar tiempo en el futuro es que
podría tomar el Archivo de instanciar y definir por cases
los códigos para cacular los datos del sistema en las distintas
regiones. Es decir, los datos fuera de la región de transición
y los datos en la región de transición. Así con uno simples 
números o comentando y descomentando puedo mandar a correr
todo. Pensalo.

Ahora creo que lo que debería hacer es ir actualizando el 
Power Point con las cosas de Cambio dt y la nueva fase del
proyecto, mientras el programa corre eternamente montones
de cálculos extra de fondo.

-------------------------------------------------------------------

08/03/2021

Hoy mandé a correr el programa para todos los alfas a partir
de 0,2 hasta 1; todos los cos(delta) entre 0 y 1, y todos los
N = 10,100,1000. Esto tomó unas 12 o 13 horas de cálculo.
En este caso dt=0,1. Las iteraciones extra son sólo 50.

No hice ninguna cuenta al respecto, porque el tiempo no me alcanzó.

--------------------------------------------------------------------

09/03/2021

Hoy mandé a correr el programa para todos los alfa entre 0,01 y
0,2; moviendo el alfa de a 0,01. El cos(delta) lo moví entre 0 y 1
y N = 10,100. El de 1000 lo dejé para hacerlo otro día. Es más,
creo que eso va a tomar más de un día. El dt=0,01 y las iteraciones
extra son 2000.

-------------------------------------------------------------------

10/03/2021

Borré todos los archivos con N=10 que estuve calculando. Pablo dijo,
y con mucha razón, que esos no sirven realmente. Los de 100 tampoco
son realmente útiles, pero en lo que voy armando los de N=1000
prefiero tenerlos para ir haciendo algún cálculo.

Hoy lo que voy a hacer es armar algún código que me permita graficar
en una sola imagen todo un ensamble de simulaciones para cada
Alfa y coseno(Delta). La idea va a ser trabajar con una reducción
de la opacidad de las líneas, y simplemente mandarlo todo a correr.

Si tengo tiempo, voy a arrancar a armar una función que pueda
decidir si el estado final es uno de polarización, estado
ideológico o consenso central.

También debería subir algunas imágenes más al power point y 
hacer la burocracia de estos días, que no debería ser mucha
porque hace varios días que lo único que vengo haciendo es 
crear archivos en simulaciones, no actualizo el código.
Excepto para algún cambio de Iteraciones extra y cosas.

De paso, TODAS las simulaciones hechas tienen dt=0,1. 
Al revisar el archivo del main acabo de observar que eso
nunca lo cambié. Fue un error, pero un buen error porque
me ahorró tiempo de cálculo.

De paso, las simulaciones les había puesto que corten
cuando la variación promedio era menor a 10^(-8).
Pero lo repensé y me parece un trabajo innecesario, así que a 
las siguientes simulaciones no se lo exijo. En
pocas palabras, las simulaciones que tienen criterio
de corte 10^(-8) son todas las de N=100 y las de
Alfa =0 y N=1000. El resto de las de N=1000
tienen criterio de corte 10^(-6)

Ya tengo en buena medida armado el código para hacer los 
gráficos de trayectoria combinando todas las iteraciones
en un sólo gráfico. Ahora mismo lo que estoy haciendo es
comparar distintas transparencias de las líneas para ver
cuál hace que el gráfico se vea mejor. Básicamente, lo que
quiero es que las líneas grises no me tapen el resultado
final. Después lo que voy a tener que agregar es el 
normalizar los gráficos.

Creo igual que el Alfa cómodo de la transparencia es
0,1

También se me ocurre armar un diccionario con los nombres
de todos los archivos de manera de que el diccionario contenga
los nombres de los archivos con los valores de Alfa, Cdelta
y N correspondientes. La idea de eso es no tener que repasar
TODA la lista de nombres para cada gráfico. Además me puedo
sacar de encima los if que comparan Alfas y Cdeltas y demás.

Creo que voy a tener que corregir la forma en que se marcan
los puntos finales. Porque las líneas grises terminan tapando
los puntos.

Una vez que eso esté hecho, ahí si vas a tener que hacer lo
que sigue, armar el gráfico del espacio de fases. Eso va a
ser un poco complejo de determinar.

------------------------------------------------------------------------

11/03/2021

Bien, ya tengo armado el código que me grafica las trayectorias
de opiniones. Ahora hay que darle a eso. Me da un poco de miedo
mandarlo a correr eso ahora. Lo voy a dejar para mañana y así
lo mando todo de una desde la mañana, y lo voy vigilando cada
tanto. En caso de tener problemas con el tema de la memoria del
programa, veré de descargar el programa como un archivo py y mandar
a correr todo de una desde un archivo py, eso no debería cortar
por temas de memoria.

Dios, la ansiedad que me está generando el hecho de que me ocupe
toda la memoria el trabajar con estos archivos. Ya tengo más de
200 Gigas de datos en la carpeta ER2 nomás. Esto se está descontrolando
fuerte.

Casi me olvido, necesito primero ver de normalizar los valores 
que estoy graficando. Así que voy a ver de tomar el máximo
de todas las opiniones, y al guardarlas en el diccionario OdT
ahí aplicar el factor de normalización. Aunque atento con eso
porque entonces los valores de los puntos finales no van
a estar normalizados. Voy a tener que normalizar esos
también.

Pablo dice de normalizar los gráficos. Hay un tema que
me parece bueno marcar antes de hacer eso. Acabo de mirar
un gráfico hecho en el caso de un valor en la región
de transición. O cerca por lo menos. Cuestión, que el
gráfico tenía las opiniones finales sobre la diagonal
del estado ideológico, pero lo importante es que los 
valores máximos de opiniones que tomaba eran (3,3) y
(-2,-2). También tomaba valores intermedios. Ahora, esto
me resulta muy difícil de creer, si entendemos que el
sistema está tendiendo a valores que sean el grado
del nodo. Es decir, de alguna manera la distribución
de opiniones finales debería, aunque nunca lo corroboramos,
ser similar a la distribución de grado del sistema. El sistema
tiene un grado medio ocho, eso es incuestionable. (Podría
comprobarlo de ser necesario mirando la matríz de adyacencia.
Gracias Dios por no haber dejado de anotar eso nunca.)
Cuestión, mi impresión es que lo que está pasando es
que el valor de alfa en esta región es el justo de manera
de que la tanh no cappea, entonces lo que aporta son
valores fraccionarios que hacen que el sistema no
tienda a K*(Nº 1eros vecinos), sino a cosas más
pequeñas. Entonces si yo normalizo eso, ese comportamiento
se va a perder en la normalización. Mencionárselo a Pablo

---------------------------------------------------------------------------

12/03/2021

Hagamos una lista de las cosas por hacer.


.) Armar función que identifique el estado final del
sistema en función de las opiniones finales de los
agentes.
.) Estudiar la Variación Promedio de los sistemas, para ver
si el sistema converge.
.) Armar distribuciones de los valores finales de los
agentes, de manera de caracterizar la zona de transición
del sistema

Ya tengo hecho le mecanismo de normalización de los gráficos
Ahora pasemos a identificar estados finales.

Ahí armé la función, parece funcionar bien, habría que ver 
si identifica correctamente los gráficos. Para ponerla a 
prueba voy a empezar a armar algunos gráficos y que les
ponga un texto diciendo cómo los califica.

Todavía tengo que mirar mejor los casos, pero pareciera
estar haciendo una clasificación correcta. Creo que va 
a quedar realmente claro cuando haga un estudio de la
distribución de los valores finales de los agentes.
Hasta entonces lo único que puedo hacer es ver algunos
archivos directamente con el Notepad.
Ahora, tengo que admitir que mirando los archivos del
Notepad en uno de los casos en que manda consenso, se
nota que como el programa dice, muchos casos cayeron al
consenso y unos pocos llegaron a estado ideológico.

Bajé el programa y lo corrí con el Python Spyder.
Al parecer el programa del consumo excesivo de memoria
se daba específicamente por los gráficos. Voy a ver
si quitando los delete sigue funcionando igual,
más que nada para poder seguir trabajando como hacía
siempre.

El problema central parece ser la creación de los gráficos.
Es como si en el Notebook se estuvieran guardando 
los gráficos que deberían cerrarse. Entonces esa 
acumulación de gráficos es el problema que se está
comiendo la memoria. Al trabajarlo en el Python
Spyder, esa memoria consumida se resetea cada
vez que se genera un nuevo gráfico. Así que
creo que lo mejor de ahora en adelante es
en Notebook probar funciones e ir viendo que tal,
y después en el Python Spyder correr todo.

----------------------------------------------------------------------

14/03/2021

Probé hacer que los gráficos se realizaran dentro
del mismo Spyder, pero eso de nuevo crea el problema
de que la memoria se sobrecarga. Así que por ahora mi
mejor opción es dejar que el programa arme los gráficos
de manera externa, es decir en ventanas extra, y luego
los vaya cerrando. Al hacer eso, pareciera que con
cada nueva ventana que abre, el consumo de memoria
se reinicia y de esa manera no me come toda la pc.
Aunque sigue comiendo un poco.

Lo malo de esto, es que cada nuevo gráfico que arma
me genera una ventana emergente que tapa lo que 
estaba haciendo. Voy a ver que tanto puedo ignorar
eso, aunque pareciera ser que va a ser una paja.
Igual, mejor eso y que el programa pueda laburar
todos los gráficos de una, a tener que andar con
el culo en la mano y sin poder usar la pc mientras
hace de algunos pocos.

Bueno, discutido esto, ahora viene el siguiente
punto. La función de identificar estado final 
no estaría funcionando perfectamente como yo
esperaba. Estoy notando que el problema está
en cómo interpreté yo que debería funcionar.
Así que por eso, la voy a modificar. El error
crucial que está teniendo parte de que la función
mira el estado final de UNA simulación. El tema
con esto, es que una simulación tiene dos opciones:
O cae al (0,0) o sus estados se alejan en una
direccion diagonal. El tema con eso es que una sóla
dirección diagonal jamás generaría el estado de
polarización. Por eso mi programa sólo reconoce
Consenso e Ideológico como estados finales. Entonces
lo que voy a tener que hacer es armar un vector
que guarde los estados finales de las 40 (O más
en un futuro) simulaciones y a esas les aplique el
mismo exacto algortimo que vengo usando. En ese caso, vas a ver
que los estados de Polarización aparecen.

Ahí lo corregí, o eso creo. Habrá que ponerlo a prueba.
En cuanto termine con el siguiente alfa, lo mando
a ver que tal. Lo que hice fue ir sumando las listas,
que entiendo que eso lo que hace es a la lista actual
ponerle todos los elementos de la lista nueva al final.
Eso me arma una única lista con las opiniones finales
de los agentes de todas las simulaciones realizadas
con un mismo conjunto de parámetros alfa y Cdelta.

Lo puse a prueba, ahora identifica correctamente las 
polarizaciones. Así que eso ya funciona.

Lo siguiente entonces es armar un código que me arme
las Variaciones Promedio y otro que me arme la distribución
de valores.

Empecemos por la Variación, aunque la variación no me veo
graficándola hoy. Mañana lo que haré será graficar la variación
Y las TdO todo de nuevo. Lo que voy a necesitar es guardar
en algún lugar la Info del estado final del sistema. Quizás
en el título de las imágenes. Eso es importante, porque
sino la próxima vez que quiera hacer el gráfico de fases
voy a necesitar revisar TODOS los archivos de nuevo. 
Aunque quizás no me tarde tanto porque como no esté graficando
sino sólo mirando el último sujeto de la fila, quizás la
cosa se haga rápido. Bueno, igualmente no me gustaría 
arriesgarme. Supongo que antes de mandarlo a correr podría
revisar cómo hacer para guardar eso en un archivo de datos.

Por otro lado, sospecho que esa forma de clasificar los gráficos
no va a ser del todo buena, porque lo que observe en los gráficos
de distribución de valores va a resultar más definitorio.
Así que esto va a ser más que nada una cuestión previa antes
de usar los verdaderos criterios para definir Consenso, Ideología
y Polarización.

Creo que ahí incorporé correctamente el graficado de las
Variaciones Promedio. Habría que ver que tal queda. Ahí lo
puse a prueba, se ve bastante bien. Ahora, suerte estudiando
eso. Van a ser 290 gráficos para mirar. No sé si va a haber
tanto que puedas sacar de ahí. ¿Podría poner los distintos 
alfas en un mismo gráfico?

De paso, resolver las Variaciones Promedio resultó una
boludez en tiempo. Por lo que claramente lo complicado
es el graficar todos los datos, eso es lo más complejo.
Así que superado el gráfico de los TdO, creo que no
voy a tener tantos problemas de cómputo.

Ahora debería armar un módulo que se encargue de graficar la 
distribución de valores finales. Para eso voy a querer que
tome los estados finales, arme un histograma con eso y luego

Bueno ahí lo armé y lo probé. Terminé usando 30 bins para el histograma.
Parece un número bueno para que no me aparezcan bines vacío en
el medio de una curva de distribución. Lo bueno es que en la
curva que grafiqué se puede ver a ojo que la distribución es
una distribución media centrada en 8 para los valores de opiniones,
que es lo que esperábamos ver dada la forma de la ecuación
diferencial.

Listo, ya están hechos todos los preparativos. Mañana simplemente
mandá a correr el programa y este tendría que armar todos los
gráficos de Distribución de opiniones, Variación Promedio y
TdO del sistema. También, deberías primero borrar todas las imágenes
que están en la carpeta de imágenes. Para que no haya imágenes
repetidas y demás. Y por supuesto, primero una plegaria a 
todos los santos que conozcas, como para que no le pase nada durante
la ejecución y arme todos los gráficos.

-----------------------------------------------------------------------------

15/03/2021

Vamos a ver de ir armando el gráfico de fases del sistema.
Para eso voy a necesitar de la función que identifica 
si el estado final del sistema

Ahí armé la parte del código que hace el gráfico de Fases.
Lo cual está bien, ahora el tema es mejorarlo para que se vea más lindo.
Pero más allá de eso, el gráfico se ve genial, respeta muy bien
la línea del Alfa Crítico y además la región de polarización e ideología
parece estar en sintonía con lo de Baumann. Yo diría que la diferencia
entre lo de Baumann y lo mío es un tema del barrido.

Creo que lo que voy a hacer hoy es dejarlo terminar de armar los
gráficos y después de jugar la partida con Diego me voy a poner
a armar los gráficos que voy a mostrar mañana. Hay muchos gráficos
para mostrar, no estoy seguro de cómo lo voy a hacer.

--------------------------------------------------------------------------------

18/03/2021

Estoy tratando de armar los gráficos para una mejor muestra de los
datos que charlamos la otra vez. Para eso, me interesa de alguna forma
intentar levantas las imágenes con Python y colocarlas en algún subplot.
Se me ocurre que una idea copada para mostrar serían las Trayectorias
de Opiniones y las Variaciones Promedio de los siguientes valores:
Alfa = [0; 0,05; 0,1; 0,15; 0,2; 0,3; 0,4]
Cos(delta) = [0; 0,2; 0,4; 0,6; 0,8; 1]

Eso me quedaría un gráfico total de 7x6. Me parece un buen número
son 42 gráficos todos juntos. El problema que estoy encontrando
ahora es un tema de una caída en la calidad de la imagen
mostrada en comparación con la imagen original.

Bueno, ya logré levantar una imagen y ahora se ve bien. Por lo menos
no noto problemas en la calidad. Para que la imagen salga copada
uso el plt.imshow con el comando interpolation="spline16". Posiblemente
esto le consuma algo de tiempo, pero eso en leer y graficar unos simples
42 gráficos no será mucho drama. Digo que va a consumir tiempo en
comparación a alguna interpolación más cruda.
Después para simplificar en términos de la imagen use el plt.axis
para quitarle los ejes que se me producen por el plt.show().

Lo siguiente es probar en hacer subplots. Fueron varias pruebas, pero
ya tengo algo que funciona más o menos bien. Habrá que ver de ir 
mejorándolo, pero eso es un detalle. Cuestión que la idea es partir
de una figura hecha con plt.figure. Luego, esa la divido en pedazos
usando add_gridspec. Por último, al elemento gridspec creado le
ejecuto el comando subplot para obtener los axes sobre los cuales
voy a hacer mis gráficos y listo, tengo todo hecho.

Después puedo iterar los axes para graficar en ellos cada imagen que
quiero y listo. Por último guardo la imagen, la cierro y trabajo completado.
Me queda quizás la idea de armar a mi figura los ejes que indiquen 
cómo varía Alfa y Cos(delta)

---------------------------------------------------------------------------

19/03/2021

Esto es continuación de lo de ayer. Terminé de colocar los ticks
de mis gráficos de manera correcta. Para eso le tuve que poner
explícitamente ejes al gráfico con plt.axes(). Luego con
plt.xlim() y plt.ylim() definí los máximos y mínimos de graficación
de los ejes. Sobre estos ejes armados y de tamaño definido usé
plt.xticks() y plt.yticks() para ubicar a los ticks en el lugar correcto
y ponerle los números que yo quería.

Después de eso simplemente grafiqué como venía haciendo hasta ahora, 
usando el imread y el imshow para levantar y graficar la imagen.

Hoy estuve haciendo la burocracia correspondiente anotando las cosas
acá y en Documentación en ER2. Ahora voy a subir todo con Github y
revisar qué queda por hacer. Y si vale la pena empezar algo hoy o dejarlo
para mañana.

Por lo que veo, la idea es usar pcolormesh. Realmente ahora no me veo con ganas
de hacer esto. Por otro lado, creo que básicamente o lo hago mañana o no lo
hago. Interesante lo rápido que puede pasar una semana.

-----------------------------------------------------------------------------

20/03/2021

Trabajando con el pcolormesh armé el gráfico de fases, ahora se 
ve bastante más lindo. Además, me guardé con el np.savetext 
los datos de la matriz ZZ con los datos de si en ese punto
el sistema tiene Consenso, Polarización o Ideológico.

Ahora lo lindo sería poder hacer que esto tenga los colores
que quiero yo. Y ver de graficar la curva que dice Pablo.
La curva de la región de transición empírica.

--------------------------------------------------------------------------------

23/03/2021

Ayer fue que logré hacer que el gráfico tenga los colores que
quería, le armé el colormap según mis intereses. El que yo armé
tiene básicamente tres colores para todo el rango de valores,
y esos colores están en orden verde, azul y rojo de manera
de que los colores se distribuyan al consenso, polarización
e ideología correctamente. Si no entiendo mal, de querer
poner más colores tendría que agregar más de esas tuplas
de tres elementos que me generen nuevos colores. Por otra
parte, si quisiera un degradado más lindo entre el rango
de valores, lo que tendría que hacer es aumentar el binneado,
es decir el N en el LinearColorsegment.

A este gráfico le agregué en amarillo la curva que define 
la frontera experimental entre el estado de consenso y 
polarización general. También le puse unos gráficos de TdO
de cada región para ejemplificar el comportamiento del sistema.

Cabe aclarar, estoy hablando del que llamo, incorrectamente,
"Gráfico de Fases". En realidad es un mapa de colores de los estados
finales del sistema en el espacio de parámetros Alfa-Cos(delta).

Algo que estuve pensando, es sobre armar este gráfico para el caso de N=1000.
Porque el mapa de colores se arma tomando la lista de Conjunto_Alfa
y Conjunto_Cdelta. Pero estas listas toman en cuenta TODOS los alfa
en la carpeta de archivos con datos. Entonces el problema es que
N=100 tiene más alfas calculados que los N=1000. Es decir, va a
armarme una matriz ZZ más grande de lo necesario. Esto no es un 
problema para los otros gráficos porque cuando paso por esos Alfa
no existentes para N=1000 el código simplemente intenta iterar listas
vacías, lo cual hace que los saltee. En este caso lo que va a pasar
es que mi matriz va a quedar mal armada, porque va a asignar resultados
a valores que no están ahí.

Partamos de que el ZZ se crea como una matriz de ceros, es decir que los
valores que no se retoquen correctamente quedarán como ceros por default.
Es decir, esto no va a generar un error que frene el programa, sino que
lo que va a hacer es graficar cosas equivocadas.
Luego, el ZZ se va escribiendo en cada fila y columna en la medida que
recorre la lista Conjunto_Alfa y Conjunto_Cdelta. En conclusión eso
va a generar que haya elementos de mi matriz que no sean correctamente
reescritos. Por tanto voy a tener que ver la forma correcta de hacer
esto.

Se me ocurre lo siguiente, en vez de usar las listas Conjunto_Alfa
y Conjunto_Cdelta de la manera en que las armo actualmente, podría
usar las keys del SuperDiccionario como una forma de crear esas
listas. Suena como una opción viable.

------------------------------------------------------------------------------

25/03/2021

Ahí revisé la función que asignaba colores. Funcionaba en términos de que
no generaba errores, pero en realidad tenía cierto problema en el cálculo
del resto. Creo que eso podía afectar en distorsionar el cómo estaban separados
los 360º en pedazos, pero no era algo terrible. Igual ahí lo corregí, el
problema estaba en el cálculo del Resto. Básicamente, por lo que entiendo,
este resto resultaba más chico de lo que debería, entonces podría hacer que
el if que separaba en caso de que fuera más chico o más grande que 
Delta/2 diera siempre para el caso de R más chico.

Ya que estoy, voy a cambiar el número de divisiones al doble,
en vez de 72 que sean 144. Así los pedazos de región tienen un
tamaño de 2,5º.

Estoy haciendo muchísimos cambios en el código para guardar una versión del
Graficador ER2.py que sea un poco más eficiente y más importante, que lo pueda
cargar para correr y listo. Lo primero y principal es que modifiqué el 
SuperDiccionario de manera de que sólo arme entradas en los diccionarios
para los Alfas y Cdeltas que el N correspondiente tenga. Esto es porque
mis archivos con N=1000 no tienen tantos alfas calculados como el de
N=100, entonces cuando quisiera armar el gráfico del estado final
en el espacio de parámetros con un mapa de colores eso sería un problema a 
futuro.

Por otro lado, corrí para más arriba el inicio de la iteración en agentes porque
justamente quería meter la creación de la matríz ZZ dentro de esa iteración.
Comenté la creación de las matrices de Superposición y Adyacencia porque me parecen
totalmente innecesarias. O mejor dicho, jamás las usé.

Modifiqué la lista de Opi, ahora lo que tiene no son listas con floats, sino arrays
con floats. Confío que eso acelere algunos procesos. Aprovechando esto es que modifiqué
el cómo se arma la lista de valores Var aprovechando que lo que tengo son arrays y no
listas.

Ya que estoy haciendo varios cambios, podría aprovechar y directamente transformar
el Opi en una matriz. Eso me permitiría construir las listas del
diccionario OdT con muchísima más facilidad y rapidez.

Fijate que la lista PuntosFinales debería poder resolverse de una forma
más simple que con un for. Revisá cómo se hace para concatenar arrays.

Che, ahora que lo miro OpinionesFinales es una lista que es lo mismo
que PuntosFinales, pero no está normalizada. Fijate eso, quizás
podemos juntar todo en un solo array.

El resto lo voy a revisar una vez que haya cambiado mis listas por arrays.
Pero creo que la graficación es la que menos problemas tiene. Me gustan
estos cambios que estoy haciendo. Mañana confío tener esto solucionado
y ya el sábado me pondré con lo que dije que iba a hacer.


--------------------------------------------------------------------------------------

26/03/2021

Probé armar un array de zeros e ir definiendo sus filas a partir
de listas con strings. El tipo parece tomar la lista de string y
convertirla a float todo en una sin dramas. Parece que podría
tomar tranquilamente la lista de Datos y convertirla en un array.

¿Podría hacerlo todo en un saque en vez de con un for? Por lo que
estuve mirando creo que no se puede. Dejémoslo, no vale tanto la pena
eso.

Ya armé Opi para que ahora sea un array, no una lista de listas.
Ahora que esto es un array, creo que ni necesito armar el diccionario
OdT. Creo que eso ya lo puedo sacar directo a partir de mi array Opi.
Sacar ese for va a ganar tiempo, pero más que seguro.

Ahroa voy a sacarme de encima la lista PuntosFinales, me quedo sólo
con la de Opiniones Finales, porque en definitiva esas dos se 
diferencian sólo en un factor de normalización.

Ya puse para que OpinionesFinales se arme concatenando arrays.
Casi me olvido de hacer que los puntos finales de los gráficos
estén normalizados. También acabo de sacarle los números y
ticks a los gráficos. La idea es que cuando quiera armar
mis supergráficos ahora se vean mucho mejor. También les
saqué los títulos dentro de la figura. Pasé los datos del alfa,
cdelta y N a las leyendas dentro del gráfico.

Aún así no borré los comandos que ponen nombres y demás a
los ejes, porque eso lo puedo necesitar después.

También modifiqué algo en la creación del histograma, pero es 
una boludez. Básicamente antes tenía que convertir al
OpinionesFinales en un array, pero como ya lo es me ahorré
ese paso.

Bueno, ya están hechas todas las modificaciones, probémoslo.
Luego de corregir varios errores tontos, y algunos no tanto,
ya el programa funciona bárbaro. Hay un pequeño problema con
el tema del plt.legend, no me está tomando el texto como quería,
pero eso se puede solucionar. Lo importante es que no sólo
funciona, la ganancia de tiempo pareciera ser más que
considerable. Si no hice mal la cuenta, eso debería
permitirme reducir las 8 horas de trabajo que calculé
antes a simplemente 2 horas. Es bastante, valió la pena
el modificar el código.

Bueno, lo último que me faltó fue colocar en un cuadro dentro
del gráfico los valores de Alfa, cos(delta) y N del gráfico
realizado. Con esto ubicado, ya mis gráficos pueden reconocerse 
de qué tratan a pesar de no tener título.

Bueno, mañana lo que haré es rehacer todos los gráficos para
N=100 de manera que no tengan los títulos ni nada. De esa manera
podré armar mi super gráfico un poco más nítido. Luego, los rearmaré
para que queden como estaban y listo. Mientras la pc trabaja en eso
en el Spyder, en el Notebook estaré haciendo los gráficos que charlé
con Sebas y Pablo el martes pasado.

Fijate que el armado del colormap no lo guarda.

-----------------------------------------------------------------------

27/03/2021

Ya mandé a hacer todos los gráficos, y por ahora está funcionando. Esperemos
que mis cuentas hayan estado bien y esto realmente tarde unas dos horas
nomás. Bueno, planteado esto, ahora me queda ponerme a hacer lo que desde
el martes dije que iba a hacer. Los gráficos de caracterización de la región
de transición.

El cuadro que indicaba el alfa, coseno(delta) y el N estaba un poco salido,
ahí lo corregí. Estaba un poco en la duda, pero voy a dejar el legend en
las distribuciones de opinión. En el peor de los casos, después lo corrijo.

Mirando los gráficos que se estaban armado de las TdO me doy cuenta que
al final voy a necesitar el array de PuntosFinales, quiera o no. El 
problema que surgió es que los puntos finales no pueden quedar
en el lugar correcto, porque el MaxNorm asociado varía en cada simulación,
entonces al final los puntos terminan en cualquier lugar y a veces incluso
no tienen líneas que lleven a ellos o las líneas van a parar a lugares que
no tienen punto.

Por el otro lado, armé ya el código para graficar en un mapa de colores 
la varianza en el tiempo de simulación de todas las simulaciones asociadas
a un mismo par de valores Alfa y Cdelta. Lo que se puede observar es que 
los tiempos de simulación crecen únicamente en la región de transición entre
el estado de consenso y el estado de polarización, lo cual me marca que
este parámetro sirve únicamente para distinguir Consenso de Polarización General.

De paso, estoy modificando un poco el gráfico para que el colorbar asociado
sea exactamente la varianza del tiempo simulado y no la varianza de iteraciones.

Cuando termine el spyder de rearmar todos los gráficos, debería mandar a correr
al programa que me arma el SuperGráfico. Antes de eso debería guardar en
un lugar seguro los originales, no sea que esos se me arruinen y no tenga
nada nuevo mejor. Luego los comparo y listo.

Hechos mis nuevos supergráficos, tendría que mandar el Spyder para que
una vez más rearme TODOS los gráficos y con eso dejar ahora sí mis 
gráficos con los nombres en los ejes y el cuadrito dentro del gráfico
indicando qué valores de Alfa y cos(delta) se están usando.

Ya hice todo este tema de armar y rearmar los gráficos, creo que quedó
mejor sinceramente, aunque el cuadrito de anotaciones un poquito tapa
algunos puntos, pero bueno, eso es un detalle.

Veamos de armar un estudio del sistema en función de los máximos que alcanza
como para ver si el máximo valor que alcanza el sistema es un indicador
de su cambio de regiones.

El código está armado y lo mandé a correr. Ahora queda ver si estoy funciona
o si la cagué. El código funcionó bárbaro, el colormap muestra lo que yo decía,
sólo diferencia Polarización General de Consenso,

--------------------------------------------------------------------------------

28/03/2021

Ahora debería armar la parte final, lo del esudio de las opiniones por cuadrantes.
Tengo entonces que tomar las OpinionesFinales, anotarlas en un vector según si
están en primer, segundo, tercer o cuarto cuadrante y luego con esos números 
armarme un histograma.

---------------------------------------------------------------------------------

29/03/2021

Ayer había más o menos terminado el código. Hoy lo pasé a Python
y empecé a armar todos los gráficos de distribución de opinión por cuadrantes,
el gráfico de Máximos en EP y Varianza de tiempo de Simulación.

El código que armé no resultó tan adecuado, removí todo el trabajo
en axes y subplots, y lo pasé a trabajar directamente con fig. Luego
descubrí que estaba armando mal los bottom de los gráficos de barras,
tenía mal definido el nombre del gráfico y eso me armaba sobre un 
único gráfico todos los dibujos. Cuestión, alto bardo se me generó.

Lo solucioné, armé los gráficos correctos, ahroa sólo me queda
pasarlo al PowerPoint para charlarlo con Pablo y Sebas. Por
desgracia, me da que excepto por el gráfico de las varianzas,
los otros dos son un poco inconclusos, no parecen mostrar mucha
info que se pueda usar para estudiar el sistema.

--------------------------------------------------------------------------------

02/04/2021

Desde ayer estoy intentando hacer los cálculos para el sistema
con N=1000. En general venía trabajando bien, pero el programa
tuvo una imposibilidad para poder trabajar con unos archivos
cuyo peso estaba entre 1 y 2 GB. Por lo que lo que hice fue
directamente borrar esos archivos. Fueron 5 archivos,
tres del Alfa 0,11 y Cdelta 0, y otros dos de valores 
cercanos. Técnicamente afecta a la estadística eso, pero
al carajo, no los pudo ni leer. Qué sentido tiene mantenerlos
si no los puedo leer. Vamos a ver si ahora el programa puede
terminar con todo.

Al final logré hacer los gráficos de todas las TdO. Mañana queda
hacer las Variaciones Promedio y si puedo los gráficos de Caracterización
de la región de transición.

-------------------------------------------------------------------------------

03/04/2021

Hoy seguí armando los gráficos para el sistema con 1000 agentes.
Ya tengo todos los gráficos de Variación Promedio de las
opiniones, los de Distribución de las opiniones y ahora estoy
haciendo los de Distribución por cuadrantes, y los mapas de colores
de los Máximos y Varianza de Tiempo de Simulación en EP.

Decidí que ahora me iba a poner a cambiar el gráfico de los Máximos
de Opiniones en EP para que lo que grafique sea el valor esperado
de los sujetos. La idea es ver que los agentes en efecto tienden a K*<k>.

¿Cuál sería el valor esperado? No es el promedio, ¿o sí? Sí, termina siendo
un promedio. Modifiquemos eso en el código entonces.

Copié el código de Caracterización Transición en Análisis Datos y lo modifiqué
para incorporar el cálculo de los promedios. Ahora vale la pena mencionar
que el cálculo de los promedios está dado en el caso de que primero tomo
todas las opiniones y les tomo valor absoluto. Hago esto porque lo que
quiero ver es directamente si el sistema tiende a K*<k>, y como el sistema
tiende a opiniones cuyos valores de T1 y T2 son iguales, entonces
no me afecta meter todo en una misma bolsa. Incluso, si tuviera dudas de
esto, podría armar dos mapas de color, uno para T1 y otro para T2, y ver
que efectivamente en promedio tienden a lo mismo.

---------------------------------------------------------------------------------

04/04/2021

Voy a dejar en posibilidades la idea de graficar dos mapas de colores,
uno para cada tópico. Arranquemos primero con el tema de la entropía.

Se terminó el cálculo de los valores medios, y parece que correctamente
el sistema tiende al valor K*<k>, así que eso va bárbaro.

Sobre la entropía, no llegué a armarlo hoy. Cuestión que lo pensé,
la idea va a ser armar un array de Probabilidades, que lo podés
tomar del Yhisto, ese que usas para graficar la distribución por
cuadrantes. Luego, a Probabilidades le tomás el np.log2() cosa
de que te quede el log2 de cada uno de los elementos. Luego haces
un dot product, supongo, con el array de Probabilidades y a eso lo
multiplicás por -1 y listo. Estás hecho.

Eso te va a dar la entropía para un conjunto de valores Alfa y Cdelta.
Luego, le asignás eso a la matriz ZZE, y por último lo graficás en el 
mapa de colores con un pcolormesh.

----------------------------------------------------------------------------------

05/04/2021

Bueno, estoy graficando el tema de la entropía ahora. Hice unas modificaciones
que son las siguientes.

En el tema de graficación de la varianza de los tiempos de Simulación, le tomé
logaritmo en base diez a los valores para graficarlo en términos de números más
sencillos de visualizar. Para eso, al valor de la varianza graficada le sume
1 primero ya que algunos valores de Varianza estaban muy cerca de cero y entonces
el logaritmo hubiera cadía a valores muy negativos y no hubieramos ganado nada.

Sobre la entropía, hice tal cual lo que escribí más arriba. Usé el np.matmul
para hacer el producto de los arrays y desde ahí lo ubiqué en la matriz
ZZE. Al final copié el código de graficación de Promedios para graficar
esta nueva matriz y listo.

Estuve mirando los gráficos que tengo, increíblemente no hice un gráfico de
fases, ese tengo que hacerlo. También tengo que hacer los gráficos recopilación
del resto de gráficos, eso lo puedo hacer con Python en un pedo. Eso es lo que
necesito para mostrar mañana.

------------------------------------------------------------------------------------

19/04/2021

En estas dos semanas estuve entre armando el informe, tomando más datos y
replicando los gráficos que tengo para N=1000. Hoy aprovecho para decir
que había armado un nuevo programa .py que el objetivo es que armara
directamente la tabla de gráficos en vez de tomar imágenes y cargarlas.
El problema es que el gráfico no se pudo hacer todo, así que hay dos
opciones, dejarlo para cuando lo pasemos a las máquinas, o no volver a usarlo
nunca más. Lo que se me ocurre es primero volver a probarlo, esta vez
con una tabla de 2x2, a ver si eso me lo corre. Confío en que sí.

Ahora estoy viendo de hacer los gráficos que necesito para la tabla
de TdO, y después haré los gráficos de Variaciones Promedio.
Todavía faltaría entonces hacer lo de generar datos usando
una semilla fijada para ver si por casualidad vuelve a surgir
un dato absurdamente largo en comparación con el resto.

-------------------------------------------------------------------------------------

20/04/2021

Ya armé el gráfico de Variaciones Promedio para Alfa=0,9 y Cdelta=0,6
usando las 100 iteraciones que calculé. Por lo visto, sigue habiendo
una única instancia que se separa mucho del sistema. Igualmente, por
si acaso, voy a realizar otras 100 iteraciones, pero esta vez definiendo
la semilla random a partir de un número entero que es el número de iteración.
La idea es ver si vuelve a surgir este extraño caso en el cual el sistema
tarda una cantidad absurdamente grande de tiempo en converger.

Mientras esto se resuelve, podría intentar revisar la librería de 
redes en Python, la nx.network, o algo así. La idea es ver de poder construir
redes en Python con ciertas propiedades, guardar las redes de adyacencia
en alguna carpeta y usar esas redes para armar las redes en C a la hora
de trabajar. Espero que funcione