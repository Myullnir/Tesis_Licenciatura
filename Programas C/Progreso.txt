Código para generar .e a partir de los .c. Importante para el Prosem.c

"gcc -Wall -O3 -o miprog.e miprog.c -lm"

Para correr el programa el código es: "./miprog.e"

---------------------------------------------------------------------------------------

19/11/2020

Hoy voy a intentar revisar entre todos mis archivos viejos para organizarme todo lo que voy
a necesitar para empezar a trabajar. Tengo que revisar los .c, los .h, ver cómo funciona
el make file, cómo funciona el Ejecutar.sh, el Instanciar.sh.


---------------------------------------------------------------------------------------

25/11/2020

Voy a empezar a programar algunas boludeces, así ya voy de a poco recuperando el tacto con
esto. Después debería armarme un diagrama del orden de cosas para programar, así como la
jerarquía en que voy a plantear las funciones. Quizás convenga redefinir los nombres de
los archivos tipo avanzar, general y esos.

-----------------------------------------------------------------------------------------

26/11/2020

Luego de batallar con arrancar, ya estamos empezando. Me armé un diagrama de trabajo y una
lista de las cosas que tengo que poner en el programa. El diagrama lo tengo en una hoja
escrito a mano. Podría pasarlo a un power Point, ya veré si vale la pena.
Modifiqué el struct base, el llamado Red, para que tenga los elementos básicos que va a
necesitar el modelo. Tiene número de agentes, de tópicos, matriz de superposición, matriz de
adyacencia, vectores de opinión y una serie de parámetros más.
La próxima arrancamos con los elementos en inicializar. La función de Visualizar queda.
Creo que voy a borrar CCP y la otra la voy a modificar para que me arme la red. Arranquemos
con una inicialización fija, después planteamos que el sistema vaya variando los valores
de la matriz de superposición.

-----------------------------------------------------------------------------------------

27/11/2020

Bien, ya modifiqué el archivo de inicializar. Saqué el CCP que colocaba condiciones de
contorno cerradas. Esas no las voy a necesitar, porque al final mi sistema es una red
finita con N nodos, no hay condiciones de contorno que cumplir. Luego, el GenerarR lo adapte
para que me inicialice los vectores de opiniones. Lo probé, parece funcionar perfecto. Por
lo menos asigna números con fracción a todas las coordenadas correctamente. Pareciera
respetar el intervalo de valores. Le cambié el nombre al GenerarR y lo llamé GenerarOpi.

También modifiqué el programa de Visualizar, para que al final reciba un número que indique
qué vector es el que planeo mirar. Las opciones son "Vectores de Opiniones", "Matriz de
Superposición" y "Matriz de Adyacencia".

Además separé el struct original de Red en dos structs. El struct Red que contiene la matriz
de Superposición, la matriz de Adyacencia y la lista de vectores de opinión. El segundo
struct es Parametros, que contiene todos los parámetros del modelo.

Lo siguiente es armar dos funciones que me inicialicen la matriz de Superposición y la de
Adyacencia. De paso, se me acaba de ocurrir. Debería probar el numerar vectores con la 
notación usual de fila y columna "[i,j]", trabajándolos como matrices, en vez de trabajarlos
como vectores de una sola fila y muchas columnas, donde hago cosas como "[i*N+j]" para
designar los casilleros del vector.
Cuestión, cuando tenga todas las funciones de inicialización las voy a empaquetar en una
sola función, para que quede todo más prolijo. O quizás vea de encerrarlo en una pestaña.

Corte a las 9:40, me pareció que no valía matarse ahora. La dificultad ahora es cómo
escribir una matriz simétrica, considerando que yo las tengo armadas como una tira y no
como una matriz. Estoy seguro que alguna vuelta sencilla tiene que haber. Una propuesta que
tengo para los elementos diagonales y no tener que meter muchos if es armar todos los
elementos con algún criterio, (Armar una matriz random en cada instanciación no suena como
una buena idea.) y DESPUÉS reemplazar todos los elementos de la diagonal por 1.

-----------------------------------------------------------------------------------------

28/11/2020

Armé las funciones que estuve mencionando antes. Ya tengo GenerarOpi que me arma la lista
de vectores de Opinión de todos los agentes. También está GenerarAng, que me crea la mitad
de la matriz de Superposición. La otra mitad la voy a construir una vez que tenga armada
una función que me simetrice matrices. Esa función la voy a meter dentro del conjunto de
funciones generales, podría servir en un futuro. La situación es la misma con GenerarAdy. Me
genera la mitad de la matriz de Adyacencia, la otra la voy a construir por simetría.

Estuve probando el Visualizar. Algo raro le está pasando, pero empecemos aclarando que me
visualiza perfecto las matrices de Adyacencia  y Superposición. Pero por alguna razón viene
tirando un error con la matriz de Opiniones. Creo que el error es puramente en la 
visualización, que no tiene nada que ver con lo que realmente hay en la matriz. Me percaté
de esto para empezar porque observaba que los elementos de la última fila de esta matriz
cuando formaba el sistema con seis agentes eran todos cero. Algo estadísticamente improbable.
Cuestión, que dependiendo de la cantidad de filas los elementos de las últimas filas se ven
peor o mejor. Noté que si cambiaba a muchas filas y columnas, de repente todo se emparejaba.
También descarté que el problema sea el casteo o el armado del Vector. El casteo no es porque
pasé el vector a Double, que es la salida del Random, y sigue funcionando mal. El armado del
vector no es porque probé ponerle enteros, y sigue dando el error de que al visualizarlo me
muestra algo que no tiene sentido, como ceros en las últimas filas o números absurdamente
largos e inexplicables. Y estos números largos eran siempre los mismos, no cambiaban en un solo
número. Algo raro pasa ahi y todavía no descubrí qué es lo que está pasando. Para comprobar que
el vector está bien rellenado podrías armar una sumatoria con sus números y ver que den algo
correcto.

Ahora voy a armar una carpeta en Github para cargar todo esto. Yo diría que mañana vayas
haciendo pruebas con lo de sumatorias para verificar que el vector se llena de manera correcta.
Luego, empezá a revisar la visualización para ver qué clase de error puede estar teniendo. Al
final del día igual no es vital, pero bueno. Visualizar las redes puede facilitar el encontrar
errores.

-------------------------------------------------------------------------------------------

30/11/2020

Encontré el error. Al final el problema efectivamente estaba en la visualización. La culpa era
mía, obviamente. Yo le calculé mal el movimiento a lo largo de las filas. En pocas palabras, el
sujeto estaba yendo a mirar en posiciones de memoria que no le pertenecían al vector. Eso hizo
que visualizara cosas raras. Porque eran espacios de memoria que estaban ocupados por cosas
raras. De paso, le agregué a la visualización que se muestren sólo dos decimales después de
la coma. Para que sea más sencillo de ver.

Lo pensé mejor, voy a escribir el código de simetrización dentro de las mismas funciones de 
Generar*. Queda para el futuro armar funciones que tomen punteros, me midan el tamaño de los
punteros y desde ahí me simetricen la matriz. Igual, el proceso de simetrización es una línea.
Genial, ya se me simetrizan ambas matrices.

Bueno, armé dos funciones en los archivos de avanzar. Pero todavía no puedo decir que funcionen
bien. Probé hacer las cuentas. En el caso de un nodo sin conexión a nada, dan bárbaro las
cuentas. En el caso de que se conecte con alguien, ya no da tan bien. Todavía no descubro
porqué.

También me ocurrió algo inentendible para mi. En el struct me surgió un error incomprensible.
Agregué el diferencial de tiempo al struct, y lo puse junto a los otros float, por una cuestión
de orden, para agrupar los mismos tipos de datos. Cuestión que por alguna razón que no
comprendo, la posición de este elemento en el struct jodía todo el programa. Pero al cambiarlo
de lugar y llevarlo al fondo del struct, todo se organizaba y funcionaba bárbaro. Cosa que no
tiene sentido, porque el struct llama a sus elementos por nombre, no por posición. El orden
no debería importar. No sé porqué, pero alto error raro.

-------------------------------------------------------------------------------------------

05/12/2020

Por lo que leí en internet, no hay una forma en C de poder leer el tamaño del array al cual 
apunta el puntero. Pero lo que podemos hacer es un truco, y esto implica un cambio en la
forma de programar de ahora para SIEMPRE. Lo que vamos a hacer es que de ahora en más, al 
principio de cada vector los primeros dos números van a estar reservados para el tamaño
de la matriz. (Si es un vector, igual lo consideraré como matriz). De esta manera, siempre
podré tener acceso al tamaño de un array. Esto va a ser un cambio que voy a implementar mañana.

Ahora me voy a poner a jugar con los structs y cosas en Prosem.c. Quiero ver si puedo armar
funciones a las que les llegan punteros y de esa manera en vez de tener que pasar todo el struct,
le paso sólo el puntero correcto. ¿Porqué querrías volver a trabajar sólo con punteros, en vez de
usar el struct que es mejor? El tema es que los nombres de los elementos de los structs va
variando con cada trabajo, entonces una función como la de simetrizar matrices no puede trabajar
con recibir el struct, porque lo que yo voy a querer es que funcione para cada matriz que 
le paso independiente del nombre, pero al usar el struct tengo que especificar el nombre
del objeto, y eso me obligaría a crear una matriz de Simetrización para cada nuevo trabajo.

Veamos si podemos hacer algo con esto. Ok, probando en Prosem, cambiar el orden de los atributos
del struct Parametros no parece crear problemas. Lo cual es lo razonable, porque no los llamé
por ningún orden, los llamé por nombre. El motivo del error en la función main original 
sigue sin ser descubierto, mucho menos solucionado.

Ahora probemos a partir de una matriz inicial el pasarle a una función de visualización básica
el que me visualice mi matriz a partir de un puntero. Armé una visualización tranca haciendo
uso de esa idea de poner el tamaño de la matriz al principio. Está bueno el truco este.
Alguien mencionó también dejar un número para identificar el tipo de dato. Esa para el
futuro queda. No quiero sumar números al pedo, para no volverme loco en la definición
de valores y cosas.

Armé la función de visualización. Efectivamente, si defino el input como un puntero, y le
paso el puntero del struct entonces lo trabaja perfectamente. En este caso le pasé el
puntero red.Ady, es decir el que apunta a la matriz de Adyacencia de mi sistema, y me
lo graficó correctamente.

------------------------------------------------------------------------------------------

06/12/2020

Hoy lo que voy a hacer es un reformateo a todo incorporando nuevas prácticas de programación.
Estas consisten en:

.) Agregar una letra al nombre de una variable que defina el tipo de variable que es.
Esto lo separo del nombre de la función con un guión bajo. Los espacios en los
nombres los voy a separar con guiones bajos también. Es importante recordar
entonces que las primeras letras SIEMPRE indican el tipo de variable.

.) Voy a dejar espacios libres entre secciones de código, separando la parte de
inicializar, desarrollo, registro de datos y demás. También vamos a poner títulos.

.) En las primeras dos coordenadas de cada vector voy a colocar el tamaño del vector.
Siendo la primer coordenada las Filas y la segunda las Columnas.

Si sobra tiempo, armaré una función que haga un RK4. Aunque primero necesito una que
calcule correctamente el campo que define mi ecuación diferencial. Y estas funciones
necesitan ser probadas primero en Prosem para poder hacer las cuentas a mano y ver
que efectivamente hacen lo esperado.

##### Ya reformatee general.c y general.h ##########

##### Ya reformatee inicializar.c e inicializar.h ##########

Llegué al punto donde la función hace lo que ya hacía antes de empezar a formattear todo.
Mañana seguiré con esto. La idea es entonces primero armar la función de la ecuación
dinámica. Probarla que calcule correctamente. Hecho esto, pasamos a armar un RK4.
Es decir, mañana arrancamos laburando con el Prosem. Hecho esto, ya vamos a poder
poner el programa a evolucionar. Lo que va a quedar es registrar los datos y pasarlos
a Python.

Vamos a terminar el día de hoy subiendo todo lo correspondiente a Github. Vale
aclarar, los archivos avanzar los dejé afuera de la carpeta src, porque sino
al compilar el make all me los intenta levantar, y a esos todavía no los corregí.
Más que nada porque las funciones de la dinámica no están terminadas.

-----------------------------------------------------------------------------------------

07/12/2020

Corregí los nombres una vez más, porque ahora a los punteros les agregué la letra p.

Además, ya armé la función Din1 y la probé en el archivo Prosem. Ya lo testee y funciona,
tanto en el caso en que el sujeto no conecta con nadie como en el caso en que conecta con
alguien, la cuenta que hace es correcta. Estaría bueno conseguir un programa que me permita
hacer las cuentas con mayor facilidad, porque la verdad es una paja hacerlo en Geogebra.
Creo que voy a hacer las cuentas en Octave de ahora en más. Va a ser mejor, me puede hacer
los productos de matrices y eso va a ser mucho más rápido.

Voy a subir todo a Github ahora.

------------------------------------------------------------------------------------------

08/12/2020

Ya armé las funciones Din1 y Din2. Confío en que funcionan perfecto porque revisé las
cuentas yo mismo usando Octave. También, como para dejar constancia del test realizado
es que guardé una imagen que se llama Cáculos Din2. Es una imagen que tiene los
resultados obtenidos por el programa en C y las cuentas hechas con el Octave, donde
se puede ver que las pendientes calculadas en Octave coinciden totalmente con las 
calculadas en C. Para mayor claridad paso a describir qué son las matrices definidas
en Octave:

- Opi es la matriz de vectores de opinión transpuesto. El motivo de hacerlo transpuesto
es porque de esta manera el producto con la matriz Ang me daría en cada elemento la
suma de las opiniones de un agente en cada tópico ponderada por la superposición con
el tópico en cuestión. El hecho de usar la matriz de forma diferente a como lo hago en
C no implica ningún error de cuentas. No hay que olvidar que en C yo no tengo Matrices,
solamente tengo vectores muy largos. Por tanto el producto entre matrices y vectores
siempre lo termino armando yo. Y eso ya lo había revisado de que estaba bien.

- Ang es la matriz de superposición de tópicos. Por simplicidad, tiene unos en la
diagonal y 0.5 afuera.

- Sup es el producto de las matrices Ang y Opi. Como dije, contiene en cada elemento
la sumatoria de las opiniones de un cierto agente ponderadas según la superposición
de cada opinión con el tópico en consideración.

- Pendiente es el nómbre del resultado total del miembro de la derecha de la
ecuación diferencial.

- K es el parámetro homónimo de la ecuación diferencial. Representa la influencia social.

- alfa es el parámetro homónimo de la ecuación diferencial. Representa la
controversialidad de un tópico.

Ahora el chiste va a ser implementar el RK4. Estuve pensando en que el RK4 debería
tomar aparte de los structs, el puntero sobre el cual va a trabajar, el puntero
a la función que define el campo de mi problema, (En este caso sería Din2),
y algo interesante sería que tuviera un puntero propio dentro de la función.
Ese puntero debería guardar información sobre el estado inicial del puntero 
con el que voy a trabajar. Esa información va a ser necesaria durante el trabajo
RK4, pero terminada la función, se lo puede liberar y listo.

----------------------------------------------------------------------------------------

14/12/2020

Vamos a armar una función que realice el proceso de RK4. Mi idea es escribir una
función que sea bastante general. Para eso voy a tomar como referencia una versión
que encontré por internet. Todavía no tengo bien claras las ideas de cómo voy a
hacer funcionar la versión del RK4. Mi idea es que tome los structs y le pase
eso a la función. Por eso es que voy a estar dentro del RK4 trabajando la Matriz
de Opinión, definiéndola y redefiniéndola muchas veces.
Estaba con la duda de cómo guardar o manejar los datos correctamente, pero al
final llegué a la conclusión de que lo mejor sería tener vectores, definidos
por punteros como siempre, que me guarden los valores de las pendientes que
calculo para cada una de mis variables. Luego al final, luego de haber calculado
cada una de las pendientes, entonces sí uso todas las pendientes para calcular
el siguiente paso temporal del sistema.
Lo interesante para esto es que me di cuenta que iba a tener que hacer cada
cálculo de pendientes en orden, entonces tenía que escribir las 4 sumatorias.
Pero para simplificar la escritura de todas las sumatorias, se me ocurrió
usar un array de punteros. Por eso decidí probar el cómo usarlo en Prosem.
Ahí lo hice funcionar para que me genere la visualización de mi red.
Funciona bárbaro. Tengo que recordar no borrar eso. Voy a tener que
armar un protocolo de guardado de estas cosas. Mañana o en la semana,
lo próximo a hacer es ya implementar esto. Con esto ya básicamente tendría
armada la función del RK4. La paja va a ser comprobar que las cuentas están
bien hechas. Voy a estar un rato con el Octave mirando eso de seguro.

------------------------------------------------------------------------------------------

18/12/2020

Hoy avancé con la construcción de la función RK4. Hay varias cosas para 
discutir al respecto. Pero en términos de lo que quiero que haga, ya la
armé. Todavía no la probé ni revisé errores de tipeo, así que todavía
queda mucho hasta que esté lista. Además, todadvía estoy indeciso sobre
el formato de la función. Debería analizarlo un poco más, como para estar
seguro de que la función es perfectamente generalizable. Cosa que sería
de mucha utilidad.

Por otra parte, rearmé la función de visualizar, de manera de que sólo
tome un puntero de entrada y de eso haga la visualización. Para eso,
hice tres funciones, una que toma enteros, otra que toma floats y una
que toma doubles. Mi idea sería armar una función visualizar Global
que las encierre, que reciba un char que elija la función correcta 
y de ahí visualice. Aunque ahora que lo pienso, eso seguiría
teniendo el mismo problema de que no puedo pasar el vector que 
quiero a la función. ¿Habrá una forma de solucionar esto en el
futuro?

Cree una segunda función que es Duplicar, esa función me calcula
las pendientes de una ecuación diferencial que definí para probar
el RK4. La ecuación diferencial sería x' = 2x.

Volviendo al RK4, estoy empezando a usar más los arrays. También
introduje un array de chars, que eso también es nuevo para mi.
Hay muchas cosas, como los arrays, el meter un for para el free,
o el visualizar que podrían funcionar mal. Va a ver que mirar
todo eso la próxima vez. La próxima entonces hay que hacer
las cuentas y ver que el RK4 esté calculando bien.
Y después, repensar un poco la forma de que sea lo más general
posible. Quizás hacer unos gráficos escritos.

Pregunta importante, ¿Cómo guardar las distintas versiones de
cosas probadas en el Prosem? Podríamos hacer un archivo
llamado Archivo.c. Ahí voy guardando las cosas con descripción.
Así no me van ocupando espacio innecesario en mi archivo Prosem.
Me parece una idea genial.

------------------------------------------------------------------------------------------

21/12/2020

Estuve haciendo las pruebas del RK4 para hacerlo funcionar.
Increíblemente, pero en cuanto puse la función funco bastante
fácil, no tuvo tantos errores como esperaba.

Cuestión, hice las pruebas con ecuaciones diferenciales lineales
en las cuales la variación de mi variable estaba sujeta a
un múltiplo de su valor actual. Es decir, usé las ecuaciones
x'=2x y x'=5x. En la primera anoté los k manualmente y
comprobé que los cálculos estuvieran dando bien los k y
el valor final del sistema. Además, como conozco la solución
a estas ecuaciones puedo comprobar que tan lejos está lo
calculado con respecto a lo real. Elegí los valores de
dt y la constante que definen mi ecuación diferencial
de manera que evolucionar temporalmente un paso a mi sistema
sea equivalente a tomar los datos iniciales y multiplicarlos
por e en ambas ecuaciones.

Guardé una imagen que se llama cálculos RK4 en la cual se
puede ver a la derecha los resultados obtenidos por el 
programa RK4. Luego a la izquierda uso el Octave para
manualmente hacer los cálculos que el programa RK4 debería
estar haciendo. En este caso estoy evaluando el sistema
en el cual la ecuación diferencial es x'=5x. A continuación
está el paso a paso de las cuentas que hice:

.) Calculo k1 como los valores del sistema en tiempo t
multiplicados por 5.
.) Calculo k2 como los valores del sistema en tiempo t
sumados a k1 por dt/2 y todo eso multiplicado por 5
.) Calculo k3 como los valores del sistema en tiempo t
sumados a k2 por dt/2 y todo eso multiplicado por 5
.) Calculo k4 como los valores del sistema en tiempo t
sumados a k3 por dt y todo eso multiplicado por 5
.) Calculo el final como los valores del sistema en
tiempo t más el producto de dt sobre 6 con 
k1 más dos k2 más dos k3 más k4.
.) Corroboro el resultado de tomar los valores del
sistema a tiempo t y multiplicarlos por e.

Finalmente cabe notar que hay una diferencia en la 
segunda cifra decimal, lo cual marcaría que debería tomar
un avance temporal más chico del que tomé. Pero en conclusión,
el programa funciona perfecto. Me encanta. Sólo queda
pensar si se la puede hacer un poco más genérica y listo.

Ahora voy a incorporar esto al programa principal. Todavía no
terminé esto. Mañana lo primero que tengo que hacer es corregir
los struct que aparecen en todo el código y reemplazarlos
por los s_cosas. Con el tiempo se me ocurrirán formas de
hacer que eso funcione más fácil

----------------------------------------------------------------------------------

22/12/2020

Ya implementé el RK4 en el programa, funca bárbaro. Hice
las anotaciones sobre el bloque que creo es crucial para
la generalización de esta función en futuros trabajos.

Hice pruebas, el programa funciona muy bien, no tira
errores ni nada. Por ahora lo único que probé a variar
son los valores de K y los de la matriz de Superposición.
Los resultados son los siguientes:

.) Si los valores de la matriz de Superposición son todos
positivos, entonces todos los signos de los tópicos se
alinean.
.) Si los signos de los valores de la matriz de Superposición
se alternan, entonces lo que me ocurre es que los signos
de las columnas se alternan igualmente.
.) Si reduzco el valor de K a cero, el sistema tiende a cero.
.) Si aumento el valor de K el sistema al evolucionar
simplemente tiende a crecer.
.) Con un valor de Tiempo de integración = 20, el sistema
con K=0 logra llegar a un estado donde todos los agentes
tienen opinión nula en todos los tópicos.

Ahora lo que voy a hacer es cortar acá, mostrarle esto
a Pablo mañana y de ahí decidir qué vamos a hacer.
Si yo estuviera por mi cuenta y tuviera que decidir,
¿Cuál sería mi siguiente paso?

Razonablemente, yo diría que por ahora lo que tengo
entonces son sólo dos fotos, la inicial y la final
y que este fue un análisis muy cualitativo. Para
poder empezar a hacer un análisis más cuantitativo
voy a necesitar ver lo que ocurre durante el proceso.

Por tanto los siguientes objetivos serían:
1) Implementar una forma de tomar registro de
los datos para poder guardar mis vectores de
opinión.
2) Armar un programa para cargar estos datos a
Python.
3) Hacer un análisis de los datos. Para esto hay
que pensar de qué forma deberían graficarse los datos.
Se me ocurre que el caso de K=0 se pueden graficar
curvas de variación del valor promedio de la opinión
en cada tópico y ver cómo eso cae a cero.
4) Podría graficar el caso de K != 0 considerando la
direccionalidad de los vectores. Sería una forma
cualitativa pero un poco mejor que lo que tengo
ahora de visualizar la evolución del sistema. Me armo
una especie de Histograma donde voy contando cuántas
personas apuntan en cierta dirección, y las
direcciones las defino en base al signo del tópico.
Eso me ayudaría a ver cómo se van moviendo las personas
de una opinión a otra y si la condición inicial del
sistema afecta mucho al resultado final.
5) Otra opción viable es armar un scatter de puntos
de colores, donde cada color representa una opinión
y luego ir acercando los colores cuyas opiniones son
más similares.

-------------------------------------------------------------------------------------

23/12/2020

Hoy tuvimos la reunión con Pablo. Le mostré los resultados
obtenidos. Me aconsejó lo siguiente:

1) Guardar todos los gráficos que vaya realizando en un
powerpoint. De esta manera vamos mejorando la forma de
presentar datos, así como también mi hablidad de
explicarlos y cosas.
2) Arrancar únicamente con 2 tópicos, así podemos
concentrarnos en replicar los resultados del paper de
Baumann.
3) Que en cada iteración el sistema recorrar a los N agentes
de manera ordenada, pero elija una pareja de manera aleatoria
(Esto va a resultar más complicado de lo que parece)

Por si acaso, antes de meterme con esto, resolver primero
el tema de registrar los datos y levantarlos con Python.

------------------------------------------------------------------

27/12/2020

En la carpeta de Aprendiendo C ayer armé un programa de Bash
para compilar archivos y correrlos. Eso lo importé acá,
se llama Compilar.sh. La idea es que para correrlo yo
uso en la línea de comando "./Compilar.sh Nombre_archivo".
El Nombre_archivo va sin .e, eso el programa de Compilar
lo agrega solo. Luego, el programa compila y te da la 
opción de elegir correr o no el .e. La idea es primero
revisar los errores. Si no salta ningún error, apretás
enter y se corre el programa. Si hay errores que corregir,
apretás una letra y después enter y no se corre el programa.

Hoy voy a encargarme de organizar el tema de que haya registros
de los datos. Lo que voy a hacer es armar archivos que guarden
la siguiente info:

.) Primera línea es la matriz de adyacencia.
.) Segunda línea es la matriz de Superposición
.) Tercera línea y en adelante es la matriz de Opiniones en cada
paso temporal.

Si tengo tiempo, me pondré a hacer lo que me propone Pablo sobre
variar la forma en que se elijen personas para realizar las
interacciones.

Se me está ocurriendo armar una función que reciba el puntero
al archivo y una matriz, y que copie toda la matriz. Como para
hacerlo más prolijo, por más que son sólo dos líneas.

Ok, ya logré usar los comandos para armar un archivo en donde
guardar los datos. Lo raro de esto es que por alguna razón no puedo
ponerle la extensión txt al archivo. No comprendo porqué. Igual eso
es lo de menos. Puedo fácilmente usar línea de comando o armar un
programa de Bash que se encargue de eso.

También me armé dos programas, uno para vectores doubles y otro
para vectores ints que me printea los datos. Teniendo esto, y
siendo que lo que me propuso Pablo es algo más trabajoso de hacer,
podría ponerme a trabajar en hacer un archivo de Bash que mueva 
mis archivos de mi carpeta actual a la de Programas Python.
Esto no está resultando tan fácil. Tampoco es tan importante,
lo dejaré para otro momento.

Más tarde, subiré las cosas a Github y armaré un registro de lo
que hice hoy. ¿Qué voy a hacer mañana?

.) Tengo que armar una carpeta en Drive y compartírsela a Pablo
y Sebastián.
.) Ahora que tengo un programa que guarda registro de mis datos,
voy a incorporar esto al main,
.) Cuando tenga al programa armando mis datos, voy a armar un
código de Bash que me los mueva todos a la carpeta de Datos
en la carpeta de Programas de Python.
.) Una vez que tenga los datos armados, voy a empezar a trabajar
en Python para ir viendo de levantar los datos y graficarlos.
.) Hecho todo esto, voy a empezar a trabajar en lo que dijo Pablo
sobre modificar la forma en que el sistema se itera.

--------------------------------------------------------------------------

28/12/2020

Ya agregué las partes de registro de datos al programa, funciona
bien. El tiempo que tarda me preocupa, voy a tener que trabajar
en armar las tablas previamente. Ahora voy a armar un archivo
que me mueva los datos.

Ya armé el programa que me mueve los archivos de datos a la carpeta
de Programas de Python. Despues quizás arme un archivo Bash que
lo levante, así de una al terminar de hacer todo me los mueva.
Posiblemente haga eso en el instanciar. Tipo, al final del for
muevo todo.

Antes de trabajar en Python, voy a anotar algo respecto a lo
que dijo Pablo sobre modificar la forma de iteración del
sistema. Entiendo que lo que Pablo dijo fue permitir que
todos los usuarios interactuen con todos, es decir una
matriz de adyacencia con ceros en la diagonal y unos afuera.
Se me ocurre que podría agregar un segundo agente en el struct
de la red. Luego debería supongo quitar a la matriz de adyacencia
de la ecuación y hacer que la cuenta se efectúe mirando a un
agente particular.
Ahora que lo pienso, el Din1 está calculando las tanh para TODOS
los agentes, incluso los no conectados. Eso es una pérdida de
tiempo abismal e innecesario. Podría meter un if en la función
Din1, pero creo que no sería lo óptimo. Lo mejor que se me
ocurre es armar un array de punteros a funciones. Que ese
array en la posición cero tenga una función que returnea un
cero, y en la posición 1 tenga a la función Din1. Ese array
lo creo en la función Din2, y eso sería mejor que tener un
if, porque no hay comparación involucrada. Y debería correr
más rápido el programa. Me gusta la idea. Ahora sí,
trabajemos en Python.

Ya logré armar un programa que levanta los datos de los archivos
y me genera un gráfico de la evolución de las opiniones. Lo que
grafica es el promedio del cuadrado de la opinión de cada uno 
de los agentes, y esto lo hace para todos los tópicos.
Este gráfico no sé si tendría mucho sentido para graficar
sistemas con K distinto de cero. Pero lo importante es que
el esqueleto está, modificar esto no sería tan difícil.

Lo interesante va a ser ver cómo graficar el caso de K distinto
de cero. Se me ocurre que puedo simplemente marcar con puntos o
líneas muy chicas las trayectorias en un plano 2D. Habrá que ver
cómo sale eso.

¿Con qué debería seguir? Ahora definitivamente es una buena idea
seguir con las propuestas de Pablo. Estos son los objetivos:

.) Modificar la iteración de manera de que cada agente se conecte
con otro agente aleatorio. (¿Cómo verifico que esto se cumpla bien?)
.) Modificar la función Din1 para que trabaje con un sujeto en vez
de con todos. (Asegurarse de guardar esta versión del programa
en algún lado, porque esta era la original. Esta era la buena.
Quizás simplemente lo deje comentado y listo.)
.) Corregir esa pila de cálculos innecesarios de las tanh que
después se mueren. Probar primero con un if. Y si eso no
es un gran progreso, probar la idea del puntero a dos funciones.
.) Armar las tablas de cálculo previo de las tanh y las funciones
de interpolación, para reducir el tiempo de cálculo de las tanh.
.) A futuro, cuando haya leído sobre el uso de archivos en el Tutorial
de C, ver de la posibilidad de armar una tabla una sola vez, y luego
cargarla, para deshacerme definitivamente de esa pérdida de tiempo.
.) Modificar el instanciar.sh para empezar a realizar muchas corridas
juntas. Agregarle al fondo el mover.sh. Eso es genial, me encanta.
.) Cuando esté consiguiendo cantidades ingentes de datos en la carpeta
de programas, ahí voy a tener que empezar a generalizar el funcionamiento
de mi programa de Python.

Mañana tengo que subir las nuevas versiones de programas de C y
de Python al Github.

----------------------------------------------------------------------------------

29/12/2020

Hoy lo que hice fue ver toda una sección de loops y decisiones en
el tutorial de C de AticleWorld. Aprendí un poco sobre los breaks
y el goto. Y sobre el continue.

-----------------------------------------------------------------------------------

30/12/2020

Ok, estoy pensando en hacer lo de la modificación al programa para que 
la iteración cumpla con la idea que propuso Pablo de agregar un elemento
random a la forma en que evoluciona el sistema. Se me ocurre que es
un buen momento para modificar el RK4 de manera de que se vuelva
incluso más general. Voy a sacar las iteraciones en agentes y tópicos
por fuera de la función.

Lo primero que debo hacer para hacer esto es guardar el programa que
tengo hasta ahora, para no perder nada de lo que ya funciona. Para
esto voy a empezar a usar la carpeta de Programas, ahí me voy a guardar
los archivos de esta versión del programa. Aunque voy a tener que usar
una carpeta, ya que esta versión que tengo del programa no va a funcionar
con la nueva versión del RK4, por lo que también va a necesitar su
correspondiente archivo general y demás.

Mañana arranco con las modificaciones. Si me pongo las pilas, ya mañana
tengo modificado el programa para que la iteración no sea ordenada.

------------------------------------------------------------------------------------

31/12/2020

Una idea para mis funciones es estandárizar el nombre que le doy a las
variables que registran el número de filas y columnas. A veces las llamo
i_C,i_CF, a veces i_columnas,i_filas, a veces i_Filas, i_Columnas. Tiene
que ser en lo posible siempre el mismo. Después voy a revisar todas mis
funciones y cambiarlo por el más sencillo, i_C e i_F.

Ahí probé la función RK4 modificada. Ahora efectivamente evoluciona a
una sola variable, y calcula todo correctamente. Ahora sólo queda
decidir detalles de si necesito o no armar un vector que me guarde
los datos extra para ir haciendo la comparación y poder decidir cuando
cortar el programa.

Igual lo siguiente a esto ahora es implementar esto en el main y ver
que efectivamente esto itera para cada sujeto. Seguramente armando 
una función Iteración que englobe al RK4.

Lo último que estaba haciendo es modificar la función Din1. El objetivo
es justamente modificar la selección del segundo agente de interacción
para volverla random. Estaba trabajando en eso armando una función
que toma un número y un rango de valores, crea un vector con todos
los números en el rango menos el que tomo como input. Luego genera
un número random menor al tamaño del vector, va a la casilla elegida
del vector y toma ese número. La función la estaba armando para el
caso de números ints.

Voy a tomar la siguiente regla de trabajo. Es cierto que soy muy
celoso con el tiempo de trabajo de mis funciones, pero voy a dejar
estos temas de optimización de código para más adelante. Por si acaso,
si pensás que te vas a ir olvidando de esto en el futuro, hagamos un
archivo llamado optimización donde guardemos las ideas de qué cosas
necesitan optimizarse, así cuando tengamos tiempo con el código y
no sepamos qué hacer, podemos invertir el tiempo en mejorar el código
y otro día resolvemos esos problemas grandes.

De paso, mañana cuando arranques con el programa de nuevo, acordate
de comentar la función esta de generar números aleatorios, sino te va 
a tirar altos errores.

-----------------------------------------------------------

01/01/2021

Modifiqué la función Din1 para que no tome al segundo agente como un
input, sino que el segundo agente sea un elemento dentro de los structs.
Luego armé al segundo agente dentro del struct de la red, que es i_agente2.

A la función Din2 le saqué el for que hacía que revisara todos los
agentes con los cuales el primer agente estaba conectado. Esto es para que
las interacciones sean únicamente de a pares.

Agregué en la iteración del main los elementos necesarios para definir
el segundo agente y para que el sistema itere en los tópicos y los agentes.

A la función RK4 le saqué todo el mambo de vectores k donde calculaba
el k para cada elemento del vector de opiniones y evolucionaba
todo el sistema junto, y lo reemplacé por un sólo vector el
cual contenía los valores de k0 hasta k4. El k0 es un extra
que agregué yo para generalizar el cálculo de los k y poder
hacerlos todos en un sólo loop, en vez de tener que escribir
4 acciones separadas para calcularlos.

Todavía no pasé esto al programa principal, todo esto fue en
el programa de pruebas.Así que todavía tengo eso por hacer.


Algo interesante que me llamó la atención. El sistema funcionando
de esta manera converge a un punto en el cual las opiniones
de los tópicos valen K o -K. El motivo de esto es que hay dos
términos en conflicto, el primero de -x y el segundo que tiene
K * tanh(...). El tema es el siguiente, como ahora no estoy
haciendo una sumatoria en todos los sujetos con los cuales mi
agente principal tiene conexiones, sino que hago interacciones
de a pares, mi segundo término es básicamente K*tanh(...).
Con un alfa suficientemente alto, la tanh cappea rápido,
por lo que se puede aproximar por 1. Esto me deja la
ecuación diferencial como -x+K*sgn(x). Esto es lo que lleva
a que el punto de equilibrio para todas las opiniones se
encuentre en el valor de K o -K. Antes esto no resultaba
tan evidente porque cada sujeto tenía una cantidad de
conexiones aleatoria, entonces me quedaba en el segundo
término K * sum(vecinos). Entonces cada agente tendía a
un valor que dependía de la cantidad de vecinos. 
De nuevo, como ahora las interacciones son de a pares,
todos tienen 1 solo vecino y por eso es que esto ahora
resulta tan evidente. Está copado poder observarlo.

Ya pasé todo al archivo principal, ahora funciona con este
sistema de interacción por pares. ¿Qué debería hacer mañana?

Los objetivos de cosas por hacer que charle con Pablo se 
pueden resumir en:

.) Optimizar más el código. Todo eso está anotado en el
archivo de Optimización.txt.
.) Modificar el archivo de instanciación de Bash para que
pueda ir tomando valores como input en la línea de comando
y que pueda correr muchas veces. Eso no es una gran
urgencia.
.) Mejorar el archivo de Python para que cuando esté con
grandes cantidades de datos poder levantarlas y graficarlas
mejor. Eso surgirá cuando esté graficando.

En principio, tengo una buena parte del trabajo ya hecho,
y eso que hoy es primero de Enero. Recordá que el gran
objetivo propuesto por Pablo es arrancar Febrero con los
dos gráficos de Baumann ya realizados. El de evolución
del sistema y el de los resultados según combinaciones 
de Delta y alfa.

Para cumplir esto también es importante revisar si Baumann
tiene algún código especial que use para hacer los gráficos.

Mañana voy a tomármelo libre. Lo que voy a hacer es armar
un archivo donde me anoto los objetivos Día a día.
Como para no tener que andar moviéndome para arriba cada vez
que necesito ir recordando qué hacer.

Algo que me estoy olvidando es de que debería armar un
mecanismo de corte automático del programa cuando el sistema
llega a un estado estable. Para esto, la idea es calcular como
un error cuadrático y ver la diferencia entre el vector antes
de iterar y después de iterar. Eso es lo siguiente a
implementar. Entonces mañana arranco con eso, y después lo
siguiente ya es empezar a trabajar en Python para armar
los gráficos. Ese debería ser el orden, creo yo. Bueno,
entonces el domingo arrancamos con ese mecanismo de corte.

Lo último que voy a hacer hoy es pasar todo el código del
archivo de pruebas al archivo Archivo.c, como para tener
guardada esta versión. Y debería subir todo a Github.

----------------------------------------------------------------

03/01/2021

Estuve intentando armar algo en el archivo Archivo.c de
manera de que me permita moverme rápido entre elementos.
No encontré un método útil. Después seguiré buscando,
y sino simplemente armaré una carpeta y pondré todo en
archivos separados.

Ahí corregí mis dos funciones, la de Norma y la de
Deltax, que ahora la llamo Delta_Vec_d.
Funcionan perfecto, Delta_Vec_d toma dos vectores
double, los resta y guarda la diferencia en la
tercer coordenada. Norma_d por su parte calcula
correctamente la norma del vector double que le
pases. Así que con esto estoy totalmente en condiciones
de hacer la modificación al programa del main para
que pueda registrar una condición de corte automático.
Igual, antes de armar la condición en el programa, me 
conviene primero calcular datos para un cálculo bien grande,
verificar cuanto tarda el sistema en llegar a un punto estable
y cuánto vale el error cuadrático en ese punto, como para tener
una noción de cuanto debe ser el error cuadrático. Además
ese error va a ser una función del tamaño de mi sistema, así
que estaría bueno ver si podemos hallar alguna relación
entre esas dos magnitudes.

Ahí coloqué esto en el main. Estoy teniendo un error de lo más
raro. Por alguna razón el programa no funciona, ya ni me itera
el vector de opiniones correctamente. No entiendo porqué
sólo está moviendo la primer coordenada, y además la mueve mal.
Es un total sin sentido esto.

---------------------------------------------------------------------

07/01/2021

Ahí empecé a retocar el archivo main que no funcionaba. Saqué los
elementos nuevos en los structs, saqué la función iteración del
main y todo rastro de los nuevos elementos del main. Con nuevos
elementos me refiero a los nuevos punteros y cosas que
puse en los struct que iba a necesitar.

El problema al parecer corría por el programa de iteración,
que lo reemplacé directamente por su código. Lo único que
realmente cambié es que en la iteración del for, en vez de
hacer un PRE incremento, lo pasé a un POST incremento.
Pareciera que esa fue la solución. Ojalá haya sido eso. 
Ahora voy a guardar el programa tal cual está, y voy a empezar
a agregar los elementos nuevos al struct. Espero que esta
vez no se rompa de nuevo.

Bien, ya incorporé todo y no explotó. No sé si funca,
pero por lo menos no explota. Quiero decir, llega al
resultado que llegaba antes, pero todavía no tengo
forma de saber que las funciones están haciendo su
trabajo correctamente. Por eso ahora lo que voy a hacer
es probar en Prosem si puedo abrir dos archivos en simultáneo
y escribir en ambos a la vez.

Ahí lo probé, funciona perfecto. De seguro el error estaba
en que quería usar el mismo puntero para dos archivos
distintos o algo así. Esto está buenísimo. Voy a además
empezar a usar esta notación de ahora en adelante, se siente
re natural.

Perfecto, el archivo ya funciona. Lo próximo que tengo que hacer
ahora es levantar los datos con Python, ver que está todo bien, y
empezar a hacer corridas masivas. Eso no suena muy bien. Me refiero
a correr el programa muchas veces, para muchos valores de N distintos.

Mi objetivo es ver si existe alguna variación de los valores de 
error cuadrático al ir variando el tamaño de la red. Considerando
que al ir aumentando el número de sujetos aumenta también el
número de coordenadas que tienen que restarse y luego sumar
su error cuadrático, yo pensaría que tiene que aumentar. Pero
también es cierto que el error cuadrático que estoy calculando
está normalizado según el número de agentes y tópicos, por tanto
NO debería cambiar con el cambio de número de agentes y tópicos.
Esto es muy importante para poder definir un criterio de corte del
sistema.

¿Que es lo próximo que tengo que hacer entonces?:

.) Levantar los datos en Python y ver que las funciones estén
trabajando correctamente.
.) Modificar el main para que tome datos desde la línea de comando.
.) Modificar el Bash para que las instanciaciones corran el programa
y luego manden todos los datos a la carpeta de C correspondiente.
.) Si queda tiempo y ganas, investigar un poco sobre cómo cambiar el
nombre de mis archivos para agregarles una extensión txt.

Voy a subir todo esto a GitHub

----------------------------------------------------------------------------------

08/01/2021

Ahí subí todo a Github hoy a la mañana. Por accidente apreté enter
y subí los archivos del Source sin un nombre correcto, ni hablar de
que no le puse descripción. En ese archivo se puede ver que ya cambié
todos los i_filas (y las columnas también), y les puse un único nombre
en todas las funciones, para que sea siempre i_F e i_C.
Eso era una tarea de unificación necesaria.

Ahora voy a empezar a trabajar en Python para armar una estructura que
levante los datos y los grafique. Mi objetivo ahora es mirar un poco
cuando se produce el corte en mis datos y ver si puedo armar alguna relación
entre la variable de corte, K y N.

El programa de Python me parece que está bastante prolijo y ahora está
armado para levantar los datos de los archivos y graficarlos. En particular
lo que hace la función es tomar las opiniones de todos los sujetos en
una iteración respecto a un tópico, elevarlas al cuadrado, sumarlas y
normalizarlas según la cantidad de agentes.
Además, por una cuestión de que la leyenda del gráfico no me tape todo,
antes de graficar los datos lo que hago es sumar estos valores al cuadrado
y sumados de todas las opiniones de un tópico sobre todos los tópicos.
De esa manera lo que estoy graficando es una sumatoria de las opiniones
sobre TODOS los tópicos. Repito que esto es para una simplificación de la
graficación por un tema de que al final del día yo espero que todas las
opiniones se estabilicen en algún valor, por lo que como todas convergen
a algo, la suma también va a converger. En este momento perder rastro de
los valores de cada opinión no es un problema porque lo que yo quiero medir
es más o menos cuantos pasos requiere el sistema para alcanzar un punto
de convergencia. Al parecer con 7500 paso le sobra a todos los sistemas.

Es más, cuando lo corra en C para tener bastantes más datos, lo que
podría hacer es reducir el tiempo de corrida a 10000 paso.

Bueno, ¿qué voy a hacer la próxima vez que agarre esto?:

.) Tengo que terminar de armar el cargado de datos de los errores cuadráticos.
Necesito eso para determinar correctamente un punto de corte.
.) Modificar el main para que tome datos desde la línea de comando.
.) Modificar el Bash para que las instanciaciones corran el programa
y luego manden todos los datos a la carpeta de C correspondiente.
.) Si queda tiempo y ganas, investigar un poco sobre cómo cambiar el
nombre de mis archivos para agregarles una extensión txt.

-------------------------------------------------------------------------------

09/01/2021

Ya armé el programa para que tome los archivos de errores y lo grafique.
Fue bastante fácil, total fue hacer una copia casi del código
para graficar los datos de opiniones. Lo raro es lo que estoy viendo en
los datos. Al parecer el sistema primero cae a un primer mínimo local.
Esto se encuentra cerca de las 2500 iteraciones. Hasta este punto, la
evolución parece bastante caótica porque la suma de diferencias cuadráticas
oscila fuertemente. Luego de eso el ruido se reduce de forma considerable
y las diferencias cuadradas comienzan a aumentar hasta llegar a un pico
claramente definido, apenas antes de las 5000 iteraciones. De ahí descienden
las diferencias a cero de manera progresiva y sin ningún ruido.
Es como si el sistema alcanzara un punto crítico en el cual se
decide un claro estado "ganador" que se encarga de absorber a todos los
demás estados. Me lo imagino como que se habían formado ciertas comunidades,
(Cosa rara porque no hay ningún mecanismo de formación de comunidades),
y luego el gran cambio se da porque una comunidad colapsa y es absorbida
por la otra.
Lo interesante es que jamás se me hubiera ocurrido ver esto tomando en cuenta
los gráficos de opiniones. Ese gráfico, con las pérdidas de información
que implica, pareciera sugerir un progresivo avance hacia una dirección
que ya en la iteración 5000 pareciera haberse prácticamente establecido.

Esto es raro, pero bueno, ahora voy a tomar MUCHAS mediciones más
y ver qué pasa. Mi mayor miedo es qué hacer con la leyenda. 
Quizás la ignore. Eso lo pensaré una vez que tenga mis datos.

Primero tengo entonces que modificar el archivo Bash.
También tengo que armar una carpeta separada para guardar mis nuevos
datos. No quiero borrar los que tengo hasta haberlo charlado con
Pablo.

De paso, te comento que por alguna razón no habías realmente
normalizado los errores. Sólo para que lo sepas. Igual quizás
también convenga multiplicarlo por algún número grande,
sólo para que no se vea tan chico.

Armé un nuevo archivo en Prosem para probar un poco el tema de
mandarle números al programa a través de input por línea de comando.
En el mismo archivo está explicado detalladamente cómo funcionan las
variables argc y argv, así como la función strtol que voy a usar para
el programa. Ya mañana lo puedo mandar a correr y sacar muchos
datos.

Lo que sí voy a necesitar es armar una segunda carpeta donde
guardar esos datos. Y modificar el instanciar.sh para que
mande todos los archivos a esa carpeta y no a la ya existente.

----------------------------------------------------------------------------------

10/01/2021

Cosas importantes de hoy. Ya modifiqué el instanciar para que
corra muchos N y muchos K. Es más, ya lo hice correr, así que
eso ya está hecho. Además el programa levanta todos los archivos
con nombre "Datos_*" y los mueve a la carpeta de Datos Corte
dentro de la carpeta de programas Python.

Por otra parte, al parecer el antivirus era lo que impedía
que el comando mv cambie los nombres de los archivos. Así que
modifiqué eso ahora para que los nombres de los archivos movidos
se escriban como txt.
Ahora que lo pienso, si bien puedo hacerlo, eso no necesariamente
es bueno. Creo que voy a seguir trabajando sin eso, porque al final
me va a generar más problemas.

También tuve que poner una nueva función, strtof, que me permite
convertir números en floats. Bah, yo digo que tuve, pero en
realidad lo hice por si acaso porque el número K está definido
como un float, pero en realidad no sé si me hubiera tirado un
error el usar el strtol.

Ahora debería levantar los datos con Python y ver qué me da. 
Eso lo haré mañana seguro.

--------------------------------------------------------------------------------

14/01/2021

Estoy anotando esto con un día de retraso.

Lo que voy a hacer hoy es tomar los archivos de Python y
modificarlos de manera de que me hagan un gráfico de Opiniones
y uno de Errores por cada valor de agentes N, y que en cada
gráfico entren todos los valores de K considerado. La idea es
separar un poco todas las curvas para poder entender mejor qué
está pasando.

Bien, ya logré hacer esto. De paso intenté generalizar el sistema
haciendo que primero reconozca los valores entre los cuales se
mueve N y de ahí el código grafica todo correctamente.
Ahora sería interesante lograr que me guarde los gráficos automáticamente,
así puedo mirarlos mejor y más cómodo en la pc en vez de mirarlos
en el notebook de Python.

Bueno, eso fue sencillo, lo realicé con la función plt.savefig().
Listo, ahora voy a subir estos datos al powerpoint de la tesis,
voy a subir todas las imágenes asociadas a una carpeta
que se llame: "Datos Corte", porque son los datos que
voy a usar para definir el mecanismo de corte.

Ya está todo subido y correctamente documentado en el archivo
de powerpoint de la tesis.

¿Qué es lo que sigue?

1) Debería empezar a pensar un criterio de corte y ponerlo
a prueba con los datos. La idea sería que el sistema me grafique
una línea vertical en mis plots y con eso poder comprobar cuándo
el sistema cortaría las iteraciones según ese criterio.
2) Para lo anterior estaría bueno comprobar si realmente
los valores de ErrCuad están debidamente normalizados o si
varían con los valores de N y K. Lo ideal sería que estén
correctamente normalizados. Eso me garantizaría que puedo
tomar algún valor arbitrario como criterio definitivo
y estar tranquilo de que nunca va a pasar que el sistema
nunca llegue a cumplirlo.
3) Hacer el gráfico de las opiniones para el caso de N=5 
que tiene ese error con el pico raro. Así lo podemos analizar
más en detalle.
4) ¿Podría normalizar los errores cuadráticos usando 
el máximo del error o no? El problema de eso es que
eso tiene sentido una vez que YA tenés los datos de errores
calculados, no me ayudaría a armar un criterio de
corte para el sistema mientras corre

-----------------------------------------------------------------------------------

15/01/2021

Lo primero que hice fue revisar que no faltara nada en el Github.
Luego cloné el repositorio de Github en mi carpeta de SiCoMoDa. La
idea es empezar a usar Github Desktop como una forma más sencilla,
(y parece que sinceramente lo es), de ir comitteando el progreso
de mis archivos. De esta manera no soy yo el que tiene que estar
atento a qué cosas se modificaron, sino que el programa me avisa de
qué se modificó, crea las carpetas necesarias, sube los archivos.
Es realmente más cómodo.

Estuve dándole vueltas al asunto del criterio de corte, todavía no
llegué a un criterio razonable. No me convence del todo la idea de 
pedirle al sistema que corte cuando llega a cero. ¿Porque si nunca
llega a cero que hacemos? ¿Y si cero es una aproximación?

Estoy pensando en una cota variable, que sea una fracción del máximo
de error. De esa manera puedo desentenderme del tamaño de mi sistema
y del valor de la influencia K. En este caso se me ocurre darle un 
período largo, unas 1000 iteraciones, en las cuales el sistema se
encuentre en un intervalo del 10% o del 5% del valor máximo.
La idea sería ver en los valores que tengo cuánto "tiempo" el
sistema se encuentra en ese intervalo. Si el tiempo que se encuentra
ahí es muy grande, es decir que ya en el principio cumple ese criterio,
el criterio es super holgado y no me sirve. Necesito ver que el sistema
comience a cumplir con el criterio que yo establezca en el último tramo
de su evolución.

Por otro lado, para ver si existe una correlación entre K y N y la
cantidad de pasos necesarios para que el sistema deje de evolucionar
es que armé un gráfico que me marca la cantidad de ceros que hay en
el vector de Errores. El motivo de mirar la cantidad de ceros es que en
todos los sistemas una vez que el sistema llega al cero, no vuelve
a levantar cabeza. Entonces el número de cantidad de ceros es de alguna
manera la cantidad de iteraciones demás que hizo el sistema. Si el
sistema tardara más en llegar a un equilibrio para valores de N y K
grandes, entonces yo debería ver una caída en la cantidad de ceros
a medida que K y N crecen.

Creo que estaría bueno hacer cuentas para algunos valores de N más.
Voy a hacer eso y ver si se ve algo claro.

Ok, hice unas cuantas sumas más para valores de N entre 16 y 30. Por lo
que vi no aportan MUCHA más claridad. Pareciera que efectivamente
valores bajos de K llegan a una convergencia total más rápido a medida
que N aumenta. Después "pareciera" que los valores de K más grande
tardan más en llegar a una convergencia, y los valores intermedio
"pareciera" que se organizan de manera que la cantidad de pasos
necesarios para converger totalmente disminuye a medida que disminuye
el K. Pongo comillas al pareciera porque la verdad tampoco es
algo definitivo. El K=1 oscila terriblemente, el K=3 a veces
está por debajo del K=5, a veces por encima del K=1. No es claro.

Después armé un código para marcar sobre los gráficos de Errores_Corte
unas barreras de porcentaje respecto del valor del error máximo.
La idea es visualizar cuántas iteraciones el sistema tarda en reducir
sus errores por debajo del valor máximo, siendo que la mayoría de los
comportamientos de los errores son monótonamente decrecientes y que
por tanto los máximos suelen encontrarse al principio del gráfico.
También sirve para ver cuánto vive el sistema en cierto umbral, de manera
de determinar si el hecho que el sistema atraviese esa barrera es un 
buen criterio de corte o si resulta muy holgado, o muy estricto.

Para la próxima la idea entonces es 
.) Guardar los gráficosde Umbrales_Error. Van a ser MUCHOS gráficos, 
la idea es mirar un poco a ojo que efectivamente una vez que el 
sistema cruza la barrera del 0.5%, por decir un número, se encuentra
a unas 1000 o 2000 iteraciones de terminar. Entonces le podemos
pedir que corte luego de 500 iteraciones una vez cruzado ese
umbral.

.) Para cerciorarme cuántas iteraciones hay de distancia desde que
el sistema cruza la barrera que le digo que cruce, podría hacer alguna
clase de resta entre el índice en el cual cruza la barrera
y el valor en el cual los errores se vuelven cero. Creo que este
gráfico va a ser el definitivo para elegir un criterio. Idealmente,
todos los sistemas tienen un mismo valor de cantidad de iteraciones
desde que cruzan la barrera y eso me permite asegurarme que nunca
corto muy temprano o muy tarde. Realísticamente van a ser números
con algo de suerte no muy dispares. De ahí, si quiero un criterio 
más holgado, puedo tirarme a elegir uno de los números altos,
si quiero un criterio estricto puedo tomar valores de los bajos,
si hay mucha disparidad puedo tomar un promedio, ya veré cuando
tenga el gráfico.

De paso, la idea es que ese gráfico me va a dar una idea
de cuánto tarda el sistema luego de cruzar una cierta barrera
en llegar a una convergencia TOTAL, donde ya no varía más.
Por eso si tomo alguno de los valores más grandes el criterio es
holgado, porque en ese caso lo que va a pasar es que los sistemas
van a por si acaso pasar tiempo sin evolucionar en favor de que
ningún sistema se quede sin llegar a ese estado.

-------------------------------------------------------------------------------------

17/01/2021

Por alguna razón, recién hoy se me ocurrió pensar que quizás, sólo quizás,
estaba guardando mal mis datos. Efectivamente se me estaban guardando los datos
con una precisión de seis decimales. Lo cual es una gran cagada consideranco que
estoy trabajando con doubles, es decir que el programa tiene datos con precisión
de 12 decimales. (Leí que técnicamente son 14, igual yo por si acaso le pido
sólo 12).

Así que lo que hice fue primero probar en un archivo aparte cómo lograr que se
printearan datos con precisión de más de 6 decimales, logré que se printeara
con 12.

Luego, pasé eso al main y me aseguré que los errores y los valores de opinión
tuvieran esa precisión. Aunque siendo sincero, los valores de opinión no necesitan
semejante precisión. Digo, esos números rondan los enteros, ¿Para qué carajos
quiero 12 decimales más de precisión? Pero como digo, ya están calculados,
no es que me esté ahorrando cómputo no anotándolos.

Lo otro que hice, que me siento muy bien por haberle encontrado la vuelta,
es darle un sentido a la variable de Error_Cuad. Hasta ahora era sólo un
número que en la medida que se iba a cero me marcaba que el sistema cambiaba
cada vez menos, pero ni idea de qué representaba. Lo que hice ahora es normalizarlo
usando la definición de norma para que ese número represente la VARIACIÓN PROMEDIO
DE CADA OPINIÓN. De esta manera, si ese número es 0.0001, lo que me dice es que
en promedio TODAS las opiniones variaron eso. Eso es lo que significa ese número
ahora. Por eso decidí llamarlo ahora en el código VarProm, es decir Variación Promedio.

Me doy cuenta que el nombre de los archivos sigue siendo Datos_ErrCuad_... .
Debería cambiarlo para llamarlo variación promedio y reflejar mejor el sentido
de ese valor. Dejaré eso para otro día. Lo bueno es que puedo cambiar eso
sin necesidad de cambiar el código de Python en nada.

Esto hace que mi análisis de la cantidad de pasos que el sistema necesita
para llegar a una convergencia TOTAL sea una total pelotudez. Debí darme
cuenta de la terrible gilada que estaba diciendo.

Ahora, visto y considerado esto, habría que considerar que el criterio sea
un número fijo ahora que la variable representa algo claro del sistema.
Por ejemplo, si propongo que el criterio sea 10^(-6), sería decir
que si las opiniones empiezan a variar en valores menores a la
millonésima parte, entonces considero que el sistema deja de
evolucionar y llegó al estado de equilibrio. Suena bastante
razonable, recordando que mi sistema se mueve en el orden de los
enteros. Es decir, las opiniones en promedio estarían variando 
en 6 órdenes de magnitud menos que sus valores actuales. Por tanto
esa es una variación desreciable. Si el sistema entra en esta región
y no escapa luego de un tiempo, lo podemos cortar y listo.

En lo que queda del día me voy a poner simplemente a ordenar todo,
subir todos los datos correspondientes y listo. Estoy actualizando
programas y archivos de Documentación.

Cosas pasaron y tuve que borrar los archivos de datos que tenía
en la carpeta local, por lo que aproveché para mandar a correr de
nuevo todo el programa. Así que no voy a tener tiempo de subir
las imágenes a la carpeta de Tesis, eso queda para otro día.

¿Qué es lo próximo para hacer?

.) Implementar el sistema de corte en el main. El criterio
seguro sea que la variación atraviese el piso de los
10^(-6) durante unas 500 iteraciones.
.) Mirar en el trabajo de Baumann cómo realizó las imágenes.
Empezar a intentar hacer esas imágenes.
.) Subir las imágenes que tengo al Drive para mantener
registro de todo. (Quizás arranque por esto)
.) Optimizar el código en el tema de la tanh. A esta altura
los tiempos de corrida están empezando a ser molestos, me gustaría
reducirlos.

------------------------------------------------------------------------------

19/01/2021

Ayer subí los gráficos al Drive. Al final borré algunos que tenía de
antes porque ya no eran válidos. Y subí uno de los nuevos de Variación
Promedio de las Opiniones.

Hoy ya armé una función que implementa el criterio de corte. Algo gracioso
es que al armar esta función pude claramente observar la diferencia entre
pasar una variable por copia o por referencia. Al principio estaba pasando
el parámetro de corte por copia. Eso hacía que mi sistema nunca cortase
porque el parámetro de corte nunca variaba, sino que lo que variaba era su copia.

La solución entonces fue pasar el parámetro por referencia, y se solucionó
bárbaro. Creo que ese problema no lo voy a tener en mi sistema, pero bueno,
esto muestra lo importante que es seguir con el tutorial de C. Quiero llegar
a la parte donde habla sobre structs.

Al final decidí pasar el código y no armar una función en el main. El motivo
de esto es que siento que sino estoy pasando MUCHAS veces el struct por copia.
Eso me preocupa un poco. También está el tema de que iba a tener que definir
muchos argumentos para la función, por el hecho de que iba a tener que
pasar los punteros a mis archivos en los cuales estoy anotando los datos.
Para ahorrarme eso, decidí simplemente copiar el código fuente en vez
de pasarlo en una función. En un futuro, cuando decida corregir el pase
por copia a un pase por referencia, quizás convierta todo esto a una función.
O cuando me arme la función que interpola las LUT.

Ahí lo implementé y estuve mirando los números. Tomando un criterio de corte
de 10^(-6), lo que observé es que en las últimas iteraciones el sistema
está variando en la séptima cifra decimal en valores promedio de tres o cuatro.
Es decir, ~4*10^(-7). Por tanto, el programa está cortando correctamente.

Ya subí el programa con las nuevas implementaciones a Github.

--------------------------------------------------------------------------------

21/01/2021

Ahora que el mecanismo de corte funciona, lo que voy a hacer
es crear una nueva muestra de datos y graficarlos en Python, para
ver que el código funcionó bien.

Ahí miré las imágenes, parece estar bárbaro. Cada iteración corta
luego de 1000 iteraciones de haber atravesado el piso de 10^(-6).
Y en los gráficos de las opiniones se ve que cada uno corta
a ritmos diferentes, pero siempre la curva se ve como que
deja de variar.

Ahora lo que voy a ver es si Baumann tiene algo anotado
sobre el código con el cual arma sus gráficos. Sino, tuve algunas
ideas de cómo reproducir esos gráficos por mi cuenta.
Bueno, en lo que yo leí no vi ninguna referencia a un código
o software especial usado para generar esos gráficos.
Queda entonces arremangarse y hacerlo yo mismo. Por ahora
dejemos de lado las distribuciones, arranquemos con los gráficos
que muestran como la opinión de los agentes se van
moviendo. La idea es que las opiniones armen una trayectoria
en línea gris. Es decir, voy a tener que armar vectores con 
la opinión de un agente en cada iteración, graficar eso con una
línea muy fina y gris. Luego, en el punto final, le pongo
un punto grande con un color que tenga que ver con el ángulo
que forma el vector, tomando en cuenta una distribución de todos
los colores según el ángulo que forman con la horizontal.

Para el tema de estudiar cómo varía el estado final del sistema
con delta y alfa, lo que voy a hacer es dentro del mismo programa
de C, o quizás en Python, un código que discrimine los distintos casos
en función de los signos de las opiniones finales y de sus módulos y
le ponga la etiqueta de: Consenso, Polarización y Estado Ideológico.
Luego, en Python tomo la etiqueta y a ese punto le coloco un marcador
cuadrado del tamaño correcto y con el color asociado a la etiqueta.

Ya modifiqué un poco el archivo del main para que me empiece a 
generar los datos que voy a necesitar graficar. En este caso
cambié los nomrbes de los archivos, me deshice del archivo de
Variación promedio de las Opiniones porque creo que no lo voy a necesitar
y ya modifiqué el Mover.sh. Ahora me voy a poner en Python a armar
el gráfico de estos datos.

De paso, también voy a armar una documentación en la carpeta con
imágenes de los archivos creados con el mecanismo de corte implementado.

-------------------------------------------------------------------------

27 y 28/01/2021

Esto es una entrada doble porque ayer arranqué con esto.

Mi idea es empezar a probar el tema de optimizar el código. Busco
implementar el uso de un archivo en el cual se encuentren ya calculados
los valores de la tanh. De esa manera mi intención es reducir el 
tiempo de cómputo de cada iteración y así poder correr los datos más rápido.
Porque con el tiempo actual esto resulta muy poco viable. Supongo que
podría ganar tiempo reduciendo alguna cantidad de cuentas, pero entiendo
yo que si quiero armar un gráfico similar al que hace Baumann para
determinar en el espacio de sus variables cuando el sistema llega 
a un consenso o cuando se polariza, voy a necesitar hacer un
barrido fino. Por no decir que técnicamente tendría que iterar
muchas veces y tomar un promedio, no alcanza con una sola iteración.

Reducir el tiempo de cómputo es PRIORITARIO.

Por eso partí de armar un archivo "Tabla_Valores_Prueba". En este
archivo armé una matriz de 3x3 con números en sucesión.
Probé el uso de la función fgets. Si bien está copada porque
lee una cierta cantidad de caracteres, los convierte a string
y los mete en un puntero, mi problema es que hasta donde
entiendo sólo puede levantar chars. Eso es un problema, porque
entonces no podría correctamente levantar los números doubles.

Ahora estoy pasando al uso del fscanf. Por lo que entiendo,
fscanf busca el patrón que le armás en el centro y eso se lo
pasa a las variables que le indicas. Entonces ignora totalmente
los newline y por lo que vi, ignora las tabulaciones también, porque
le estuve pidiendo que levante ints, que son los números que como
dije antes puse en la matriz, y ni registro tuvo de las tabulaciones.
Entonces se me ocurre que puedo armar un vector, o dos valores ya veré,
en los cuales guardar los números que necesito para la interpolación.

Mi idea hoy seria investigar un poco el fseek(). La idea sería poder
leer el archivo sin tener que pasar todos los datos a un puntero.
Lo cual sería una re cagada y creo yo que haría que todo este trabajo
sea reverendamente al pedo, porque el archivo va a tener muchos datos
ya de por sí. Eso es algo que también tengo que ver, si abrir un archivo
pesado requiere mucho trabajo o no.

Pero volviendo al fseek(), eso me permitiría fácilmente moverme en el
archivo. Esto además es muy importante porque el instanciar va a hacer
que el programa se cargue muchas veces, haciendo que el archivo se abra
y cierre muchas veces. Entonces, nuevamente, estaría perdiendo un
montón si tengo que cada vez cargar todo el archivo. De nuevo,
reverendamente al pedo.

Ok, el fseek() parece bastante sencillo. Además, comprobé que las 
tabulaciones ocupan una posición cada una, efectivamente. Ahora
debería comprobar que efectivamente los double ocupan un único
espacio, cosa razonable, y algo más importante, que el fscanf
avanza el indicador de posición a la posición inmediatamente
siguiente en la cual termina el patrón que busca, una vez
encontrado. Esto es importante porque si quiero moverme leyendo
desde el archivo, necesito total control de la posición del indicador.

Para esto voy a necesitar entonces las funciones ftell() y rewind().
ftell() me devuelve un int que me indica la posición del indicador,
mientras que rewind() regresa el indicador a su posición inicial. 
No sé si ese sea tan necesario o útil.

Bien, como yo supuse, el fscanf revisa el archivo hasta encontrar
la primera instancia del patrón indicado. Luego lee el patrón,
envía los elementos a las variables indicadas y luego avanza
una posición más. Cabría ver que los doubles ocupan exactamente
el mismo lugar, pero eso es casi obvio. Igual lo voy a probar.
Una vez hecho eso, lo que voy a hacer es archivar esto, armar
un archivo gigante y ver si el revisar algunos de sus elementos
me consume mucho tiempo.

Menos mal que probé si los doubles ocupaban el mismo lugar. No lo
hacen. Cada número guardado de un double tiene una posición propia.
Lo que ocurre es que el scanf sabe interpretar el número y lo lee
entero cuando le decís que tiene que levantar un %lf. La pregunta
interesante es, ¿Lo levantaría igual si el número tuviera más decimales?
¿O en ese caso habría que indicarle la cantidad de decimales? Además,
¿Qué pasa si arranca a leer número por la mitad?

.) Ya probé lo de indicar más decimales, eso no funcionó. La función
fscanf() no permite agregar el número de decimales como parámetro
a la hora de levantar datos, entiendo yo que lo que pasa es que lo levanta
todo y listo.

.) Sobre arrancar el número por la mitad, lo que hace es tomar el número
desde el cual arranca y lo lee todo hasta encontrar un punto o un final.
Vas a tener un problema en las cuentas, pero nada malo va a pasar en términos
de crasheos.

.) Si el número de decimales es mayor, el tipo lo lee correctamente. Recordá
que lo que hace fscanf al leer un "%lf" es mirar el número, ubicar el punto
y leer hasta que haya un corte del número. Entonces si el número tiene más
decimales no importa, lo va a leer hasta que se corte. Por tanto, lo que
marca el final del número es la tabulación al final del día.

Bueno, habiendo hecho todas estas pruebas estoy en condiciones de armar
una función que lea los datos que yo quiero leer. La idea de
la función es que reciba el puntero al archivo, usando fseek() ubique
los valores a interpolar y luego pase esos valores a una función de
interpolación. O que ella misma interpole. Igual quizás la interpolación
la arme aparte sólo para tenerla para futuras funciones.

Entonces, ¿Cuáles son los objetivos mañana?

.) Primero, mandar a correr el programa para crear un archivo con millones
de datos. Estaría bueno primero probar de armar un archivo de muchos datos
y ver si el programa le toma tiempo abrirlo, buscar dos números y volver.

.) Definir los intervalos en los cuales voy a calcular mi tanh. La idea
es cortar en algún punto donde pueda aproximar la tanh por 1. También
definir cada cuanto debería hacer el paso de la tanh. Para eso estaría
bueno ver una medida de cuánto varía la tanh punto a punto, porque
quizás varía cada 10^(-6) y el tipo varía en 10^(-3) cada 10^(-4).
Entonces tendría una precisión de 100 valores en los cuales yo no noto
diferencia de la función.

.) Armar una función de Interpolación de los datos.

.) Archivar la función de prueba armada hasta ahora. Ordenalo para
que se vea un poco más fácil de entender la próxima vez que quieras
mirarlo. Separa la parte escritura de la parte de lectura.

------------------------------------------------------------------------------

29/01/2021

Ya guardé los datos en el programa de Archivo.c. Ahora lo que voy a hacer
es armar la tabla de datos. Ya hice un primer archivo en el cual guardé
1 millón de datos double. Eso me tomó siete segundos. Por tanto podría guardar
10 o 100 veces esta cantidad de información y no sería mucho problema.
Sería una tarde trabajando, ningún problema. Creo, veamos si ahora
intento guardar doubles, pero que sean tanh.

Es algo totalmente inesperado, pero el programa tardó 4 segundos menos
en guardar valores de tanh. No entiendo qué pasó ahí. Pero bueno,
cosas pasan. Hagamos unas pruebas sobre tiempo de cómputo primero.

Esto no pareciera que es un problema. Estoy probando el uso de la función
tanh para realizar diez millones de cuentas y lo estoy comparando con la
realización de unas simples cuentas de multiplicación y suma. Las mismas
cuentas que más o menos haría con la función de interpolación.
No logro ver una diferencia  de tiempo en el cálculo que hace el programa. 
Digo, los dos parecen tardar cero segundos. No parece haber ninguna
diferencia, como que la tanh no le aporta mayores problemas al cálculo. 
De ser así, no tiene ningún sentido que haga todo este quilombo para cambiarla
en mi función original. Quizás vale más la pena que revise el
funcionamiento general del modelo y empiece a trabajar en el 
armado del gráfico de los estados finales del sistema en función
de las opiniones alcanzadas.

Ahí está, ahora sí pude poner a prueba esto. Casi se me cae el
mundo a pedazos. Como mi medición del tiempo no tenía una precisión
que me permitiera diferenciar cuál proceso tardaba más, lo que hice
fue meter a los dos loops en un while y ver cuál proceso lograba
en un segundo realizar más ciclos. El resultado fue que el proceso
que agregaba una tanh realizaba un 40% menos de ciclos en el mismo
intervalo de tiempo. Por tanto, hay una diferencia apreciable
y vale la pena optimizar al sistema con la implementación de un método
de interpolación de los valores de mi tanh().

Ok, momento, todavía no está todo dicho. Esto parece tener una variabilidad
de la concha de la lora. Volví a mirar al sujeto que hace las cuentas
simples, el que le ganó por un 40% en los ciclos, y en una iteración
apenas logró realizar 9 ciclos. En otra realizó 76. Creo
que voy a tener que guardar muchos valores y luego armar un histograma
y compararlos. Qué paja.

---------------------------------------------------------------------------

31/01/2021

Bueno, estuve retocando el Instanciar.sh para que mañana simplemente lo
mando a correr y eso pueda estar tranquilamente tres putas horas corriendo.
Todo para armar un cuarto de los archivos que necesito y encima con sólo
500 agentes, no con 1000. Pero bueno, es lo que hay, quiero gráficos.

Por otro lado, también armé un programa para medir los tiempos de cómputo
y comprobar que efectivamente vale la pena implementar una función de
interpolación. Con eso me guardé los datos y armé un histograma. El histograma
está guardado en la carpeta de Tesis. La idea es simplemente que hice
que el programa realice muchos cálculos, unos simples, unos con tanh.
La idea es que para medir cuál caso trabaja más rápido los hice
hacer una tarea similar y ver quién puede hacerlo más veces seguidas.
Guardé esa cantidad de veces y eso es lo que grafiqué en el histograma.

Ya archivé los datos, pero no subí nada a Github.

¿Entonces, qué debería hacer mañana?

1) Armar muchos archivos y empezar a armar los gráficos
correspondientes en Python. Va a haber muuuucho cálculo de fondo.
2) Definir los intervalos para calcular la tanh. Para esto calcular
la derivada de la función y ver cada cuanto varía. Creo que puedo
incluso hacer un barrido cada 10^(-5).
3) Armar efectivamente la función de interpolación.

------------------------------------------------------------------------

06/02/2021

Estos días estuve estudiando E4 fuerte, por eso casi ni me puse con esto.
Hoy ya me encargué de poner en todos los gráficos subidos al powerpoint
de la tesis las ecuaciones que indican los valores graficados.

Mañana debería preparar se gráfico explicativo de la forma en que se
resuelve cada iteración. También estaría bueno ver lo que me preguntó
Pablo sobre si el sistema sigue reduciendo el valor promedio de 
variación de opiniones de manera indefinida. Ya comprobé que un double
te puede guardar hasta 30 decimales sin problemas. Re flashero.

También tendría pronto que empezar a trabajar en el tema de la función de
interpolación y el armado de la tabla de valores de tanh calculadas.

------------------------------------------------------------------------

07/02/2021

Hoy terminé de subir unas imágenes de los gráficos de trayectorias de
opiniones que ayer no había subido. Ahora sí, no me queda nada por
subir al archivo de power Point. Debería empezar a trabajar en el
armado de los gráficos para explicarle a Pablo las interacciones entre
agentes. Y en el armado del archivo que guarde datos para ver que el sistema
no cae hasta el infinito.

------------------------------------------------------------------------

09/02/2021

Armé un archivo con las opiniones y otro con la variación promedio para un
número de 200 agentes, un K=5, T=2  y lo hice correr un total de 60000 pasos.
El objetivo es ver si el error cae infinitamente o en algún momento se plancha.

Ahí armé el gráfico de esto, y de paso corregí el código de Python para que
cuando arme estos gráficos, si hay un salto en el número de agentes no
guarde gráficos vacíos. Eso me pasaba porque antes me meovía entre valores
mínimos y máximos de N, por lo que no sabía que había en el medio.
Ahora lo corregí para que me arme un conjunto con todos los valores
de N para los cuales existe un archivo de datos, así sólo grafica
para valores de N existentes. Es decir, para archivos que tienen
ese valor de N.
	De paso, no corregí esto para la sección del código que arma
los gráficos sacados de la carpeta de interacción de Pares. Así que
atento a eso.

Cuestión, por lo que vi del gráfico hecho, el error no cae infinitamente.
Pareciera que llega como hasta 4*10^(-17) y después cae directamente
a cero. Y eso teniendo en cuenta que guardé hasta 20 decimales. Así
que pareciera que no cae infinitamente, lo cual es genial

------------------------------------------------------------------------

10/02/2021

Subí los gráficos que hice para el caso de 200 agentes con el objetivo
de ver si el error del sistema eventualmente caía a cero. También
lo puse en el archivo de Powerpoint. Los gráficos están en la 
carpeta de Corte, para que no quede ninguna duda de dónde están.

------------------------------------------------------------------------

16/02/2021

Al parecer en algún día antes me armé la tabla de valores de la Tanh.
El problema es que parece que eso no lo anoté acá, lo cual es una muy
mala situación. Así que anotemos directamente lo que pasó desde la
reunión con Pablo el miércoles 10.

Juraría que la Tabla_Valores_TANH la armé antes de juntarme con Pablo.
Durante la reunión con Pablo y Seba, charlamos sobre que deberíamos
entonces rearmar el programa para lograr replicar el modelo de Baumann
entendiendo que hay dos formas de realizar el trabajo.
La primera sería armando una red impulsada por actividades, con agentes
que se conectan con m otros agentes siguiendo homofilia e interactúan
con varios sujetos al mismo tiempo
La segunda es la que venía haciendo, que es pensar al modelo como un
modelo de agentes, con interacciones de a pares, quizás habría
que incorporar a eso también el tema de la homofilia y la red
impulsada por actividades.

Cuestión, decidimos trabajar sobre el primer caso que sería un poco
más exactamente lo que hizo Baumann, y luego pasaremos al caso
dos con más tiempo.

Entonces, lo que voy a tener que hacer ahora es reformar el programa
que tengo armado, porque tengo que modificar la función Din1 y
Din2 para que ahora trabajen con más gente, tengo que conectar usando
a la red de adyacencia, tengo que armar un mecanismo de homofilia y el
de variación temporal de la red según una cierta cantidad de pasos.

Hay claramente MUCHO por hacer, no parece que vaya a correr el programa
pronto, en especial considerando que voy a empezar a dar clases, por
no mencionar el final de E4.

Considerando todos los cambios que voy a tener que hacer al programa
principal, creo que es una muy buena oportunidad para modificar las
funciones de manera que no tomen al struct por copia, sino que lo tomen
por referencia. Me gustaría creer que haciendo eso, ahora esto va 
a funcionar mejor.

Lo primero que voy a hacer entonces es guardar el programa actual en la
carpeta de Grafico Baumann dentro de Programas y actualizar la documentación.
Luego, voy a empezar a hacer pruebas modificando un poco los structs
y con algunos structs en Prosem. Hecho eso, pasaré a hacer la modificación
total, desde adentro hacia afuera, arrancando por las funciones Din1 y Din2
y terminando con una nueva función Paso Temporal que se encargue de
evolucionar TODO el sistema en cada iteración. O quizás no, quizás 
mantenga el mismo formato más o menos abierto del código como
para poder intercalar más fácilmente ciertas cosas.

En el archivo de Powerpoint, acabo de agregar unas diapositivas
de separación, cosa de que quede claro y sin lugar a dudas cuáles
archivos los hice con cual código.

Luego de un poco de experimentación, ya descubrí qué era lo que
hacía que los structs funcionaran raro. El problema estaba en 
la forma de compilación de mi programa que yo hago, el hecho
de que uso el make all y que este programa es el que decide cómo
se updatean los archivos.o. En resumen, el problema que tenía
es que el archivo .o debía de alguna manera guardar registro
de la posición en el código de las variables. Lo cual no es
algo tan descabellado. Entonces, al moverlas de lugar se generaban
todos esos errores de acceder a memorias de manera equivocada.
La solución que encontré es simplemente reescribir los archivos
.o cada vez que modifique los structs. Eso se va a encargar de
armar correctamente las direcciones y todo va a estar bien.

Lo importante, ya no hay motivo para temerle a los structs,
ya encontramos el motivo de las fallas. Ahora sí, son una
herramienta bajo mi control. Hermoso

---------------------------------------------------------------------

17/02/2021

Ya hice un archivo para probar el uso de punteros a structs,
es realmente muy sencilla la cosa, así que no va a haber problema
en pasar todo el uso de structs a punteros de structs.

Entonces, lo que voy a hacer es lo siguiente:
Primero voy a cambiar todo el uso de variables de structs por
punteros de structs. Para esto voy a ir desde afuera hacia
adentro, arrancando por el main, luego voy a pasar a los demás
archivos, y una vez que haya modificado todas las funciones en
los paquetes del main, ahí si regresar al main y desde ahí
modificar el input de las funciones.

Hecho esto, recién ahí voy a empezar a agregar las funcionalidades
nuevas al programa. Esto debería terminarse en no demasiado tiempo.

Ya hice los cambios en todos los archivos, el programa funciona tal
cual como funcionaba antes, ahora sí podemos empezar a trabajar
en implementar las nuevas ideas. Están todas anotadas en objetivos,
para que no te olvides. El interpolador de la tanh por ahora
no lo voy a usar. Cuando digo cambios me refiero a que ahora
todos los programas corren con punteros a structs, no con
variables de structs.

--------------------------------------------------------------------

21/02/2021

Ya estuve revisando el tema de cómo armar la red de Erdos Renyi.
Mi interés principal hoy es organizar claramente el trabajo para
hacer mañana y el martes, cosa de llevar algo programado para el martes
para mostrarle a Pablo.

Primero, voy a querer crear redes de Erdos Renyi. Eso es bastante fácil,
ya tengo el generador de la matriz de Adyacencia. Lo único que tengo que
hacer es definir un nuevo parámetro que sea el grado medio y con ese
definir la probabilidad de interacción, una sencillez.

Luego, voy a querer que los sujetos operen sobre esa red. Así que voy a tener
que ir a revisar la función Din1 y cambiar su interacción entre sujetos de
manera que ahora interactúe con varios a la vez.

Lo siguiente, que es un poco más enrevesado, es lograr que el RK4 me
evolucione el sistema de forma sincrónica. La forma en que evoluciona
el vector actualmente me gusta. Lo que voy a tener que ver es cómo guardar
correctamente la parte evolucionada. Lo que podría hacer es intentar guardar
los datos en un vector extra, como el preOpi. Pero no me gusta la idea
de tener 15 vectores dando vueltas. Aunque creo que podría agregar un vector
de estado final y usar eso. Habrá que ver.

Con esto creo que ya el sistema podría funcionar correctamente. Y estaría
listo para hacer simulaciones. Lo de variar Alfa, Delta y N es tema de la
instanciación nomás.

------------------------------------------------------------------------

22/02/2021

Trabajando en el código, ya hice que el programa genere mi red de adyacencia
con una probabilidad que va variando según la cantidad de agentes. 
Visualicé la red y en efecto se genera de manera correcta. De paso, 
también agregué en el generador de la red una línea que pone ceros
en la diagonal. Por si acaso nomás, no estaba seguro de si se ponían
ceros al principio. CORRECCIÓN: Ya lo vi, en la inicialización del
vector ya se colocan ceros en todos lados, así que no debería haber
ningún problema con eso. Igualmente, no pierdo nada por hacerlo dos
veces. Digo, es algo bastante rápido de hacer.

Revisé la función Din1. La modifiqué para agregar el factor de 
la matriz de Adyacencia, de manera que ahora toma en cuenta los
agentes con los que está conectados. También le agregué un if
que cancele la cuenta en caso de que los agentes no estén conectados.
Porque para qué hacer toda la cuenta con la tanh si al final
el cero de la matriz de Adyacencia lo mata.
También modifiqué la función Din2, para que haga la sumatoria 
sobre los todos los agentes de manera que la tanh se calcula 
sobre toda la red y se van sumando todas las tanh que correspondan.

Ahora estoy modificando también la iteración, de manera de incluir
una nueva matriz que sea la que itere y en cada paso me guarda el nuevo
valor de opinión en la matriz de Opinión final. La idea es esta:
.) Antes de iterar, saco una foto a todo con mi vector PreOpi.
.) Inicio la función Iteración.
.) En la función iteración, antes de mandar a ejecutar el RK4,
primero copio mi foto (El vector PreOpi) en el vector que SÍ voy
a iterar, el vector ItOpi.
.) Evoluciono para un agente y un tópico al vector ItOpi.
.) Luego de evolucionar el valor de opinión de un agente respecto de
un tópico, copio ese valor en la matriz Opi.
.) Repito los últimos tres pasos hasta haber recorrido todos los agentes
y tópicos.
.) Termino la iteración, teniendo ahora en el vector Opi el estado
del siguiente paso temporal, en el cual todas las opiniones fueron
evolucionadas a partir de una foto.
.) A continuación guardo los valores que correspondan, calculo las
diferencias y vuelvo a empezar.

Al final logré modificar a la Iteración de manera de no necesitar
un tercer vector de opiniones. Para eso lo que hice fue modificar el
RK4 de manera de que la evolución de la función ahora no se guarde
en el vector que le paso al RK4, sino que ese valor sea returneado por
mi función. Luego, lo que hice fue guardar ese valor en el vector
Opi, de manera que el Opi que se construye sea mi siguiente paso temporal.
Entonces, la "foto" que le paso al RK4 de mi sistema en el paso actual
es el PreOpi. Y lo genial de esto es que el RK4 NO modifica de ninguna
forma el vector que le estás pasando, entonces ese vector puede servir
sin problemas como una foto estática del sistema.

En otros temas, descubrí un craso error en las pendientes usadas 
al calcular el RK4. No puedo creer que no hubiera notado esto antes.
Me da bastante bronca que esto haya pasado por fuera de mi radar,
esto es un error importante en las cuentas. La cosa es que por como
yo armé el RK4, tengo un vector de k, que son las pendientes intermedias
que calcula el RK4 para evolucionar el sistema. Por una cuestión de simpleza
en el código, yo hice que existan 5 k, donde la primera es siempre cero.
Por eso yo las había numerado desde k0 hasta k4 mis k, entendiendo que la
k0 es un fantasma creado por mi. El problema es que cuando calculaba el paso
siguiente del modelo, en vez de usar el conjuntos que va desde k1 hasta k4,
usaba el conjunto que va desde k0 hasta k3. Ya lo corregí, espero que
no haya más sorpresas de este estilo.

Falta ponerlo a prueba, pero la idea es que el código ya está reformado
para ponerlo a probar con lo que decía Pablo. Anotemos entonces las características
de este nuevo código:
.) Ahora el ejecutable recibe tres valores como input por línea de comando.
Estos valores, en orden, son el número de agentes N, el valor de Alfa*10
y el valor de Cosdelta*10. Los valores de Alfa y Cosdelta están multiplicados
por 10 porque no descubrí como pasar en línea de comando números fraccionarios,
así que paso números más grandes y lo que hago es en el programa de C
dividirlos para que sean lo que tienen que ser.
.) El programa genera una red de Erdós-Renyi con grado medio igual a 8.
.) Al evolucionar a los agentes, el programa mira ahora la opinión de todos
los vecinos de un agente, en vez de mirar la opinión de un sólo "vecino".
.) El programa genera archivos con nombres:
"Datos_Opiniones_alfa=$_Cdelta=$_N=$" (Los $ significan números").
.) Los parámetros son K=1, T=2, dt=0.001, Máxima Opinion inicial = 3,
Criterio de Corte = 10^(-6), Iteraciones Extra = 2000.

Hagamos una primer prueba del programa. Pareciera que funciona lo más bien,
habría que graficarlo para ver que todo está en orden. Hagámoslo hoy eso.
Así ya para mañana tengo los datos hechos, y a lo sumo dedico la mañana
a hacer el commiteo correcto de los archivos.

Para hoy entonces queda crear las carpetas para guardar los archivos de datos,
crear la carpeta para las imágenes, modificar el Mover.sh, modificar el 
Instanciar.sh y largarlo a correr, por lo menos para 10 y 100 agentes.
Es más, siendo sincero, no pongas al N como un valor de Input, sacalo
por ahora, y que el Instanciar sólo recorrar en Alfa y Delta. Porque
siendo sincero, no vas a poder calcular con tiempo las simulaciones
para N=1000. Vos y yo lo sabemos muy bien. Esto no es un tema de
pajero, es simplemente una cuestión de que no quiero comprometerme
a cálculos que después no se puedan terminar.

---------------------------------------------------------------------------

23/02/2021

Creo que hoy tampoco voy a hacer el trabajo más burocrático, seguro lo
termine haciendo después del final.

Hoy voy a intentar implementar una función que lo que haga es detectar
grupos en mi red. La idea es que pueda detectar si la red es conexa. Con
eso creo que ya voy a poder hacer correr los casos de N=10 y N=100 que me
dijo Pablo. Luego, mejoraré eso para que en caso de que la red no sea conexa
pueda artificialmente crear una red conexa. En el sentido de generar enlaces
en lugares específicos para que quede todo conexo. Más que nada
porque sino creo que hay grandes chances de que el sistema se quede mucho
tiempo intentando crear de manera aleatoria una red que justo resulte conexa.
Estoy seguro que esa probabilidad se puede calcular, pero ahora no quiero
perder tiempo en eso, pero después lo veré.

Armé la función, la probé y parece que funca bárbaro. La probé arrancando
desde cualquier nodo, con grupos que yo sé exáctamente cuáles son. Funciona
muy bien, siempre identifica correctamente el tamaño del grupo. En el futuro
me gustaría que además identificara todos los grupos que hay, no sólo el
que está revisando, pero para eso voy a tener que trabajar más el código.
Por no decir que la forma en que funciona ahora es con un montón de
ifs y eso no me gusta una mierda. Después veré de corregirlo de una manera
que resulte más fluída y razonable. Pero lo importante es que funciona y
ya podría implementarla en la función del main, con el objetivo de que compruebe
que el grupo sea de tamaño N. De esa manera, garantiza que el sistema sea conexo.

-------------------------------------------------------------------------------

24/02/2021

Como la función que mide el tamaño de la componente a la cual pertenece un nodo
parecía funcionar bastante bien, decidí poner a prueba cuánto tiempo podría tardar
el programa en crear una red conexa para los valores de N que iba a usar. Por suerte,
para N=10 y N=100 las chances de crear una red conexa son grandes, así que básicamente
en un intento lo hacía siempre. Para N=1000 ya necesitaba dos o tres intentos algunas
veces. Para N=10000 le costó muchos intentos crear una sola red conexa.
Como mi intención era medir cuanto tardaba en crear redes conexas, y como mi
ignorancia me hacía pensar que iba a costarle hacerlo, le puse un while para
que lo repitiera 10 veces confiando en que eso me iba a dar una medida de tiempo.
Lo hizo muy rápido en general como para que tardara más de un segundo.

.) Este Programa con esta descripción entonces lo tengo que pasar al Archivo
todavía.

Luego implementé la función de Tamaño_Comunidad en el programa principal
y lo estoy mandando a correr para 10 y 100 agentes. Las simulaciones para
10 agentes me están tomando entre 1 y 4 segundos, promedio 2. Las de 100
agentes están tomando entre 12 y 240 segundos, promedio 25. Considerando
que en total estoy realizando 200 simulaciones, al variar Alfa y Cdelta,
La corrida total para N=10 me toma unos siete minutos, mientras que
la corrida para N=100 me toma 1 hora y 20 minutos. Un poco más de 10 veces
el tiempo de lo que tomó para N=10, justamente. Para N=1000 entonces, me
va a tomar razonablemente unas 13 o 15 horas. Lo puedo separar en dos
tandas, y hacerlo en dos días distintos, eso no es problema. Mi problema
va a ser el levantar los datos. El jupyter Notebook estuvo teniendo problemas
la última vez con programas que pesaban más de 1 Giga. Así que voy a tener que
pensar enformas de levantar mejor esos datos, o pasarme al Python propiamente
dicho, dejar por un rato el Notebook más que nada para pruebas. No me copa
mucho eso. Eso me está pasando desde que me pasé al navegador Opera. Quizás
Opera tiene algún mecanismo de corte de memoria en cuanto ve que está
consumiendo demasiado. No hice pruebas como para saber si el problema
estaba en la lectura o en la manipulación de los datos, tendré que
revisar eso cuando tenga mis datos calculados. Hay bastante para ver
en este aspecto.

En Python simplemente copié un código ya existente para realizar estos
gráficos y lo modifiqué para que levante los archivos de la nueva carpeta
guarde los gráficos en la nueva carpeta y que corrija los nombres de
manera acorde. De paso, modifiqué la función que asigna colores, porque
me pareció que no los estaba asignando de manera correcta. Por ahora
parece que funciona, pero definitivamente tengo que revisarla de nuevo
y hacer las cuentas, ya no confío en eso. Más que nada porque al modificarla
no hice las cuentas, simplemente escalé los parámetros. Lo cual no
me da mucha confianza. Así que eso, atento, es algo a mejorar.

Todavía no hice los commits correspondientes. Es obvio que al haber
hecho una simulación masiva, obviamente usé los Instanciar.sh
y Mover.sh, por tanto esos están modificados. También me falta
crear el archivo de Documentación en la carpeta de imágenes de
Erdos-Renyi. De paso, a esta altura me parece una buena idea revisar
la carpeta original de Programas C y Programas Python, ver si
hay algo rescatable y si no borrarla.

También falta subir los gráficos al Power point y hacer las anotaciones
correspondientes sobre los parámetros del modelo y demás.

-------------------------------------------------------------------------

28/02/2021

Hoy lo que hice fue commitear cosas al Github, Agregar archivos de 
documentación en las carpetas de imágenes, Modificar el archivo
de Documentación de Programas y agregar notas en el archivo
Archivo.c. Lo que me queda para terminar la burocracia de organizar
mis cosas es agregar notas en el powerpoint.

Seguro haga eso el martes, y ya mañana voy a tener que armar
una carpeta extra donde guardar archivos en los que voy a hacer
unas pruebas con distintos valores de dt, para ver cómo quedan
los gráficos al ir aumentando el valor de dt. La idea es que
eso me permita reducir el tiempo de cómputo, el tamaño de mis
archivos y demás.

------------------------------------------------------------------------

01/03/2021

Hoy voy a arrancar armando el gráfico del alfa crítico en función del
coseno de delta. Luego de eso voy a armar una tira de datos en los que
voy a hacer cálculos mientras varío el dt.

Bien, ya tengo hecho el gráfico del Alfa, ahora vamos a subirlo
al Power Point y empezar a armar las tiradas de varios valores de 
dt.

Voy a armar las nuevas tiradas cambiando el dt. Para un cos(delta)=0.2
voy a hacer cálculos con alfa entre 0 y 0.5. Voy a usar 100 agentes
y variar el dt entre los siguientes valores:
[0.001,0.005,0.01,0.05,0.1]
Estos datos los voy a guardar en una nueva carpeta llamada "Cambios dt".

Ya estoy correctamente armando los archivos. Ahora voy a tener
que armar los gráficos asociados. Para eso voy a levantar los datos
en Python, graficar las trayectorias de opiniones, y podría además
hacer un gráfico de cantidad de iteraciones según dt y según alfa,
que permita visualmente comparar la cantidad de iteraciones de cada
combinación, como una forma de medida del tiempo ahorrado 
entre iteraciones.

Me acabo de dar cuenta que los gráficos que voy a comparar son todos
distintos, eso no me sirve. Necesito que la comparación de los dt
surja sobra las mismas redes, y sobre los mismos estados iniciales,
para ver que lleguen a los mismos valores finales. Por eso es que
voy a rearmar el main para que itere en los dt DENTRO del programa
y no en el instanciar. Eso va a hacer que mueva muchas cosas de lugar.
Igual el Erdós-Renyi lo guardé ya en programas, para evitar problemas.
Este código lo usaré para esto y luego lo guardaré en una carpeta 
aparte en Programas.

Perfecto, ahora el programa funciona generando una red de adyacencia
y una serie de opiniones iniciales, y sobre esos datos el sistema
corre evolucionando, únicamente cambiando el dt. (Técnicamente lo
que también varía es el random porque no reseteo el random, pero
creo que no vuelvo a lanzar dados en ninguna otra cuenta)

Estoy trabajando en Python. La idea es armar tres gráficos a partir
de estos datos:
.) Trayectoria de Opiniones, para ver que tan bruscos son los saltos
al evolucionar el sistema
.) Variación de las opiniones, para ver cuánto tarda cada sistema
en llegar a cero y comparar eso en un gráfico. Además para ver
que efectivamente se cumpla el criterio de llegar a una variación
menor al criterio de Corte.
.) Cantidad de iteraciones en función de dt y de Alfa, para ver 
gráficamente una medida del tiempo ahorrado dependiendo del valor
de dt.

Mañana continuar desde el gráfico de Variación de las opiniones.
Creo que ya tengo el orden del programa correctamente armado

-------------------------------------------------------------------------

07/03/2021

En algún momento en la semana resolví el tema de que el código
de Python me arme los gráficos de Trayectoria de Opiniones,
el de Variación de las opiniones y el de Cantidad de iteraciones
vs dt. Así que ya tengo los gráficos correctamente armados,
guardados en sus carpetas correspondientes y con los archivos
de Documentación pertinentes.

Estudiando los datos, pareciera que lo más razonable actualmente
es usar valores de dt = 0.1 fuera de la región de transición y
usar valores de dt = 0.01 o 0.02 en la región de transición.

Las trayectorias de opiniones no parecen en lo más mínimo afectadas
por el cambio de dt, tanto en los caminos tomados como en los
resultados finales alcanzados. Sí es importante notar que el número
de iteraciones se reduce dramáticamente, y es mi entender que de 
aumentar el dt por encima de 0.1 el número de iteraciones podría
reducirse a apenas decenas, lo cual resultaría muy poco para
poder hacer estudio de la evolución del sistema.

La idea para hacer hoy, no sé si llegaré a hacerlo todo, es
primero hacer el commit a Github con todas las actualizaciones de
archivos.

Luego, tendría que armar un nuevo programa para empezar a simular
datos con el nuevo valor de dt y barriendo valores de Alfa y 
cos(delta) como los mencionados en el pdf que le mandé a Pablo.
Estos datos los voy a tener que guardar en una nueva carpeta
y además voy a tener que hacer varias corridas para cada valor
de Alfa y cos(Delta), de manera de empezar a producir estadística.
Para arrancar, yo diría hacer entre 20 y 40 corridas. Confío que
40 puedo hacer sin problemas.

Lo siguiente será levantar estos datos de manera correcta y
graficar las trayectorias de todos en un mismo gráfico, para 
ver qué pasa en promedio. Y de ser posible, armar una función que
reconozca en función de los valores finales, qué está ocurriendo con
el sistema.
(Cuando digo graficar la trayectoria de todos en un gráfico, me
refiero a tomar todas las instanciaciones de un valor particular
de alfa y cos(delta) y esas instanciaciones guardarlas en un
solo gráfico, no digo de mezclar datos de alfas o cos(delta)
distintos)

Hecho eso, ya tendría todas mis herramientas para ir armando los
gráficos de Baumann, así que tendría prácticamente la mitad de
la tesis hecha. Digo yo, inocentemente.

La nueva fase del proyecto la voy a llamar "ER2". Los parámetros
de esta nueva fase son: N = 10,100,1000. K = 1, T = 2, <k> = 8,
dt = 0.1 para valores fuera de la región de transición
( 0 < Alfa < 0.2) y dt = 0.01 para la región de transición.
El Criterio de corte lo baje a 10^(-8), El máximo de opiniones
iniciales es 3, El cos(delta) varía entre 0 y 1 de a 0,1, la 
cantidad de iteraciones extra las bajé a 50.

Los archivos creados tienen nombres: 
"Datos_Opiniones_alfa=$_Cdelta=$_N=$_Iter=$".
Los estoy guardando en la carpeta ER2 dentro de la carpeta de
Python. Para cada terna de valores N, Alfa y Cdelta estoy 
armando 40 iteraciones cosa de armar estadística con eso
por ahora. En el futuro serán más.

Lo que estoy pensando apra ahorrar tiempo en el futuro es que
podría tomar el Archivo de instanciar y definir por cases
los códigos para cacular los datos del sistema en las distintas
regiones. Es decir, los datos fuera de la región de transición
y los datos en la región de transición. Así con uno simples 
números o comentando y descomentando puedo mandar a correr
todo. Pensalo.

Ahora creo que lo que debería hacer es ir actualizando el 
Power Point con las cosas de Cambio dt y la nueva fase del
proyecto, mientras el programa corre eternamente montones
de cálculos extra de fondo.

-------------------------------------------------------------------

08/03/2021

Hoy mandé a correr el programa para todos los alfas a partir
de 0,2 hasta 1; todos los cos(delta) entre 0 y 1, y todos los
N = 10,100,1000. Esto tomó unas 12 o 13 horas de cálculo.
En este caso dt=0,1. Las iteraciones extra son sólo 50.

No hice ninguna cuenta al respecto, porque el tiempo no me alcanzó.

--------------------------------------------------------------------

09/03/2021

Hoy mandé a correr el programa para todos los alfa entre 0,01 y
0,2; moviendo el alfa de a 0,01. El cos(delta) lo moví entre 0 y 1
y N = 10,100. El de 1000 lo dejé para hacerlo otro día. Es más,
creo que eso va a tomar más de un día. El dt=0,01 y las iteraciones
extra son 2000.

-------------------------------------------------------------------

10/03/2021

Borré todos los archivos con N=10 que estuve calculando. Pablo dijo,
y con mucha razón, que esos no sirven realmente. Los de 100 tampoco
son realmente útiles, pero en lo que voy armando los de N=1000
prefiero tenerlos para ir haciendo algún cálculo.

Hoy lo que voy a hacer es armar algún código que me permita graficar
en una sola imagen todo un ensamble de simulaciones para cada
Alfa y coseno(Delta). La idea va a ser trabajar con una reducción
de la opacidad de las líneas, y simplemente mandarlo todo a correr.

Si tengo tiempo, voy a arrancar a armar una función que pueda
decidir si el estado final es uno de polarización, estado
ideológico o consenso central.

También debería subir algunas imágenes más al power point y 
hacer la burocracia de estos días, que no debería ser mucha
porque hace varios días que lo único que vengo haciendo es 
crear archivos en simulaciones, no actualizo el código.
Excepto para algún cambio de Iteraciones extra y cosas.

De paso, TODAS las simulaciones hechas tienen dt=0,1. 
Al revisar el archivo del main acabo de observar que eso
nunca lo cambié. Fue un error, pero un buen error porque
me ahorró tiempo de cálculo.

De paso, las simulaciones les había puesto que corten
cuando la variación promedio era menor a 10^(-8).
Pero lo repensé y me parece un trabajo innecesario, así que a 
las siguientes simulaciones no se lo exijo. En
pocas palabras, las simulaciones que tienen criterio
de corte 10^(-8) son todas las de N=100 y las de
Alfa =0 y N=1000. El resto de las de N=1000
tienen criterio de corte 10^(-6)

Ya tengo en buena medida armado el código para hacer los 
gráficos de trayectoria combinando todas las iteraciones
en un sólo gráfico. Ahora mismo lo que estoy haciendo es
comparar distintas transparencias de las líneas para ver
cuál hace que el gráfico se vea mejor. Básicamente, lo que
quiero es que las líneas grises no me tapen el resultado
final. Después lo que voy a tener que agregar es el 
normalizar los gráficos.

Creo igual que el Alfa cómodo de la transparencia es
0,1

También se me ocurre armar un diccionario con los nombres
de todos los archivos de manera de que el diccionario contenga
los nombres de los archivos con los valores de Alfa, Cdelta
y N correspondientes. La idea de eso es no tener que repasar
TODA la lista de nombres para cada gráfico. Además me puedo
sacar de encima los if que comparan Alfas y Cdeltas y demás.

Creo que voy a tener que corregir la forma en que se marcan
los puntos finales. Porque las líneas grises terminan tapando
los puntos.

Una vez que eso esté hecho, ahí si vas a tener que hacer lo
que sigue, armar el gráfico del espacio de fases. Eso va a
ser un poco complejo de determinar.

------------------------------------------------------------------------

11/03/2021

Bien, ya tengo armado el código que me grafica las trayectorias
de opiniones. Ahora hay que darle a eso. Me da un poco de miedo
mandarlo a correr eso ahora. Lo voy a dejar para mañana y así
lo mando todo de una desde la mañana, y lo voy vigilando cada
tanto. En caso de tener problemas con el tema de la memoria del
programa, veré de descargar el programa como un archivo py y mandar
a correr todo de una desde un archivo py, eso no debería cortar
por temas de memoria.

Dios, la ansiedad que me está generando el hecho de que me ocupe
toda la memoria el trabajar con estos archivos. Ya tengo más de
200 Gigas de datos en la carpeta ER2 nomás. Esto se está descontrolando
fuerte.

Casi me olvido, necesito primero ver de normalizar los valores 
que estoy graficando. Así que voy a ver de tomar el máximo
de todas las opiniones, y al guardarlas en el diccionario OdT
ahí aplicar el factor de normalización. Aunque atento con eso
porque entonces los valores de los puntos finales no van
a estar normalizados. Voy a tener que normalizar esos
también.

Pablo dice de normalizar los gráficos. Hay un tema que
me parece bueno marcar antes de hacer eso. Acabo de mirar
un gráfico hecho en el caso de un valor en la región
de transición. O cerca por lo menos. Cuestión, que el
gráfico tenía las opiniones finales sobre la diagonal
del estado ideológico, pero lo importante es que los 
valores máximos de opiniones que tomaba eran (3,3) y
(-2,-2). También tomaba valores intermedios. Ahora, esto
me resulta muy difícil de creer, si entendemos que el
sistema está tendiendo a valores que sean el grado
del nodo. Es decir, de alguna manera la distribución
de opiniones finales debería, aunque nunca lo corroboramos,
ser similar a la distribución de grado del sistema. El sistema
tiene un grado medio ocho, eso es incuestionable. (Podría
comprobarlo de ser necesario mirando la matríz de adyacencia.
Gracias Dios por no haber dejado de anotar eso nunca.)
Cuestión, mi impresión es que lo que está pasando es
que el valor de alfa en esta región es el justo de manera
de que la tanh no cappea, entonces lo que aporta son
valores fraccionarios que hacen que el sistema no
tienda a K*(Nº 1eros vecinos), sino a cosas más
pequeñas. Entonces si yo normalizo eso, ese comportamiento
se va a perder en la normalización. Mencionárselo a Pablo

---------------------------------------------------------------------------

12/03/2021

Hagamos una lista de las cosas por hacer.


.) Armar función que identifique el estado final del
sistema en función de las opiniones finales de los
agentes.
.) Estudiar la Variación Promedio de los sistemas, para ver
si el sistema converge.
.) Armar distribuciones de los valores finales de los
agentes, de manera de caracterizar la zona de transición
del sistema

Ya tengo hecho le mecanismo de normalización de los gráficos
Ahora pasemos a identificar estados finales.

Ahí armé la función, parece funcionar bien, habría que ver 
si identifica correctamente los gráficos. Para ponerla a 
prueba voy a empezar a armar algunos gráficos y que les
ponga un texto diciendo cómo los califica.

Todavía tengo que mirar mejor los casos, pero pareciera
estar haciendo una clasificación correcta. Creo que va 
a quedar realmente claro cuando haga un estudio de la
distribución de los valores finales de los agentes.
Hasta entonces lo único que puedo hacer es ver algunos
archivos directamente con el Notepad.
Ahora, tengo que admitir que mirando los archivos del
Notepad en uno de los casos en que manda consenso, se
nota que como el programa dice, muchos casos cayeron al
consenso y unos pocos llegaron a estado ideológico.

Bajé el programa y lo corrí con el Python Spyder.
Al parecer el programa del consumo excesivo de memoria
se daba específicamente por los gráficos. Voy a ver
si quitando los delete sigue funcionando igual,
más que nada para poder seguir trabajando como hacía
siempre.

El problema central parece ser la creación de los gráficos.
Es como si en el Notebook se estuvieran guardando 
los gráficos que deberían cerrarse. Entonces esa 
acumulación de gráficos es el problema que se está
comiendo la memoria. Al trabajarlo en el Python
Spyder, esa memoria consumida se resetea cada
vez que se genera un nuevo gráfico. Así que
creo que lo mejor de ahora en adelante es
en Notebook probar funciones e ir viendo que tal,
y después en el Python Spyder correr todo.

----------------------------------------------------------------------

14/03/2021

Probé hacer que los gráficos se realizaran dentro
del mismo Spyder, pero eso de nuevo crea el problema
de que la memoria se sobrecarga. Así que por ahora mi
mejor opción es dejar que el programa arme los gráficos
de manera externa, es decir en ventanas extra, y luego
los vaya cerrando. Al hacer eso, pareciera que con
cada nueva ventana que abre, el consumo de memoria
se reinicia y de esa manera no me come toda la pc.
Aunque sigue comiendo un poco.

Lo malo de esto, es que cada nuevo gráfico que arma
me genera una ventana emergente que tapa lo que 
estaba haciendo. Voy a ver que tanto puedo ignorar
eso, aunque pareciera ser que va a ser una paja.
Igual, mejor eso y que el programa pueda laburar
todos los gráficos de una, a tener que andar con
el culo en la mano y sin poder usar la pc mientras
hace de algunos pocos.

Bueno, discutido esto, ahora viene el siguiente
punto. La función de identificar estado final 
no estaría funcionando perfectamente como yo
esperaba. Estoy notando que el problema está
en cómo interpreté yo que debería funcionar.
Así que por eso, la voy a modificar. El error
crucial que está teniendo parte de que la función
mira el estado final de UNA simulación. El tema
con esto, es que una simulación tiene dos opciones:
O cae al (0,0) o sus estados se alejan en una
direccion diagonal. El tema con eso es que una sóla
dirección diagonal jamás generaría el estado de
polarización. Por eso mi programa sólo reconoce
Consenso e Ideológico como estados finales. Entonces
lo que voy a tener que hacer es armar un vector
que guarde los estados finales de las 40 (O más
en un futuro) simulaciones y a esas les aplique el
mismo exacto algortimo que vengo usando. En ese caso, vas a ver
que los estados de Polarización aparecen.

Ahí lo corregí, o eso creo. Habrá que ponerlo a prueba.
En cuanto termine con el siguiente alfa, lo mando
a ver que tal. Lo que hice fue ir sumando las listas,
que entiendo que eso lo que hace es a la lista actual
ponerle todos los elementos de la lista nueva al final.
Eso me arma una única lista con las opiniones finales
de los agentes de todas las simulaciones realizadas
con un mismo conjunto de parámetros alfa y Cdelta.

Lo puse a prueba, ahora identifica correctamente las 
polarizaciones. Así que eso ya funciona.

Lo siguiente entonces es armar un código que me arme
las Variaciones Promedio y otro que me arme la distribución
de valores.

Empecemos por la Variación, aunque la variación no me veo
graficándola hoy. Mañana lo que haré será graficar la variación
Y las TdO todo de nuevo. Lo que voy a necesitar es guardar
en algún lugar la Info del estado final del sistema. Quizás
en el título de las imágenes. Eso es importante, porque
sino la próxima vez que quiera hacer el gráfico de fases
voy a necesitar revisar TODOS los archivos de nuevo. 
Aunque quizás no me tarde tanto porque como no esté graficando
sino sólo mirando el último sujeto de la fila, quizás la
cosa se haga rápido. Bueno, igualmente no me gustaría 
arriesgarme. Supongo que antes de mandarlo a correr podría
revisar cómo hacer para guardar eso en un archivo de datos.

Por otro lado, sospecho que esa forma de clasificar los gráficos
no va a ser del todo buena, porque lo que observe en los gráficos
de distribución de valores va a resultar más definitorio.
Así que esto va a ser más que nada una cuestión previa antes
de usar los verdaderos criterios para definir Consenso, Ideología
y Polarización.

Creo que ahí incorporé correctamente el graficado de las
Variaciones Promedio. Habría que ver que tal queda. Ahí lo
puse a prueba, se ve bastante bien. Ahora, suerte estudiando
eso. Van a ser 290 gráficos para mirar. No sé si va a haber
tanto que puedas sacar de ahí. ¿Podría poner los distintos 
alfas en un mismo gráfico?

De paso, resolver las Variaciones Promedio resultó una
boludez en tiempo. Por lo que claramente lo complicado
es el graficar todos los datos, eso es lo más complejo.
Así que superado el gráfico de los TdO, creo que no
voy a tener tantos problemas de cómputo.

Ahora debería armar un módulo que se encargue de graficar la 
distribución de valores finales. Para eso voy a querer que
tome los estados finales, arme un histograma con eso y luego

Bueno ahí lo armé y lo probé. Terminé usando 30 bins para el histograma.
Parece un número bueno para que no me aparezcan bines vacío en
el medio de una curva de distribución. Lo bueno es que en la
curva que grafiqué se puede ver a ojo que la distribución es
una distribución media centrada en 8 para los valores de opiniones,
que es lo que esperábamos ver dada la forma de la ecuación
diferencial.

Listo, ya están hechos todos los preparativos. Mañana simplemente
mandá a correr el programa y este tendría que armar todos los
gráficos de Distribución de opiniones, Variación Promedio y
TdO del sistema. También, deberías primero borrar todas las imágenes
que están en la carpeta de imágenes. Para que no haya imágenes
repetidas y demás. Y por supuesto, primero una plegaria a 
todos los santos que conozcas, como para que no le pase nada durante
la ejecución y arme todos los gráficos.

-----------------------------------------------------------------------------

15/03/2021

Vamos a ver de ir armando el gráfico de fases del sistema.
Para eso voy a necesitar de la función que identifica 
si el estado final del sistema

Ahí armé la parte del código que hace el gráfico de Fases.
Lo cual está bien, ahora el tema es mejorarlo para que se vea más lindo.
Pero más allá de eso, el gráfico se ve genial, respeta muy bien
la línea del Alfa Crítico y además la región de polarización e ideología
parece estar en sintonía con lo de Baumann. Yo diría que la diferencia
entre lo de Baumann y lo mío es un tema del barrido.

Creo que lo que voy a hacer hoy es dejarlo terminar de armar los
gráficos y después de jugar la partida con Diego me voy a poner
a armar los gráficos que voy a mostrar mañana. Hay muchos gráficos
para mostrar, no estoy seguro de cómo lo voy a hacer.

--------------------------------------------------------------------------------

18/03/2021

Estoy tratando de armar los gráficos para una mejor muestra de los
datos que charlamos la otra vez. Para eso, me interesa de alguna forma
intentar levantas las imágenes con Python y colocarlas en algún subplot.
Se me ocurre que una idea copada para mostrar serían las Trayectorias
de Opiniones y las Variaciones Promedio de los siguientes valores:
Alfa = [0; 0,05; 0,1; 0,15; 0,2; 0,3; 0,4]
Cos(delta) = [0; 0,2; 0,4; 0,6; 0,8; 1]

Eso me quedaría un gráfico total de 7x6. Me parece un buen número
son 42 gráficos todos juntos. El problema que estoy encontrando
ahora es un tema de una caída en la calidad de la imagen
mostrada en comparación con la imagen original.

Bueno, ya logré levantar una imagen y ahora se ve bien. Por lo menos
no noto problemas en la calidad. Para que la imagen salga copada
uso el plt.imshow con el comando interpolation="spline16". Posiblemente
esto le consuma algo de tiempo, pero eso en leer y graficar unos simples
42 gráficos no será mucho drama. Digo que va a consumir tiempo en
comparación a alguna interpolación más cruda.
Después para simplificar en términos de la imagen use el plt.axis
para quitarle los ejes que se me producen por el plt.show().

Lo siguiente es probar en hacer subplots. Fueron varias pruebas, pero
ya tengo algo que funciona más o menos bien. Habrá que ver de ir 
mejorándolo, pero eso es un detalle. Cuestión que la idea es partir
de una figura hecha con plt.figure. Luego, esa la divido en pedazos
usando add_gridspec. Por último, al elemento gridspec creado le
ejecuto el comando subplot para obtener los axes sobre los cuales
voy a hacer mis gráficos y listo, tengo todo hecho.

Después puedo iterar los axes para graficar en ellos cada imagen que
quiero y listo. Por último guardo la imagen, la cierro y trabajo completado.
Me queda quizás la idea de armar a mi figura los ejes que indiquen 
cómo varía Alfa y Cos(delta)

---------------------------------------------------------------------------

19/03/2021

Esto es continuación de lo de ayer. Terminé de colocar los ticks
de mis gráficos de manera correcta. Para eso le tuve que poner
explícitamente ejes al gráfico con plt.axes(). Luego con
plt.xlim() y plt.ylim() definí los máximos y mínimos de graficación
de los ejes. Sobre estos ejes armados y de tamaño definido usé
plt.xticks() y plt.yticks() para ubicar a los ticks en el lugar correcto
y ponerle los números que yo quería.

Después de eso simplemente grafiqué como venía haciendo hasta ahora, 
usando el imread y el imshow para levantar y graficar la imagen.

Hoy estuve haciendo la burocracia correspondiente anotando las cosas
acá y en Documentación en ER2. Ahora voy a subir todo con Github y
revisar qué queda por hacer. Y si vale la pena empezar algo hoy o dejarlo
para mañana.

Por lo que veo, la idea es usar pcolormesh. Realmente ahora no me veo con ganas
de hacer esto. Por otro lado, creo que básicamente o lo hago mañana o no lo
hago. Interesante lo rápido que puede pasar una semana.

-----------------------------------------------------------------------------

20/03/2021

Trabajando con el pcolormesh armé el gráfico de fases, ahora se 
ve bastante más lindo. Además, me guardé con el np.savetext 
los datos de la matriz ZZ con los datos de si en ese punto
el sistema tiene Consenso, Polarización o Ideológico.

Ahora lo lindo sería poder hacer que esto tenga los colores
que quiero yo. Y ver de graficar la curva que dice Pablo.
La curva de la región de transición empírica.

--------------------------------------------------------------------------------

23/03/2021

Ayer fue que logré hacer que el gráfico tenga los colores que
quería, le armé el colormap según mis intereses. El que yo armé
tiene básicamente tres colores para todo el rango de valores,
y esos colores están en orden verde, azul y rojo de manera
de que los colores se distribuyan al consenso, polarización
e ideología correctamente. Si no entiendo mal, de querer
poner más colores tendría que agregar más de esas tuplas
de tres elementos que me generen nuevos colores. Por otra
parte, si quisiera un degradado más lindo entre el rango
de valores, lo que tendría que hacer es aumentar el binneado,
es decir el N en el LinearColorsegment.

A este gráfico le agregué en amarillo la curva que define 
la frontera experimental entre el estado de consenso y 
polarización general. También le puse unos gráficos de TdO
de cada región para ejemplificar el comportamiento del sistema.

Cabe aclarar, estoy hablando del que llamo, incorrectamente,
"Gráfico de Fases". En realidad es un mapa de colores de los estados
finales del sistema en el espacio de parámetros Alfa-Cos(delta).

Algo que estuve pensando, es sobre armar este gráfico para el caso de N=1000.
Porque el mapa de colores se arma tomando la lista de Conjunto_Alfa
y Conjunto_Cdelta. Pero estas listas toman en cuenta TODOS los alfa
en la carpeta de archivos con datos. Entonces el problema es que
N=100 tiene más alfas calculados que los N=1000. Es decir, va a
armarme una matriz ZZ más grande de lo necesario. Esto no es un 
problema para los otros gráficos porque cuando paso por esos Alfa
no existentes para N=1000 el código simplemente intenta iterar listas
vacías, lo cual hace que los saltee. En este caso lo que va a pasar
es que mi matriz va a quedar mal armada, porque va a asignar resultados
a valores que no están ahí.

Partamos de que el ZZ se crea como una matriz de ceros, es decir que los
valores que no se retoquen correctamente quedarán como ceros por default.
Es decir, esto no va a generar un error que frene el programa, sino que
lo que va a hacer es graficar cosas equivocadas.
Luego, el ZZ se va escribiendo en cada fila y columna en la medida que
recorre la lista Conjunto_Alfa y Conjunto_Cdelta. En conclusión eso
va a generar que haya elementos de mi matriz que no sean correctamente
reescritos. Por tanto voy a tener que ver la forma correcta de hacer
esto.

Se me ocurre lo siguiente, en vez de usar las listas Conjunto_Alfa
y Conjunto_Cdelta de la manera en que las armo actualmente, podría
usar las keys del SuperDiccionario como una forma de crear esas
listas. Suena como una opción viable.

------------------------------------------------------------------------------

25/03/2021

Ahí revisé la función que asignaba colores. Funcionaba en términos de que
no generaba errores, pero en realidad tenía cierto problema en el cálculo
del resto. Creo que eso podía afectar en distorsionar el cómo estaban separados
los 360º en pedazos, pero no era algo terrible. Igual ahí lo corregí, el
problema estaba en el cálculo del Resto. Básicamente, por lo que entiendo,
este resto resultaba más chico de lo que debería, entonces podría hacer que
el if que separaba en caso de que fuera más chico o más grande que 
Delta/2 diera siempre para el caso de R más chico.

Ya que estoy, voy a cambiar el número de divisiones al doble,
en vez de 72 que sean 144. Así los pedazos de región tienen un
tamaño de 2,5º.

Estoy haciendo muchísimos cambios en el código para guardar una versión del
Graficador ER2.py que sea un poco más eficiente y más importante, que lo pueda
cargar para correr y listo. Lo primero y principal es que modifiqué el 
SuperDiccionario de manera de que sólo arme entradas en los diccionarios
para los Alfas y Cdeltas que el N correspondiente tenga. Esto es porque
mis archivos con N=1000 no tienen tantos alfas calculados como el de
N=100, entonces cuando quisiera armar el gráfico del estado final
en el espacio de parámetros con un mapa de colores eso sería un problema a 
futuro.

Por otro lado, corrí para más arriba el inicio de la iteración en agentes porque
justamente quería meter la creación de la matríz ZZ dentro de esa iteración.
Comenté la creación de las matrices de Superposición y Adyacencia porque me parecen
totalmente innecesarias. O mejor dicho, jamás las usé.

Modifiqué la lista de Opi, ahora lo que tiene no son listas con floats, sino arrays
con floats. Confío que eso acelere algunos procesos. Aprovechando esto es que modifiqué
el cómo se arma la lista de valores Var aprovechando que lo que tengo son arrays y no
listas.

Ya que estoy haciendo varios cambios, podría aprovechar y directamente transformar
el Opi en una matriz. Eso me permitiría construir las listas del
diccionario OdT con muchísima más facilidad y rapidez.

Fijate que la lista PuntosFinales debería poder resolverse de una forma
más simple que con un for. Revisá cómo se hace para concatenar arrays.

Che, ahora que lo miro OpinionesFinales es una lista que es lo mismo
que PuntosFinales, pero no está normalizada. Fijate eso, quizás
podemos juntar todo en un solo array.

El resto lo voy a revisar una vez que haya cambiado mis listas por arrays.
Pero creo que la graficación es la que menos problemas tiene. Me gustan
estos cambios que estoy haciendo. Mañana confío tener esto solucionado
y ya el sábado me pondré con lo que dije que iba a hacer.


--------------------------------------------------------------------------------------

26/03/2021

Probé armar un array de zeros e ir definiendo sus filas a partir
de listas con strings. El tipo parece tomar la lista de string y
convertirla a float todo en una sin dramas. Parece que podría
tomar tranquilamente la lista de Datos y convertirla en un array.

¿Podría hacerlo todo en un saque en vez de con un for? Por lo que
estuve mirando creo que no se puede. Dejémoslo, no vale tanto la pena
eso.

Ya armé Opi para que ahora sea un array, no una lista de listas.
Ahora que esto es un array, creo que ni necesito armar el diccionario
OdT. Creo que eso ya lo puedo sacar directo a partir de mi array Opi.
Sacar ese for va a ganar tiempo, pero más que seguro.

Ahroa voy a sacarme de encima la lista PuntosFinales, me quedo sólo
con la de Opiniones Finales, porque en definitiva esas dos se 
diferencian sólo en un factor de normalización.

Ya puse para que OpinionesFinales se arme concatenando arrays.
Casi me olvido de hacer que los puntos finales de los gráficos
estén normalizados. También acabo de sacarle los números y
ticks a los gráficos. La idea es que cuando quiera armar
mis supergráficos ahora se vean mucho mejor. También les
saqué los títulos dentro de la figura. Pasé los datos del alfa,
cdelta y N a las leyendas dentro del gráfico.

Aún así no borré los comandos que ponen nombres y demás a
los ejes, porque eso lo puedo necesitar después.

También modifiqué algo en la creación del histograma, pero es 
una boludez. Básicamente antes tenía que convertir al
OpinionesFinales en un array, pero como ya lo es me ahorré
ese paso.

Bueno, ya están hechas todas las modificaciones, probémoslo.
Luego de corregir varios errores tontos, y algunos no tanto,
ya el programa funciona bárbaro. Hay un pequeño problema con
el tema del plt.legend, no me está tomando el texto como quería,
pero eso se puede solucionar. Lo importante es que no sólo
funciona, la ganancia de tiempo pareciera ser más que
considerable. Si no hice mal la cuenta, eso debería
permitirme reducir las 8 horas de trabajo que calculé
antes a simplemente 2 horas. Es bastante, valió la pena
el modificar el código.

Bueno, lo último que me faltó fue colocar en un cuadro dentro
del gráfico los valores de Alfa, cos(delta) y N del gráfico
realizado. Con esto ubicado, ya mis gráficos pueden reconocerse 
de qué tratan a pesar de no tener título.

Bueno, mañana lo que haré es rehacer todos los gráficos para
N=100 de manera que no tengan los títulos ni nada. De esa manera
podré armar mi super gráfico un poco más nítido. Luego, los rearmaré
para que queden como estaban y listo. Mientras la pc trabaja en eso
en el Spyder, en el Notebook estaré haciendo los gráficos que charlé
con Sebas y Pablo el martes pasado.

Fijate que el armado del colormap no lo guarda.

-----------------------------------------------------------------------

27/03/2021

Ya mandé a hacer todos los gráficos, y por ahora está funcionando. Esperemos
que mis cuentas hayan estado bien y esto realmente tarde unas dos horas
nomás. Bueno, planteado esto, ahora me queda ponerme a hacer lo que desde
el martes dije que iba a hacer. Los gráficos de caracterización de la región
de transición.

El cuadro que indicaba el alfa, coseno(delta) y el N estaba un poco salido,
ahí lo corregí. Estaba un poco en la duda, pero voy a dejar el legend en
las distribuciones de opinión. En el peor de los casos, después lo corrijo.

Mirando los gráficos que se estaban armado de las TdO me doy cuenta que
al final voy a necesitar el array de PuntosFinales, quiera o no. El 
problema que surgió es que los puntos finales no pueden quedar
en el lugar correcto, porque el MaxNorm asociado varía en cada simulación,
entonces al final los puntos terminan en cualquier lugar y a veces incluso
no tienen líneas que lleven a ellos o las líneas van a parar a lugares que
no tienen punto.

Por el otro lado, armé ya el código para graficar en un mapa de colores 
la varianza en el tiempo de simulación de todas las simulaciones asociadas
a un mismo par de valores Alfa y Cdelta. Lo que se puede observar es que 
los tiempos de simulación crecen únicamente en la región de transición entre
el estado de consenso y el estado de polarización, lo cual me marca que
este parámetro sirve únicamente para distinguir Consenso de Polarización General.

De paso, estoy modificando un poco el gráfico para que el colorbar asociado
sea exactamente la varianza del tiempo simulado y no la varianza de iteraciones.

Cuando termine el spyder de rearmar todos los gráficos, debería mandar a correr
al programa que me arma el SuperGráfico. Antes de eso debería guardar en
un lugar seguro los originales, no sea que esos se me arruinen y no tenga
nada nuevo mejor. Luego los comparo y listo.

Hechos mis nuevos supergráficos, tendría que mandar el Spyder para que
una vez más rearme TODOS los gráficos y con eso dejar ahora sí mis 
gráficos con los nombres en los ejes y el cuadrito dentro del gráfico
indicando qué valores de Alfa y cos(delta) se están usando.

Ya hice todo este tema de armar y rearmar los gráficos, creo que quedó
mejor sinceramente, aunque el cuadrito de anotaciones un poquito tapa
algunos puntos, pero bueno, eso es un detalle.

Veamos de armar un estudio del sistema en función de los máximos que alcanza
como para ver si el máximo valor que alcanza el sistema es un indicador
de su cambio de regiones.

El código está armado y lo mandé a correr. Ahora queda ver si estoy funciona
o si la cagué. El código funcionó bárbaro, el colormap muestra lo que yo decía,
sólo diferencia Polarización General de Consenso,

--------------------------------------------------------------------------------

28/03/2021

Ahora debería armar la parte final, lo del esudio de las opiniones por cuadrantes.
Tengo entonces que tomar las OpinionesFinales, anotarlas en un vector según si
están en primer, segundo, tercer o cuarto cuadrante y luego con esos números 
armarme un histograma.

---------------------------------------------------------------------------------

29/03/2021

Ayer había más o menos terminado el código. Hoy lo pasé a Python
y empecé a armar todos los gráficos de distribución de opinión por cuadrantes,
el gráfico de Máximos en EP y Varianza de tiempo de Simulación.

El código que armé no resultó tan adecuado, removí todo el trabajo
en axes y subplots, y lo pasé a trabajar directamente con fig. Luego
descubrí que estaba armando mal los bottom de los gráficos de barras,
tenía mal definido el nombre del gráfico y eso me armaba sobre un 
único gráfico todos los dibujos. Cuestión, alto bardo se me generó.

Lo solucioné, armé los gráficos correctos, ahroa sólo me queda
pasarlo al PowerPoint para charlarlo con Pablo y Sebas. Por
desgracia, me da que excepto por el gráfico de las varianzas,
los otros dos son un poco inconclusos, no parecen mostrar mucha
info que se pueda usar para estudiar el sistema.

--------------------------------------------------------------------------------

02/04/2021

Desde ayer estoy intentando hacer los cálculos para el sistema
con N=1000. En general venía trabajando bien, pero el programa
tuvo una imposibilidad para poder trabajar con unos archivos
cuyo peso estaba entre 1 y 2 GB. Por lo que lo que hice fue
directamente borrar esos archivos. Fueron 5 archivos,
tres del Alfa 0,11 y Cdelta 0, y otros dos de valores 
cercanos. Técnicamente afecta a la estadística eso, pero
al carajo, no los pudo ni leer. Qué sentido tiene mantenerlos
si no los puedo leer. Vamos a ver si ahora el programa puede
terminar con todo.

Al final logré hacer los gráficos de todas las TdO. Mañana queda
hacer las Variaciones Promedio y si puedo los gráficos de Caracterización
de la región de transición.

-------------------------------------------------------------------------------

03/04/2021

Hoy seguí armando los gráficos para el sistema con 1000 agentes.
Ya tengo todos los gráficos de Variación Promedio de las
opiniones, los de Distribución de las opiniones y ahora estoy
haciendo los de Distribución por cuadrantes, y los mapas de colores
de los Máximos y Varianza de Tiempo de Simulación en EP.

Decidí que ahora me iba a poner a cambiar el gráfico de los Máximos
de Opiniones en EP para que lo que grafique sea el valor esperado
de los sujetos. La idea es ver que los agentes en efecto tienden a K*<k>.

¿Cuál sería el valor esperado? No es el promedio, ¿o sí? Sí, termina siendo
un promedio. Modifiquemos eso en el código entonces.

Copié el código de Caracterización Transición en Análisis Datos y lo modifiqué
para incorporar el cálculo de los promedios. Ahora vale la pena mencionar
que el cálculo de los promedios está dado en el caso de que primero tomo
todas las opiniones y les tomo valor absoluto. Hago esto porque lo que
quiero ver es directamente si el sistema tiende a K*<k>, y como el sistema
tiende a opiniones cuyos valores de T1 y T2 son iguales, entonces
no me afecta meter todo en una misma bolsa. Incluso, si tuviera dudas de
esto, podría armar dos mapas de color, uno para T1 y otro para T2, y ver
que efectivamente en promedio tienden a lo mismo.

---------------------------------------------------------------------------------

04/04/2021

Voy a dejar en posibilidades la idea de graficar dos mapas de colores,
uno para cada tópico. Arranquemos primero con el tema de la entropía.

Se terminó el cálculo de los valores medios, y parece que correctamente
el sistema tiende al valor K*<k>, así que eso va bárbaro.

Sobre la entropía, no llegué a armarlo hoy. Cuestión que lo pensé,
la idea va a ser armar un array de Probabilidades, que lo podés
tomar del Yhisto, ese que usas para graficar la distribución por
cuadrantes. Luego, a Probabilidades le tomás el np.log2() cosa
de que te quede el log2 de cada uno de los elementos. Luego haces
un dot product, supongo, con el array de Probabilidades y a eso lo
multiplicás por -1 y listo. Estás hecho.

Eso te va a dar la entropía para un conjunto de valores Alfa y Cdelta.
Luego, le asignás eso a la matriz ZZE, y por último lo graficás en el 
mapa de colores con un pcolormesh.

----------------------------------------------------------------------------------

05/04/2021

Bueno, estoy graficando el tema de la entropía ahora. Hice unas modificaciones
que son las siguientes.

En el tema de graficación de la varianza de los tiempos de Simulación, le tomé
logaritmo en base diez a los valores para graficarlo en términos de números más
sencillos de visualizar. Para eso, al valor de la varianza graficada le sume
1 primero ya que algunos valores de Varianza estaban muy cerca de cero y entonces
el logaritmo hubiera cadía a valores muy negativos y no hubieramos ganado nada.

Sobre la entropía, hice tal cual lo que escribí más arriba. Usé el np.matmul
para hacer el producto de los arrays y desde ahí lo ubiqué en la matriz
ZZE. Al final copié el código de graficación de Promedios para graficar
esta nueva matriz y listo.

Estuve mirando los gráficos que tengo, increíblemente no hice un gráfico de
fases, ese tengo que hacerlo. También tengo que hacer los gráficos recopilación
del resto de gráficos, eso lo puedo hacer con Python en un pedo. Eso es lo que
necesito para mostrar mañana.

------------------------------------------------------------------------------------

19/04/2021

En estas dos semanas estuve entre armando el informe, tomando más datos y
replicando los gráficos que tengo para N=1000. Hoy aprovecho para decir
que había armado un nuevo programa .py que el objetivo es que armara
directamente la tabla de gráficos en vez de tomar imágenes y cargarlas.
El problema es que el gráfico no se pudo hacer todo, así que hay dos
opciones, dejarlo para cuando lo pasemos a las máquinas, o no volver a usarlo
nunca más. Lo que se me ocurre es primero volver a probarlo, esta vez
con una tabla de 2x2, a ver si eso me lo corre. Confío en que sí.

Ahora estoy viendo de hacer los gráficos que necesito para la tabla
de TdO, y después haré los gráficos de Variaciones Promedio.
Todavía faltaría entonces hacer lo de generar datos usando
una semilla fijada para ver si por casualidad vuelve a surgir
un dato absurdamente largo en comparación con el resto.

-------------------------------------------------------------------------------------

20/04/2021

Ya armé el gráfico de Variaciones Promedio para Alfa=0,9 y Cdelta=0,6
usando las 100 iteraciones que calculé. Por lo visto, sigue habiendo
una única instancia que se separa mucho del sistema. Igualmente, por
si acaso, voy a realizar otras 100 iteraciones, pero esta vez definiendo
la semilla random a partir de un número entero que es el número de iteración.
La idea es ver si vuelve a surgir este extraño caso en el cual el sistema
tarda una cantidad absurdamente grande de tiempo en converger.

Mientras esto se resuelve, podría intentar revisar la librería de 
redes en Python, la nx.network, o algo así. La idea es ver de poder construir
redes en Python con ciertas propiedades, guardar las redes de adyacencia
en alguna carpeta y usar esas redes para armar las redes en C a la hora
de trabajar. Espero que funcione

--------------------------------------------------------------------------------------

23/04/2021

Voy a empezar a probar el armado de redes en Python con la biblioteca networkx.
Arranquemos armando redes y logrando que el programa las levante correctamente.
Entonces el trabajo de esta semana es armar un código en Python que arme las
redes, tanto de Erdos-Renyi, las random, las de Barabassi y las regulares.
Es decir, son 4 tipos de redes distintos. A partir de ahora me parece
razonable hablar de una nueva fase del trabajo, y por tanto cambiarle
el nombre a los archivos que vaya usando y guardar los que deje de usar de
manera prolija.

En algún momento tengo que ver cómo me organizo lo que hablamos con Pablo de
ir leyendo bibliografía y hacerme resúmenes de la biblio. Lo principal es concentrarme
en la biblio de Baumann y la de la red de Actividades.

------------------------------------------------------------------------------------

26/04/2021

Retoque el Graficador ER2, eso tenelo en cuenta cuando lo vayas a copiar todo para
la nueva fase. Igual, si lo mirás con detalle creo que te vas a dar cuenta de los
detalles allá y acá.

En python estuve intentando analizar un poco el distema que tiene la gran varianza.
Logré establecer que el problema está dado por sujetos cuya distancia con su estado
final es MUCHO mayor que la del resto. Ahora lo interesante sería poder estudiar ese
sujeto y ver cuales son las condiciones de sus primeros vecinos que hacen que 
tarde tanto en caer a cero. Igual, repito, lo lindo sería primero identificarlo.
Hecho eso, podría intentar mirar su columna en la matriz Opi, eso me ayudaría para
tener claro QUÉ pasa con sus valores.

Supongo que mañana me pondré a trabajar con el generador de redes y veré de investigar
al sujeto o los sujetos que están muy alto en la distancia hacia su estado final.

-----------------------------------------------------------------------------------------

29/04/2021

Ahí armé en Profunc un código que se encarga de crear redes de Erdos-Renyi y de guardar
la matriz como un txt de una única fila con todos enteros. Así que lo siguiente es armar
una función en C que se encargue de levantar los datos y arme la matriz de adyacencia
con eso.

Una vez teniendo eso, me voy a encargar de preparar el programa para que me cree todas las
redes que pueda necesitar, ya sea de Erdos-Renyi, Barabassi y Random Regular. Lo que no
recuerdo es cuántas redes debería crear. ¿Es una para cada elemento de cada ensamble?
Preguntar esto a Pablo.

Después debería ver de hacer lo de estudiar a dónde tienden los sujetos del elemento del
ensamble que tarda tanto en converger.

----------------------------------------------------------------------------------------

04/05/2021

Estoy trabajando en Prosem para armar una función que pueda levantar de un txt la matriz
de Adyacencia. Me cuesta un poco llamarlo una función porque básicamente lo que hace
es usar el fscanf y con un while lo recorre. Y listo, eso es todo.

De paso, de casualidad descubrí que había un error en el armado del txt. Menos mal que
lo vi, casi me mando alta macana. Básicamente elegí mal el sustituto para el newline
y algunos pares de números terminaban juntos. Ahora que lo pienso, me extraña que eso
no se haya notado.

Cuestión que estaba pensando en agregar a la función esta que avise si lee de menos o de más,
como para avisar si agarré un archivo equivocado. Para saber si leí de menos, puedo simplemente
comprobar si alcancé el EOF del archivo y si el indice es menor o no al tamaño del vector.

Ahí agregué algo que debería decirme si el archivo tiene una matriz más grande 
que el vector. Eso es por si me mando alguna cagada para cortar el programa y volver
a arrancar. Lo probé y dentro de todo funcionó bien. Copado

----------------------------------------------------------------------------------------

05/05/2021

Intentemos escribir este código que levanta los datos de un archivo en forma de función.
Además veamos si podemos encontrar la forma correcta de pasarle el nombre del archivo a levantar.
Eso va a definir la forma en que voy a guardar los datos.

------------------------------------------------------------------------------------------

06/05/2021

Continuando lo de ayer, ahí rápidamente probé cómo armar un string adecuado de manera
que sea los nombres que yo necesito que sean. Entonces puedo armar los nombres de 
mis redes armadas con la librería networkx de una manera específica, se me ocurre
"TIPORED_N=$_Gm=$_ID=$.txt". Luego, usando sprintf puedo poner el nombre del archivo
en un array de char. (¿No podría hacer lo mismo con un puntero? Mejor preocuparme
de eso en el futuro). Cuestión que con eso puedo levantar los nombres de mis
redes siendo que el TIPORED lo paso como input al sistema, como un for extra
en el Bash, el N viene en el for del Bash, Gm lo agrego e ID lo saco dentro
del mismo archivo C a partir de un Random, habiendo definido de antemano cuántas 
redes totales fueron creadas con Python. Bien, me gusta la idea.

Ahora debería ver si puedo hacer que se levante el archivo si el archivo está fuera
de la carpeta donde está el Main original. Probemos eso. Eso funciona genial, es simplemente
escribir el path relativo y listo. Ahora la pregunta es, 
¿Pongo todas las redes en una única carpeta? ¿O las pongo en carpetas diferenciadas?
Cuestión, ya tengo todo lo necesario para automatizar esto. Ahora tengo que hacer
el trabajo de la burocracia. Esto es una paja. Pero dale, ya tenemos lo importante
hecho.

El trabajo de Burocracia es el siguiente:
5) Ampliar la función que arma redes para que arme redes de Barabassi,
redes random con grado fijo y redes simplemente random.
6) Definir cuántas redes armar y guardarlas todas en una carpeta. (O varias).
7) Preparar las carpetas en las que voy a guardar los archivos de análisis
de las redes.
8) Cargar los archivos a Github.
9) Revisar los archivos de Documentación que se encuentran en ER2, para
ver si falta agregar info al respecto de los gráficos armados.
10) Anotar todo esto acá. (¿De verdad llego con todo esto para el
martes que viene? Lo dudo, pero si hago hasta el 8, ya tendré
bastante que charlar con Pablo, aunque no tenga resultados
para mostrar.)
11) Armar ese gráfico sobre el elemento del ensamble raro, el gráfico
de la opinión en función de la iteración. La idea es ver que la mayoría
de los sujetos se ubican rápido en su valor final y que hay uno o algunos
sujetos que tardan en llegar.

------------------------------------------------------------------------------------------

07/05/2021

Mientras fui resolviendo puntos del trabajo de Burocracia, tuve la brillante
idea de ir borrándolos. Ahora me arrepiento un poco. Cuestión que ya hice
los primeros cuatro puntos, que básicamente eran separar el antiguo programa
principal en una carpeta en programas y documentarlo, guardar el código usado
en Prosem en Archivo para cuando lo pueda necesitar, "Armé" una función que
devuelva números entre 0 y N, lo cual es hacer rand()%N y lo último fue
pasar la función de Lectura_Adyacencia al nuevo programa principal.

Voy a tener que investigar un poco sobre redes de Barabassi de nuevo
para poder ver bien cómo usar la función que crea redes de Barabassi
de manera de que mis redes tengan el grado medio deseado.

----------------------------------------------------------------------------------------

08/05/2021

Estuve revisando esa función de generar gráficos de Barabasi. Si bien
me genera muchas dudas el hecho de que el grado medio sea constante para
cada valor de m y n, la función parece claramente armar una red de Barabasi,
o eso es lo que me muestra el gráfico de distribución de grados que armé
al respecto.

Dicho esto, el programa que armé ya me guarda redes de Erdos-Renyi, Barabasi
y Regular Random en las carpetas que armé, así que con eso ya tengo el punto
5 de los 11 que me propuse hacer hechos, ahora debería ver de definir
cuantas redes tengo que crear. Mejor me hago unas notas sobre eso y lo charlo
con Pablo y Sebas mañana.

Ahí probé guardar mis archivos, va barbaro. Cada archivo de 1000 agentes pesa
2 MB aprox. Supongamos que cada par de valores alfa y delta va a generar ensambles
de 20 simulaciones. Además supongamos que tengo 25 valores de alfa por 10
valores de delta. Eso me da para un único tipo de red un total de 5000 simulaciones
totales. Supongamos que en total voy a tomar una cantidad de redes igual a la
mitad de simulaciones totales, entonces tendré 2500 redes por cada tipo de 
red. Luego, el total de redes que tendré serán 2500 y eso multiplicado por
2 MB me da un total de 15000 MB. Que dividido por 1024 no superan los 15 GB.
¿Cómo hice la cuenta antes, no entiendo?

Bueno, igualmente queda la pregunta de cuántas redes deberé armar. Charlarlo mejor
con Pablo y Sebas mañana.

---------------------------------------------------------------------------------------

09/05/2021

Ya actualicé los archivos de Documentación.txt de ER2. Puse todo lo que habíamos visto
hasta ahora al respecto. Así que ahora sigue el subir todo esto al Github. Eso quedará
para después de la charla con Pablo y Sebas. Ahora mejor intentar pulir lo que tengo.

---------------------------------------------------------------------------------------

14/05/2021

Ya subí todos los archivos a Github. A partir de ahora entonces comienza la fase del trabajo
"Redes Estáticas". Lo que tengo que hacer ahora es preparar el programa en C para
que levante las matrices de Adyacencia de los archivos de txt.

Organicémonos:
1) Armemos las matrices de Adyacencia de todas las redes que quiero. Necesito
10 matrices.
2) Preparemos el archivo del programa C para que trabaje los archivos.
3) Preparemos el archivo Bash para que corra todo en una iteración.
4) Armemos las carpetas donde guardo los archivos de datos de la red.
5) Armemos un archivo de Python que levante estos datos y me arme los
gráficos.
6) Trabajemos los datos y armemos todos los gráficos.
7) Pasemos los archivos a la computadora de la facultad.
8) Preparemos el inicio de la charla sobre redes de Actividad.
9) Analicemos el tema del elemento del ensamble para ver cuáles sujetos tardan
en converger.

Completados:
Punto 1.
Punto 2 (Aunque hay que ver cómo armo la cosa en Bash que quizás tenga que 
reorganizar algunos argv. Así que atento a eso, pero el resto debería estar.
Por si acaso, probá que lo del argv igualado a un string lo tome bien).
Listo, solucioné el tema de comparación de strings.

--------------------------------------------------------------------------------------

16/05/2021

Ya realicé el punto 3, el archivo de Bash está listo y el archivo C está arreglado
de manera de que los inputs que paso coincidan con lo que dice que cada variable recibe
en el main. Siempre queda ver qué pasa cuando lo mande a correr, pero en principio
está todo bien.

Ahora, un detalle no menor es que tengo que armar correctamente el archivo de Mover para
que los datos se me guarden en el lugar correcto. ¿O debería directamente ya mandarlos
al lugar correcto? Decidiré eso luego de haber creado las carpetas donde guardaré los
datos que haga de cada red.

Ya está hecho el punto 4 y modifiqué el main para que ahora los archivos al crearse
automáticamente vayan a parar a la carpeta que les corresponde, con el gigantesco
nombre que les corresponde, el cual tendrá ahora no sólo la cantidad de agentes
de la red, el valor de alfa, Cdelta y el número de iteración, sino también el grado
medio y el ID que los asocia con la matriz de adyacencia correspondiente.

Sobre eso, seguiré printeando la matriz de Adyacencia en el archivo de salida,
simplemente para no modificar tanto los códigos que levantan datos desde ahí, y 
porque además es un seguro de comprobar que las matrices de adyacencia son las que
yo digo que son en el ID.

Haciendo unas simples cuentas, estuve viendo que básicamente armar todos los archivos
necesarios me va a tomar más de 100 horas, todo considerando que no surjan los casos
de transición donde el tiempo de simulación escala aberrantemente. Así que estaré unas
2 semanas calculando cosas básicamente.

Ahora me voy a poner a armar el archivo de Python que levante esto, pero
más allá de eso no voy a poder hacer para el martes este, no sé si voy a llegar
para el otro martes tampoco. Esto va a tomar su tiempo. Algo que es interesante
preguntarse es si voy a poder abrir varias terminales a la vez que conecten con
las pcs de la facultad. Eso podría ayudar incluso más a que el programa se corra
más rápido.

Estuve trabajando sobre el archivo de Python para leer los datos, estoy trabado en
que estoy pensando si hay una forma de armar el SuperDiccionario de forma que
me permita ahorrar tiempo. Debería poder resolver eso, pero la verdad que no se
me ocurre. Podría simplemente mandarlo a correr agregando el Grado medio y listo.
Pero me gustaría intentar encontrar una forma diferente de que funcione.

-----------------------------------------------------------------------------------

17/05/2021

Voy a pasar esos puntos a realizar a Objetivos, y organizar el archivo de Objetivos.
Después unas notas sobre lo que tengo que hacer hoy y charlar con Pablo mañana.
Y después si, a preparar lo de red de actividades.

Probé pasar archivos a la máquina remota, pero estoy trabado en que necesito el
password de mi cuenta de login.df.uba.ar. Ya mandé un mail a compu@df.uba.ar al
respecto. El código que estoy usando para pasar archivos es:
scp -o 'ProxyJump faviodiciocco2@login.df.uba.ar' 'path local' 'faviodic@setubal.df.uba.ar:path remoto'.
Según leí en internet, para copiar un directorio voy a tener que agregar la opción
-r al comando para que me copie toda la carpeta.

Con Pablo tengo que hablar entonces sobre el resumen que tengo que enviar a
Trefemac a través de la Intranet de AFA y sobre el tema de postularme para
beca Conycet.

----------------------------------------------------------------------------------

18/05/2021

Ya logré pasar archivos directamente a las pcs de la Facultad de Setubal.
En cuanto termine de pasar las cosas de Programas Python, ya lo podré
poner a correr tranca. Solo tendré que retocar un poco el archivo Instanciar.sh,
pero nada grave.

Ahora, hablando del paper de Redes impulsadas por Actividad:
1) Importancia de poder estudiar procesos en los cuales la escala de tiempo de
variación de la red es similar a la escala de tiempo en la cual el proceso dinámico
se desarrolla.
2) Entender correctamente los tres tipos de Redes que se están analizando.
3) Definición Exacta del potencial de actividad "x_i".
4) Preguntar quizás el concepto del Eta
5) Algoritmo de generación de las redes.
6) Mostrar cómo el grado medio en cada instante t depende de la actividad.
7) ¿Debería hacer el cálculo para ver que el grado de un agente i es lo que muestra la página 3?
8) Cosas que el modelo no contempla: Los nodos pueden tener memoria, Persistencia de enlaces
y ¿Conexiones múltiples o links pesados pueden ser relevantes?
9) Mostrar cómo se integran las redes.
10) Análisis del caso de SIS
11) Figura de su modelo que se parece a lo visto en PRL.
12) La ecuación 4 es una muestra de cómo se puede analizar el modelo SIS en su escala de tiempo natural.
13) La figura 4 muestra en el panel B esta idea de que los resultados de difusión en redes que varían
en una escala temporal similar al proceso dinámico dan resultados cualitativamente y cuantitativamente
distintos a los casos en que se consideran redes fijas formadas por estos mismos procesos dinámicos
de variación de redes. La bibliografía 30 y 31 supuestamente responde por estas afirmaciones.
14) El panel A muestra que hay un buen acuerdo de lo que es la ecuación 2 sobre la relación
entre la distribución de grado en un dado tiempo T y la distribución de actividad apropiadamente
reescalada para el caso de la red de Twtitter.

Todo esto eran como los puntos principales para charlar sobre el paper de Actividades.
Al final me comprometí para hacer una charla dentro de dos semanas al respecto en las reuniones
de grupo. *Everything's Fine*.

Cuestión que para esa charla también estaría copado mostrar las cosas que tengo hechas y a partir
de eso hablar un poco de lo hecho en la tesis. Entre las cosas que charlamos, estaba lo de
revisar los modelos de Fitness, como modelos similares a lo de redes impulsadas por actividades

Hay 4 cosas mucho muy importantes que tengo que hacer ahora, y lo voy a pasar a objetivos:
1) Armar una carpeta en mi carpeta de Drive de Tesis con papers para usar de referencia y un
google doc con resúmenes de los papers que voy leyendo.
2) Implementar el modelo de redes impulsadas por actividad. (Que te ayude Dios
a lograr esto en 2 semanas).
3) Tener todo corriendo en Setubal.
4) Burocracia infernal y resolver pendientes.

------------------------------------------------------------------------------------------------

20/05/2021

Subí una carpeta con papers a la carpeta de tesis con los resúmenes. Tengo que empezar a releer
los papers y reducir los resúmenes a unos párrafos.

A mi archivo main le modifiqué el dt a 0,01 y modifiqué en la función Escribir_d para que 
escriba sólo 6 decimales de manera de no ocupar tanto espacio. Esto me parece que está
justificado porque ahora el Criterio de corte es 10^(-3).

Voy a hacer los cambios que tengo anotados en optimización para intentar reducir el tiempo de trabajo.

----------------------------------------------------------------------------------------------

21/05/2021

Ya me armé la función de interpolación en Prosem. Me hice eso y unas funciones auxiliares que
acompañan a eso. Agregar esto al programa va a requerir incluir varias funciones, un nuevo
struct y unos archivos nuevos. Voy a ver de correctamente empaquetar todo esto. Es importante
que la Tabla_Valores_TANH esté en la misma carpeta que el archivo de Opiniones.e.

Primero, hablemos de la Tabla_Valores_TANH. Esta tabla tiene un millón tres números anotados.
El primer es -1, el último es 1 y todo lo que va en el medio son números que lentamente
oscilan en ese intervalo, correspondiendo cada uno a un valor de tanh para un dominio entre
-5 y 5.

Ahora, la implementación de la función de interpolación cuenta con la implementación de un nuevo
struct, el struct Tabla. Este contiene el puntero a un vector que va a guardar todos los valores
de la tabla, un int con el tamaño del vector y creo que nada más. Me parece que vale la pena la
utilización de un struct aparte porque así diferencio el struct que tiene datos de la red, el que
tiene los parámetros y el que tiene los datos de la tabla.

Para levantar los datos de la tabla en el puntero uso una función que levanta datos con fscanf.

También armé una función que se encarga de a cada valor de argumento x asignarle los índices
del vector que le corresponden. esta función recibe el argumento y dos punteros a índices
sobre los cuales escribe el índice.

Como yapa, me armé funciones que evalúna el máximo y mínimo de dos valores double y devuelven
un double. Esas funciones las armé con operadores ternarios.

------------------------------------------------------------------------------------------------

22/05/2021

Estuve probando la interpolación un poco más y funciona perfecto, el empaquetamiento está muy bien
también. Ahora a agregar todo esto al main y mandarlo a correr a ver cuánto gané con esto.
Lo siguiente que me gustaría mirar es el RK4, a ver si se pueden reducir cuentas ahí. Esa 
es una fábrica de cálculos.

Voy a tener que definir el struct desde general, porque sino esto no va a poder resolverse.

Resolví el tema de la interpolación. No gané nada. Sigue tardando 19 segundos en resolver esto.
Que hijo de su madre. Tanto quilombo para nada. Funcionó a la primera y nada. Por lo que puedo
ver de uno de los archivos calculados, parece que el programa funciona bien.

Ahora que lo miré de nuevo, el sujeto tardó 730 aprox, cantidad de iteraciones en converger. Y
yo le había dado 500 iteraciones extra para que converja. O sea, que la cantidad de iteraciones
extra era 2 veces y medio la cantidad de iteraciones necesaria para converger. Saquemos eso y
veamos cuanto tarda en converger. Creo que el mayor tema a resolver está en la cantidad de
cálculos que el sistema realiza para evolucionarse en el RK4. Hay que ver si se puede reducir eso.

-------------------------------------------------------------------------------------------------

23/05/2021

Esto me olvidé de anotarlo en el día, pero mandé a correr el programa en la pc de setubal
con un barrido más grueso cosa de tener algunos datos en una semana y ya con eso empezar
a analizar algunos datos. Lo que mandé a correr es un programa que para las tres redes
calcule simulaciones con un alfa entre 0 y 4 con un paso de 0,1, coseno(delta) entre
-0.5 y 1 con un paso de 0,1, el dt es 0,01, 60 iteraciones, para redes con grado medio
de 4,8,12 y 16 y criterio de corte de 0,001, con 50 iteraciones extra. Esto debería
tardar entre 5 y 8 días. Quizás 10. Así que para eso del viernes 04/06 deberían estar todos
los resultados.

-------------------------------------------------------------------------------------------------

25/05/2021

Voy a empezar a calcular redes para Barabasi, Erdos-Renyi y quizás RandomR en mi pc
así puedo poner a prueba los programas de Python y tenerlos listos para cuando estén
hechas todas las simulaciones en Setubal.

Hablando con Pablo y Seba, la idea entonces es ir guardando de los archivos la semilla
y el estado final del sistema, no guardar todos los otros datos. Haciendo eso paso a
que los archivos en vez de pesar 8 megas, me pesen sólo 19 Kb. Algo claramente
más razonable. Estuve haciendo unas pruebas en la función original para guardar
correctamente la semilla y ver que funque bien. Al final el tiempo que gané no
era tanto. O al menos esa es la sensación que me dió. Igual trabaja un poco más rápido.

Ahora, borré las redes que había armado antes. Así que me voy a armar las redes que necesito,
mañana mando a correr el programa en setubal y después me fijo algo que me tarde un
cierto tiempo, y me fijo si me tarda más en setubal o en las otras. Una vez que tenga
eso corriendo, lo primero que tengo que hacer es revisar todos los papers que en su momento
leí y rearmar los resúmenes. LUEGO, me pongo a implementar el tema de redes de actividades.

De paso, todavía tengo que subir las cosas a Github, que no subí desde que
hice la implementación de la función de interpolación que tampoco me trajo
tantas satisfacciones.

------------------------------------------------------------------------------------------------

03/06/2021

Hago la suba de archivos a Github. Probé usar el scipy.stats que me dijo Sebas,
pero no lo pude ahcer funcar. Voy a probar la función que propuse yo por ahora y 
veré si con eso lo puedo hacer correr.

Al final la función que yo propuse no funcionó y no supimos como adaptar el código
de scipy.stats para que funque, así que Sebas me dió una mano para armar una función
que asigne la actividad de forma correcta.

----------------------------------------------------------------------------------------------

06/06/2021

Voy a armar la función que crea la matriz de Adyacencia en cada paso temporal.
La idea de esta función es que reciba la matriz de Adyacencia, la limpie totalmente,
luego active uno por uno a los agentes y a los que active conectarlos mediante
una segunda función. La función está casi terminada, aunque igual queda probarla.
Si funciona ya puedo mandar a correr algunos estados para ver qué ocurre, cuanto
tardan y cómo quedan.

---------------------------------------------------------------------------------------------

07/06/2021

Terminé la función que arma las redes según valores de actividad. La realice con
el criterio de que cada nodo que se activa forme m nuevos enlaces, siendo que 
no se puedan repetir enlaces con nodos previos. Había un error en un punto
en el que no estaba reduciendo correctamente el vector con agentes disponibles,
pero eso ya se arregló y parece funcionar perfecto.

Ahora tengo que hacer lo siguiente:
1) Armar la carpeta donde voy a guardar los nuevos resultados.
2) Pasar el código de Prosem a Archivo.c
3) Guardar el código de src en Programas.
4) Actualizar el archivo de Documentos.
5) Implementar esto en el programa main
6) Mandar a correr el programa como para ver cuánto tarda
7) Subir todo a GitHub
8) Preparar el archivo py que debería armar los gráficos

Realizados: 1); 2); 3); 4); 

-----------------------------------------------------------------------------------------------

08/06/2021

Continúo con lo de ayer, ya realicé el 5) y el 6)
Hagamos el 7 y salteemos el 8 por ahora así preparamos cosas de la 
presentación.

Entre cosas, cabe mencionar que el programa parece estar teniendo
problemas para llegar a la solución del sistema en el caso de alfa = 0,9
y Cdelta = 0,2. Según mis cuentas se encuentra cerca del alfa crítico.
Me parece que la aleatoriedad con la que se arman las redes hace
que la competencia entre caer al cero y polarizar se vuelva mucho
más extensa.

Ya subí todo a Github. Ahora me queda seguir con lo de la presentación.
Hablando con Pablo y Sebas el tema de que el sistema no convergía a nada lo
vamos a intentar resolver proponiendo que la red se actualice por cada 
paso temporal discreto. Bueno, ahí lo mandé a correr, veamos que tal
resulta esto.

Por otra parte, ya que hoy se termina de correr lo que mandé sobre las
Redes Estáticas en Oporto y no voy a poder esta semana ponerme
a armar los gráficos al respecto, lo mejor va a ser que el sistema
arme algunas listas extra, como para aprovechar el tiempo y no tener
la máquina parada. Ya tengo valores de Cos(delta) entre -0,5 y 1 de a 0,1,
Grados medios entre 4 y 16 de a 4, Alfas entre 0 y 4 de a 0,1 y todo con
60 iteraciones para redes de Barabasi, Erdos-Renyi y Random Regulars. Ahora
lo que voy a hacer es un barrido más fino entre Alfa 0 y 0,2, de a 0,01.
Eso significa calcular para Alfa entre 0,01 y 0,09 + 0,11 y 0,19. En total
eso son 18 alfas. Si lo separo en 6 programas, eso significan tres alfas
para cada uno. Suena bien.

--------------------------------------------------------------------------------

15/06/2021

Ayer hubo un corte de luz en la Facultad, así que lo que había mandado a correr
se cortó. Por suerte sólo faltaba el tema de las redes Random. Así que ahora las
mandé a correr esas y listo.

Lo que creo que debería hacer de acá a la siguiente semana es armar el programa que 
calcula la probabilidad de juntar agentes según homofilia y luego el programa
de Python que realiza los gráficos necesarios.

----------------------------------------------------------------------------

18/06/2021

Hoy estuve (muy lentamente) trabajando en una función que se encargue
de calcular la probabilidad de interacción de dos agentes dada la homofilia entre ellos.
La cosa trabaja, pero todavía no estoy seguro que las cuentas estén bien hechas,
eso tengo que revisarlo mañana con más detalle. En Python armé un programa
para poder calcular estas probabilidades más al detalle.
Veré de ir usando más elementos de control para ver que esté todo bien.

---------------------------------------------------------------------------------

20/06/2021

Revisé mejor la función que calcula probabilidades. Al final el problema no era cómo
calculaba los valores, sino que cuando avisaba a cuales agentes conectaba y cosas,
los números que decía que conectaba no eran los correctos.
Lo revisé con el programa que hice en Python y haciendo algunas cuentas en la calculadora.
Por lo visto, funciona perfecto ahora, la distancia entre agentes se calcula bien y la
probabilidad también. El factor de Normalización está comprobado con Python, no lo hice
yo a mano porque eso era un re bardo.
En el código hice una modificación a la generación de opiniones para que los valores
de opiniones sean enteros, así me resultaba más fácil el traspasar los datos a
Python y hacer las cuentas. Esta modificación no está en la función principal.

Ahora lo que tengo que hacer entonces es pasar el programa a Archivo.c para guardarlo
y documentarlo correctamente. Luego implementar la función en el archivo principal y ya
mandar a correr un barrido grueso del sistema con los datos de Baumann, haré ensambles de
5 elementos sólo para probarlo y veremos que tal sale. Terminado eso ya sí me tengo
que poner a armar los archivos de Python que correctamente me armen mis figuras con
los datos de RE y después leer alguno de los papers que Pablo me pasó.

El programa está funcionando. Cabe preguntarse si funciona bien, pero eso lo veremos
más tarde. Ahora, está teniendo el problema de que tarda mucho en armar la matriz
de Adyacencia. Al final lo que temía está ocurriendo. El sistema puede tardar mucho tiempo
en armar las conexiones. Eso puede ser porque hay pocos agentes con probabilidad útil
para conectarse, o directamente porque no los hay porque están todos lejos entonces
tarda mucho en conectar. Se me ocurre que una forma de acelerar esto es que en vez
de a lo bruto lanzar muchas veces la moneda hasta que uno logre conectarse, lo mejor es
lanzar la moneda una sóla vez, realizar algunas cuentas extras pero necesarias y listo.
Escribamos la idea y después pasémosla al Powerpoint.

Ya tengo una idea anotada sobre qué hacer. Me dejaré para hacer eso para la otra semana.
Ya mañana me pondré a hacer lo del archivo Python y el martes armaré en el Drive
mi idea sobre lo que planeo hacer con la implementación de la homofilia para reducir
tiempos.

-------------------------------------------------------------------------------------

21/06/2021

Voy a armar los archivos de Python que me crean los gráficos para redes estáticas.
Estos archivos tienen que levantar todos los nombres de los archivos y graficar
8 tipos de gráficos distintos. Por tanto requiero 8 archivos distintos. Los voy
a poner en una carpeta propia.

Ya armé el código que fabrica el superdiccionario, así que ya eso es el paso clave.
Ahora voy a usar esto en la base de todos los códigos para armar todos los gráficos.

----------------------------------------------------------------------------------------

23/06/2021

Hablando con Pablo ayer entonces decidimos por ahora al hacer las TdO no graficar
los valores normalizados sino graficar los valores por lo que dan. El programa
de graficación ya está bastante bien, aunque el algoritmo de detección del estado
final está teniendo algunas fallas. Le surgen situaciones donde por uno o dos
puntos levemente corridos de la diagonal, el código detecta polarización cuando 
claramente se trata de un estado ideológico. Así que habrá que ver cómo tomar
en consideración si el resultado final es ideológico o Polarización.

Se me ocurren dos ideas. La primera es darle un cierto peso a los distintos
estados, de manera de que si solo unos pocos sujetos son de polarización entonces
no los tome en cuenta. Eso no suena muy simple.

La otra, más rápido y que creo funcionaría igual de bien, es aplicar esto según
cada una de las iteraciones, de manera de que califique a cada iteración como Ideológico
o Polarización y de ahí luego haga un promedio. Eso debería ser más útil.

-------------------------------------------------------------------------------------

25/06/2021

Estuve armando el código para acelerar el proceso de selección de agentes
en el armado de la red de actividad. Para esto la idea es meter dos
nuevos punteros que guardan los valores de distancia^-beta y de probabilidades
de los agentes. La idea es considerar que a cada agente le doy una cantidad
de rifas de manera tal que esa cantidad de rifas representa la probabilidad
de que cada uno sea seleccionado. Esa cantidad de rifas se representa
por una región del intervalo [0,1]. De esta manera, lanzo un número
random una única vez y luego reviso dónde cayó (Es decir, qué número
de la lotería salió) y lo asigno al agente que le correspondía ese lugar.
Luego, saco ese agente del puntero de distancias al poner su distancia en
cero y lo saco del puntero de probabilidades al poner su probabilidad de
conexión en cero. Lo segundo sirve para que no vuelva a tocar como posible
conexión. Lo primero sirve porque voy a querer recalcular el factor de
Normalización cada vez que saque a un agente del grupo ya que al irse
él se lleva su probabilidad, entonces queda una franja vacía que no
le pertenece a nadie. Eso significa que si el número Random cayera ahí,
no podría asignar el lugar a nadie y el programa llegaría a un freno.

Ahí lo probé, el programa funciona bárbaro. Probé sumar las probabilidades
y dan 1. No probé casos de agentes cuya distancia sea cero. Hagamos eso.
Si esto funciona bien, el programa está perfecto y ya mañana lo puedo implementar tranca.
Ya probé eso y funciona perfecto. También estuve mirando las probabilidades
para ver que efectivamente lo que el programa esté haciendo tenga sentido.
Lo que vi es que justamente luego de remover a un agente de la lista de posibles
sorteados, el nuevo cálculo de probabilidades actualiza las probabilidades de
los agentes básicamente considerando cuál es su probabilidad dado que el rango
cambió. Volviendo al caso de las rifas, si había 100 números y el primer agente
en irse tenía 30 rifas, entonces ahora quedan 70 rifas. Por tanto, el que tenía
10 números, cuando había 100 rifas tenía una probabilidad de 0,1 de ser ganador,
pero ahora que hay 70 rifas su probabilidad es 1/7. Esa es la forma en
que esto va cambiando. Por lo que estoy viendo, este no es un proceso
de probabilidades independientes. Por el contrario, las probabilidades
de que el sujeto sea seleccionado en segunda o tercera vuelta dependen
de quiénes se vayan en primera y segunda vuelta. Consultar si esto
puede ser un problema.

También de paso mandé a correr hasta terminar el armado de imágenes para los
casos de redes de Random Regulars. La verdad que los gráficos de estados final
en el espacio de Parámetros se ven bastante bien en este caso.

.) Mañana tendré que hacer el tema de volver un poco más robusto el reconocimiento
de casos de Consenso y demás. Y la burocracia Infernal.

Veamos si llego hoy a implementar el código que acelera el proceso
de selección de agentes en el código principal.

--------------------------------------------------------------------------------------

27/06/2021

La implementación del código que acelera la selección de agentes funciona bien.
Aunque sigo con el problema de que el sistema no converge a nada para 
los casos de Polarización. Me pasa con Alfa 0,2 y de ahí para arriba.
La situación es que la red es muy esparsa y cambiante como para que
el sistema pueda converger a una solución, por lo que se queda oscilando infinitamente.
Esto es una gran cagada, porque no puedo resolver una única iteración del problema,
ni hablemos de lograr que corra un ensamble. Probé reducir el dt como para que
la variación promedio baje, pero eso no afectó. El resto de los parámetros los
tengo tal cual como baumann, por lo que no entiendo cómo es que a él el sistema
le converge y a mí no.

También probé con aumentar el tiempo que la red de actividad se mantiene armada.
Eso sólo logró reemplazar el problema al tener una red cuasi fija. No creo que sea
el camino. Vamos a mirar los parámetros específicos que usa Baumann, después voy a
ponerme con la Burocracia Infernal y si me queda tiempo ya miraré el archivo

Ví que me faltaba poner el K=3 para que mis parámetros sean exactamente los mismos
que Baumann. Aún así, dudo que eso alcance para que el sistema converja correctamente.

Ya hice la burocracia, ahora pongámonos con el tema de los gráficos de Python.

-------------------------------------------------------------------------------------

28/06/2021

Hablando con Pablo y Sebas, me propusieron que para poder ver estados finales del
sistema en el caso de Redes de Actividad lo que voy a tener que hacer es 
cambiar el criterio de convergencia. Para eso voy a hacer que los sistemas
evolucionen 1000 iteraciones, guardarme esas opiniones que hayan alcanzado
y luego promediar los valores de opiniones obtenidos. Esto significa entonces
que si por ejemplo hago que un punto del ensamble tenga 10 simulaciones, el resultado
va a estar promediado con 10 iteraciones nomás. Ok, razonable.

Eso significa que el código tiene que cambiar para que itere una cantidad fija de veces,
no que haga un while. Luego simplemente me guardo las opiniones finales y la variación
promedio.

---------------------------------------------------------------------------------------

29/06/2021

Ahí corregí la función que define si el sistema llega a Polarización, Consenso o 
estado Ideológico. Ahora la cosa es mucho más robusta. Ya puedo mandar esto a 
correr en las pcs. Lo que voy a hacer es cancelar todo el tema de graficar las
TdO y sólo armar los gráficos de Estados Finales en el espacio de Parámetros.
Luego tendré que ver de hacer lo de los gráficos específicos, pero imagino que eso
será sencillo.

Ahora voy a corregir la función principal así ya mañana lo puedo mandar a correr y
tengo datos para trabajar lo de Redes de Actividad. Ya corregí la función. El tema ahora
es ver cómo pedirle que corra una cantidad de tiempo más razonable,
porque ahora mismo está tardando eternidades.

Una opción es hacer lo que decía Pablo de quizás cambiar el criterio de convergencia,
diciendo algo como que si el sistema empieza a tener una variación promedio
en cierto rango, o si su varianza es baja, entonces podemos cortar el programa.

Charlarlo para entenderlo mejor. Igual ya mañana me conviene mandar a correr
esto así como está. Podría ya mandarlo a correr en Oporto y después
traer los datos a mi pc para mirarlos más rápido. A 1 minuto por iteración,
si hago 20 iteraciones para cada punto del ensamble y separo esto en
5 archivos con dos alfas cada uno, en una hora tendría que estar hecho.
Habría que ver entonces si con 30 ciclos del sistema me alcanza. Yo diría de 
mejor en ese caso hacer 500 ciclos. Si 100 ciclos tardaba 180 segundos,
500 ciclos serán entonces 15 minutos. Eso por 20 me dan 5 horas.
Me gusta la idea. Mañana a la mañana lo mando porque hoy las pc no están.

-----------------------------------------------------------------------------------

02/07/2021

Luego de mucha pelea, logré mandar a correr en Oporto los programas que tenía armados.
Por un lado, está armando las redes de Actividad con 500 ciclos, es decir rearmando
las redes 500 veces. Hasta donde vi, eso tarda unos 500 segundos por iteración. Yo lo
mandé a correr en 7 corridas diferentes, con tres alfas cada corrida, de manera de recorrer
alfa entre 0 y 2 de a 0,1. El Cdelta se mueve entre -0,5 y 1,0 de a 0,1 y con 30 iteraciones
por cada punto del ensamble. Si no calculé mal, en 4 días esto debería estar hecho.

Por otro lado, mandé a hacer los gráficos de estado Final en el Espacio de Parámetros,
e increíblemente el coso esto resolvió eso en dos pedos. Estoy totalmente sorpresa, no
entiendo cómo pudo trabajar con una magnitud de 100 veces más archivos y lo resolvió
en mucho menos tiempo. Veamos los gráficos. Mañana

------------------------------------------------------------------------------------

03/07/2021

Tome los gráficos que hice de Estado Final en el espacio de Parámetros y los pasé
a mi pc. Los gráficos salieron muy mal, no entiendo porqué se veían como consenso
regiones de alfas altos, no aparecían regiones de polarización y sólo de
Estado Ideológico. Por si acaso voy a descargar los gráficos de eso para ver
que tal.

-------------------------------------------------------------------------------------

04/07/2021

Ahora estoy armando los gráficos de TdO para mirarlos por mi mismo y ver que
tal. Mientras, haré un archivo Py que grafique las TdO y el Estado Final en el
espacio de Parámetros para las redes de Actividad. La idea es probar eso primero
en mi pc, y luego lo paso para Oporto. Por otro lado, debería ver si tengo tiempo
para armar un gráfico de Varianzas, Opiniones Promedio y de Entropía.

Bueno, armé un archivo que se encarga de tomar las opiniones finales de las Redes
de Actividad, las promedia y las grafica. Parece funcionar bárbaro, así que eso
pareciar estar bárbaro. El único tema es que el valor de Alfa Crítico me queda muy
por encima del lugar donde se da el cambio entre la región de Consenso y la región de
Polarización. ¿Será porque la cantidad de veces que se arman las redes de Actividad
es 30 veces nomás? ¿Si le doy más iteraciones a las redes, eso hace que todo quede mejor?
¿Si no es eso, entonces qué?

Ahora voy a mirar un poco el tema de la beca de Doctorado.

--------------------------------------------------------------------------------------

08/07/2021

No quiero perderme en lo que estaba haciendo, en especial porque hoy no terminé logrando
nada en particular. Por un lado quise preparar el programa para la corrida en mi pc
en la cual agrego iteraciones a uno de los puntos del ensamble de Redes de Actividad
y a su vez hacer que guarde la evolución de un grupo de 5 agentes. El tema es que
se me ocurrió que para hacer esto iba a ser útil trabajar con un array de punteros,
de manera de que cada puntero tenga la información de cada agente. Cuestión,
todavía estoy probando como se arman los arrays de punteros, así que eso no está listo
para correr.

Por otro lado, estaba intentando mandar a correr el barrido fino en Cos(delta) de manera
de ver cómo trabajan las redes Estáticas en la región de cos(delta) entre -0,2 y 0,2.
El tema es que para eso necesitaba primero volver a mandar el main que tenía en la pc
sobre redes estáticas, cosa que ya hice. También tuve que rearmar el instanciar y decidí
guardarlo como un Instanciar2 para diferenciarlos y no sobreescribir el que ya tengo.
Creo que eso ya casi estaba para correr, sólo faltaba pasar el Instanciar2 a la pc de
Oporto. Hay que mirar correctamente los Cos(delta) que recorre el sistema, pasar el
Instanciar2 a Oporto y luego correrlo en 4 instancias distintas esto. No quiero robarme
TODOS los hilos, pero quiero que sepas que esto va a tardar una buena eternidad.
De paso, antes de mandarlo, vale la pena probarlo, como para estar seguro de que esto
realmente funca.

Después cuando lo grafique tengo que tener en cuenta lo de acortar el tamaño del conjunto
de Alfa Considerado, pero eso es un detalle.

----------------------------------------------------------------------------------------

09/07/2021

Bueno, a ver, esto es un bardo de hacer. Cuestión que puse el Instanciar2.sh en la pc de
Oporto y lo mandé a correr, pero eso no funcionaba porque estaba mal armado el código de Instanciar2.
Y una vez que eso funcionaba, el problema es que el Cdelta y alfa no estaban bien en el programa,
así que terminé teniendo que borrar ciertos datos, y por accidente borré datos que no quería.
En particular, lo datos de Alfa=0.00 y Cdelta 0.20. Así que voy a rearmar esos datos de nuevo.
Paja.

Todo esto tuvo un problema extra, que es que cambié el archivo de Opiniones.e. Eso hizo
que las corridas de Redes de Actividad dieran problemas, porque ahora corrían cualquier cosa.
En cierto punto, lo bueno de eso es que tiró error, porque sino nunca me hubiera enterado.
Cuestión, ahora tengo que re enviar el main de las redes de actividad, que antes reemplace
porque necesitaba el de Redes Estáticas. Una vez reemplazado el main, tengo que hacer makefile
con un nuevo archivo exe, el Opiniones2.e. Y tengo que correctamente mandar a correr las 
siete instancias que me armen mis redes de actividad para alfa hasta 2. Es importante entonces
aclarar que todas las instancias habían resuelto un alfa y Cdelta hasta 0,2. Así
que empezaré las 7 instancias a partir del alfa corrido, y de Cdelta siendo 0,2,
y luego además tendré que mandar a correr el Alfa final y Cdelta entre -0,5 y 0,2.
(Hablo de alfas Final porque acordate que cada instancia recorría tres alfas, entonces
el final es alfa =2 para primer instancia, alfa =5 para segunda instancia y así.)

Bueno, ahí arreglé eso y ya tengo corriendo todo. Me robé 11 hilos de Oporto,
soy un hoarder de las pcs de la facultad. Cuestión, tengo corriendo tanto los programas
de Redes Estáticas como los de Redes de Actividad. Así que ahora a esperar. Stonks.

--------------------------------------------------------------------------------------------

12/07/2021

Hoy estuve revisando cuanto tiempo van a tardar en resolverse las cuentas de Redes de Actividad y
las de Redes Estáticas. Redes estáticas debería estar terminado para el 26/07, mientras
que lo de Redes de Actividad debería estar resuelto para el viernes, suponiendo que el
miércoles me acuerdo de poner a hacer las cuentas que faltan para el alfa final
con los deltas entre -0,5 y 0,2.

Por otra parte, estuve probando cómo hacer lo de los arrays a punteros. Ya lo probé y pasé
el código al Archivo.c. Cuestión que estaba implementando eso en el main, quedé trunco a mitad
de camino, mañana me encargaré de corregir eso poniendo en el bucle central de iteración
los for necesarios de manera que en el array de punteros se anoten las opiniones de los
agentes considerados.

---------------------------------------------------------------------------------------------

13/07/2021

Ya hoy actualicé la función que arma los datos de las redes de Actividad de forma de que
guarde además 5 filas extras con los datos de 5 agentes a lo largo de toda la evolución del
sistema. Con eso me armé 40 iteraciones más de datos para el punto del ensamble con alfa = 0,3
y Cdelta = 0

Mañana tendría que ver de hacer los gráficos con estos datos.

-----------------------------------------------------------------------------------------------

14/07/2021

A ver, veamos qué es lo que tenemos que hacer hoy. Se me ocurre aprovechar y mandar a
armar más datos para otras regiones, como por ejemplo Alfa = 0,2 y todos los Cdelta.
Como para aprovechar a la pc que labure en algo.

Lo que estoy pensando en hacer hoy es:
1) Plantear el tema de la implementación del modelo de actividad empezando en 
un sistema de dos agentes y 1 tópico para ir probando el uso de la sigmoidea.
2) Armar los gráficos de las trayectorias de los agentes de Redes de Actividad
de manera de observar qué está pasando con ellos. Como para ver porqué el sistema
está correctamente convergiendo, siendo que en parte uno esperaría que esto
caiga todo al cero.
3) Armar los gráficos de Varianza, Valor promedio y Entropía para las Redes Estáticas
de forma de poder observar cómo se comportan estos sistemas.
4) Mirar el tema de las bases y condiciones de la presentacióna beca doctoral.
5) Leer Papers
6) En algún momento, subir todo a Github

-----------------------------------------------------------------------------------------------

15/07/2021

Ok, me faltaba poner a correr los datos para las redes de Actividad con el Alfa final
de cada iteración y con Cos(delta) entre -0,5 y 0,1. El problema es que al hacer eso
por accidente mandé a correr el código de Redes de actividad, por lo cual los datos
de Alfa = 0 y Cos(delta) = 0,12 de Barabasi pueden no estar bien. Conviene que los mande
a correr de nuevo a esos.

Bueno, luego de mucha pelea y errores, ya dejé todo corriendo correctamente. Para el sábado
debería estar todo lo de Redes de Actividad resuelto, por lo que estaría bueno que para
entonces tenga bien visto el código que arma los gráficos con eso para evitar problemas
o dudas.

Tengo que pronto armar un MetaInstanciar que se encargue de mandar a correr los Instanciar
de manera más rápida, porque esto así no estaría funcionando, es un bardo impresionante,
además de que hay muchas posibilidades de error.

Lo último que voy a hacer hoy, si me alcanza el tiempo, es ponerme a ver lo de la Tesis.

------------------------------------------------------------------------------------------------

17/07/2021

Bajé unos cuantos archivos para leer sobre el tema de postularme a la beca de 
doctorado. Por lo que vi hay varias modalidades, Pablo dijo que miremos el tema
de la beca de temas generales y quizás el de temas Específicos.

Ahí estuve mirando el archivo de TdO_Actividad.py. En ese archivo ahí estoy viendo
que tengo que agregar el armado de las TdO de los agentes testigos. Esas TdO no
necesitan ser promediadas, sino que simplemente tengo que tomar el conjunto de opiniones
y graficarlos. Eso lo tengo que hacer sobre todas las iteraciones extras armadas.
Para no tener que hacerme la cabeza, simplemente le metí un if al programa para
que el programa sólo se entere de cuáles son los archivos que tienen datos Testigos.

La idea del código entonces es que dentro del if me vaya guardando los datos de las
opiniones de los testigos. Luego tendré que armarme un for extra para graficar
todas las TdO correspondientes, una por cada testigo, y además eso para cada caso
de alfa y Cdelta correspondiente. La idea es ver si las líneas tienden a un sólo lado
o qué carajos pasa.

Luego de eso, voy a tener que pasarme el archivo que grafica a Oporto y ahí
armar los gráficos de Entropía, el de valor final Promedio y Varianza para las
Redes Estáticas.

Terminado eso, veré si me pongo a leer cosas de la introducción a becas o si me 
pongo a hacer lo de mirar la implementación del campo externo

-------------------------------------------------------------------------------------------------

18/07/2021

Armé el código para graficar las TdO de los agentes Testigos. Vamos a ver
cuáles son las trayectorias que hacen los agentes testigos y con eso
vamos a ver qué está pasando en el sistema

--------------------------------------------------------------------------------------------------

19/07/2021

Luego de bastante pelear con esto, ahora sí creo que tengo resuelta la función
que grafica los gráficos de Opiniones, las TdO y los estados finales en
el espacio de parámetros. Lo siguiente que debería ahcer es pasar esto a oporto
y mandarlo a correr con los datos de las Redes de actividad que hice en Oporto.

Y ya que estoy, si esto funca, hoy debería subir todo a Github. Bueno, ya subí todo
a Github. Mañana empezaré entonces con el tema de la beca y ya el miércoles o jueves
seguiré con el tema de los gráficos de Entropía y Varianza.

---------------------------------------------------------------------------------------------------

20/07/2021

Armé los gráficos de Opiniones y el de estados finales en el espacio de Parámetros para
las redes de Actividades con 500 ciclos. Al final, da más o menos igual que lo que hice
en mi pc. Incluso, pareciera que los sistemas van cayendo a cero, porque las máximas
opiniones van promediando a 3. Entonces me da que esto si va cayendo con el tiempo.
Quizás con más iteraciones todo caería a cero. La abismal diferencia entre el 
Alfa teórico y el experimental me hace pensar que hay un error fundamental en la
forma de correr los programas de actividad. Tiene que haber otra forma de hacer esto.
Además de un posible error sustancial en la forma en que se calcula el alfa teórico,
aunque no creo que ese realmente sea un problema.

Dicho esto, tengo para hacer entonces lo del campo externo, leer papers y lo de la presentación
a beca. Diría de mirar un poco lo de campo externo y lo de la beca, ya mañana leeré papers
y veré si tengo cómo encarar estos bardos que no funcionan.

Bardos que no funcionan: Ver otras caracterizaciones de Redes Estáticas.
¿Cómo carajos se supone que trabaje las redes de Actividad?

Hablando con Sebas él me propone no mirar el promedio de los ensambles, sino mirar
el estado final de cada elemento del ensamble, y juzgar el estado final en base
al que aparezca más veces.

También me dijo de analizar el valor de los tópicos en función del tiempo, para ver
si llegan a algún estado más o menos estacionario.
Ver gráficos de Estado final para los puntos del ensamble en Redes de Actividad.
Mirar en Alfa=0.0, 0.2, 0.6.

Armar sobre los gráficos de Estados Finales un mapa de porcentajes. Y sobre la
corrida de agentes con rearmado de la red de 500 veces tomar un estado y guardar
los datos de la trayectoria total de los agentes para ver si llegan a un estado
estacionario o no, de manera de estar seguro de estar etiquetando estados finales.

--------------------------------------------------------------------------------------------

22/07/2021

Ahí revisé los datos que están corriendo en Oporto y parece que van a tardar unos
dos días más hasta terminar. Así que ahora me voy a poner a terminar lo de
la entropía, el promedio de las opiniones y la Varianza de los sistemas.

-------------------------------------------------------------------------------------------

23/07/2021

Armé el código que hace los gráficos de Entropía, de Varianza y el de Promedios.
Por lo pronto parece que funcionan bien, aunque los gráficos no se ven tan lindos
como esperaba. Quizás sea la falta de estadística o la falta de un barrido fino.

Lo que voy a hacer ahora es modificar el código que clasifica a los puntos en cuadrantes
ya que quiero diferenciar los puntos que caen al cero. También voy a modificar el archivo para
que me muestre los graficos de Variación Promedio y el de clasificación de opiniones 
en cuadrantes. Creo que ahí está hecho, ahora lo mando a correr y a ver que tal sale.

Lo siguiente es trabajar el código de Redes de Actividad para que NO promedie las opiniones
finales, sino que en cada iteración defina el estado final y con los estados finales
de todos los elementos de un punto del ensamble, promedio y calculo el estado
final del sistema. Una idea copada, habrá que ver cómo se hace, es en cada punto del ensamble
colocar un porcentaje de cuál es la probabilidad de que ese estado marcado sea el estado final.
Lo ideal sería ver que en las regiones bien definidas la probabilidad va al 100% mientras que
en las zonas de transición la probabilidad debería decaer.

Mirando el código de Redes de Actividad estoy notando que la cosa no funciona como pensaba.
El estado final se determina en función de TODOS los valores de opiniones, no
en función de los promedios. Aún así sigo teniendo que diferenciar el estado final
para cada elemento del ensamble, pero estoy empezando a pensar que esto no será la solución.
Entonces mañana tengo que hacer dos cosas con esto:
1) En la iteración de los nombres usar la función que clasifica el estado final para armar
un vector que contenga los estados finales de todos los elementos de ese punto del ensamble.
Luego, cuando quiera poner en el elemento de la matriz el estado final de ese punto,
ahí si promedio todos los estados y obtengo el verdadero estado final.
2) Modificar el armado de los gráficos de opiniones de manera que grafique todos los puntos
de opiniones, no que grafique las opiniones promediadas

---------------------------------------------------------------------------------------------------

24/07/2021

Resolví los 2 puntos que marqué ayer. Hice los gráficos de Opiniones y el de Estados Finales
con los nuevos cambios. 

Analizando el gráfico de Estados Finales en el Espacio de Parámetros:
En primera medida, considerando que por encima de la región de consenso
debería haber Polarización en los cos(delta) entre 0 y 0,1, hay mucha más polarización de la
que debería en el alfa=0,1. Luego de eso, pareciera que el programa ubica correctamente
los estados finales.

Esto suena muy bien, pero en realidad, según lo que marca el cálculo del Alfa
teórico, estoy MUY por debajo del valor de alfa y debería estar cayendo
al consenso, no logro entender porqué pasa esto. A esta altura, realmente
queda pensar que mi cálculo del alfa crítico está mal, debería revisar eso.

--------------------------------------------------------------------------------------------------

25/07/2021

Vamos a intentar hacer lo que me dijo Seba de graficar los tópicos en función del tiempo.
Mañana debería intentar plantear lo del campo externo empezando a probar lo de la sigmoidea.
Supongo que en la semana iré leyendo algunos papers. Aunque quizás esta semana no esté tan
bien de tiempo como para hacerlo.

--------------------------------------------------------------------------------------------------

27/07/2021

Cambié unas cosas en el archivo de Graficos Var RE.py de manera de que la varianza se calcule
usando el método de numpy para ello. Ahora sólo queda pasar esto a la pc de Oporto y mandar
a correr esto para tener los gráficos de esas redes. Y debería ver de implementar lo que me 
dijo Seba sobre los cuartiles para reducir el tema del efecto de los Outliers.

Por otro lado, en el archivo de TdO_Actividad.py puse la parte del código que grafica los
tópicos en función del tiempo. Eso igual lo voy a tener que comentar al mandarlo a oporto.
Aunque en Oporto ya está corriendo esto para los gráficos de Actividad con redes de 500
ciclos de armado de la red de actividad. Ya tenía gráficos de eso, pero ahora acordate
que lo que ocurre es que estás graficando las imágenes de gráficos de Opiniones y además
ahora el estado final se define como el que aparece más veces en las iteraciones de un
dado punto del ensamble.

Habría que subir esto a Github y además hacer que en las redes de Actividad se grafique lo de
los valores promedios, la ¿Varianza? y la Entropía

----------------------------------------------------------------------------------------------------

30/07/2021

Estuve peleando pero no logré conseguir que mi idea sobre un programa que me mande a correr de
de una todos los programas funcione. Está teniendo un problema de considerar como si fueran 
strings los números que le paso y entonces no recorre los alfas en los valores que quiero.
De paso descubrí que tengo que pedirle el nombre del script aparte. Eso es un poco paja porque
no me lo está guardando en donde me gustaría pero por ahora todo bien. Esta forma
de mandar a correr los archivos tiene sus claras ventajas, si logro dominarlo
va a ser genial. Sólo me falta ese detalle de cómo configurar los intervalos.

También tengo todavía que hacer lo que decía Seba de revisar los datos de la Mediana de los
tiempos de Convergencia en vez de mirar la Varianza, eso debería ser una medida más robusta.
Usar el scipy.stats.iqr. 

Por último tengo que subir el Aval, el CV y los datos del Poster a la presentación de Beca.

---------------------------------------------------------------------------------------------------

01/08/2021

Estuve haciendo bastante mierda hoy. Por un lado en Oporto mandé a correr los programas para graficar
los gráficos de Estados finales, Varianza, Promedios y Entropía. Lo hice como cinco veces, creo
que esta vez al fin está bien. Mañana cualquier cosa me entero si hubo problemas. Sobre esto,
debería encargarme de modificar los gráficos de Varianza por gráficos de Medianas de tiempos
de simulación (TideSi).

También estuve haciendo lo de las redes de Actividad. Por un lado rearmé los gráficos de tópicos
de manera de que cada gráfico corresponda a una simulación del sistema y que me muestre la evolución
de los 2 tópicos de los 5 agentes testigos. También hice gráficos de Opiniones finales para cada
iteración y coloqué la etiqueta a cada uno. Con esto queríamos ver si el etiquetado es correcto
o no. Sobre el etiquetado cambié también la forma de definir consenso. El consenso se da si
más del 90% de los agentes están en valores por debajo de 0,01.
Lo malo de esta forma de graficar los datos es que básicamente voy a armar 4100 gráficos. Eso
no va a ser bueno. Voy a tener que ver cómo filtrarlos, sintetizarlos o elegirlos correctamente.
Espero que el graficado de los estados finales en el espacio de parámetros salga bien.
Mañana debería mandar a correr todo o completar los que faltan. 

También podría subir esto a Github. Y mañana debería arrancar con la presentación de esto.
Y si puedo leer un paper, mejor.

---------------------------------------------------------------------------------------------------

02/08/2021

Las cosas nunca van como uno quiere. En primer lugar, los gráficos sobre Redes Estáticas no se ven
tan lindos como me gustaría. Queda revisar si los algoritmos para realizarlos están bien, tengo
algunas sospechas. Además la Varianza difícilmente esté dando buenos resultados, hay que probar lo que
dijo Sebas. Así que este tema NO está cerrado. Y de paso, por algún motivo desconocido, los gráficos
de Entropía, varianza y cosas de la red Random Regulars no se armaron, no entiendo porqué.

El algoritmo de la Entropía es muuuuy raro. Revisalo.

Por otro lado, los gráficos hechos de Redes de Actividad parecen estar bastante bien, pero el gráfico
de Estados finales no se armó, ese parece tener problemas. Entiendo que el problema surge porque
tengo una sola columna para graficar. Si no es ese el problema, entonces no sé que es.

Ahora lo que voy a hacer es empezar a preparar la presentación de mañana y ver qué se me ocurre
para solucionar eso entre hoy y mañana. Es importante tener a mano todo lo que me pidió Pablo.

--------------------------------------------------------------------------------------------------

04/08/2021

Implementé el uso de la función Gaussiana que me devuelve un número a partir de una distribución
Gaussiana para la inicialización de las redes de Actividad. Luego mandé a correr esto en la computadora
de Oporto para barrer fino en alfa para un cos(delta) = 0. El alfa lo barrí entre 0 y 0,6
con un paso de 1/60.

---------------------------------------------------------------------------------------------------

06/08/2021

Ya subí a Github todas las nuevas implementaciones. Ahora lo que voy a hacer
es descargarme de Oporto los datos de las simulaciones realizadas y usar eso para 
ver qué pasó con Redes de Actividad. Me da bronca que por segunda vez no guardé correctamente
el valor de Alfa. Como guardé sólo 2 números, tuvo que aproximar algunos resultados y da medio raro.

Acabo de descubrir que al final me había equivocado al mandar a correr el programa porque no había
hecho un make clean y make all. Así que por eso los alfas estaban todos mal, y no tenía los datos
de inicio de la matriz. Por eso, de la bronca terminé rearmando el código de Metainstanciación
y ahora funciona. En vez de pasarle el input a las llaves del for, decidí pasar el valor del input
a la variable y luego meter eso en un while. Luego uso la variable como un contador y todo
funca mejor. También de paso armé un código llamado asesinar que levanta desde los archivos
de salida los ID de los script, de forma de borrar todos esos códigos de un plumazo. Revisarlo
que la idea de eso es que levanta el id de los archivos salida*, así que atento con cuales salidas
levantás.

Metainstanciación es el que manda a correr los Instanciar y prepara las salidas. Requiere un input
de un número, que es básicamente el número de hilos-1 que planeo mandar. Tomar en cuenta formas
de reutilizar esto en el futuro cuando quiera barrer en otros parámetros.

---------------------------------------------------------------------------------------------------------

13/08/2021

Entre el 06 y hoy claramente hice cosas, pero al parecer no las anoté. De lo que hice, claramente
habré bajado los datos que mandé a correr con Metainstanciación y grafiqué lo que veía para tener
una idea de lo que estaba pasando. Si no me equivoco, los datos que mandé a graficar son los
de Reversión Actividad, que tienen las opiniones inicializadas con una distribución gaussiana
de media 0 y sigma cuadrado = 2,5. 

Por lo que estuvimos charlando con Seba y Pablo el martes, vamos a tener que hacer bastantes
nuevas corridas para ir revisando datos y ver que todo cierra correctamente. Por esto es que
entre ayer y hoy estuve ordenando los archivos que tengo en carpetas dentro de una carpeta llamada
DilAct (Dilema Actividades). En esta carpeta están las otras carpetas con datos y además un archivo
que guarda registro de qué cosas van en cada carpeta. Esto me pareció necesario para no marearme
sobre qué cosa va donde.

Ahora, lo siguiente que tengo que hacer es mandar a correr en la pc de Oporto ciertos códigos.
Tengo varias cosas para hacer. En primer lugar tengo que mandar a correr un programa que guarde
los valores de actividades, calcule valores medios y eso me lo compare con el valor medio de
actividad que nosotros calculamos a mano. Eso lo puedo tranquilamente hacer en mi pc. Aunque
considerando las otras cosas que tengo que mandar a correr, creo que casi que puedo simplemente
guardarme los datos de actividad sorteados en las simulaciones realizadas y luego con eso calcular
el valor de actividad media. No veo que vaya a ganar mucho por hacerlo por mi cuenta.

Necesito armar histogramas de actividades sorteadas. Para eso tengo que guardarme las actividades
sorteadas, así que eso es una cosa para agregar al main.
Luego tengo que llevar registro de quiénes son los sujetos que se activan. Para eso necesito
armar algunas corridas largas (500 aprox) en las cuales distribuya las actividades, guarde
registro de eso y guarde también registro de cuáles son los agentes que se activan. Para evitar
problemas, yo haría un programa aparte con eso. Esto lo puedo hacer en mi pc, esto si vale
la pena hacerlo en la pc, porque la idea de esto es ver simplemente que los sujetos con cierta
actividad se activan según lo que marca su actividad, no necesito correr la red de fondo. Así que
puedo simplemente distribuir actividades y ver de que se activen correctamente.

También tengo que hacer una corrida con Beta=0. Eso lo haría usando la configuración de la primer
corrida descripta. Es decir, aprovecharía que me guardo los valores de actividad sorteadas, simplemente
cambiaría el Beta y listo, lo vuelvo a mandar a correr.

Por último tengo que también guardarme el valor de grado medio. Eso lo puedo calcular simplemente sumando
todos los números en la matríz de Adyacencia y luego dividiendo por 1000.

Ok, eso es todo lo que tengo que hacer. Se me ocurre una forma razonable de no mezclar información, que es
simplemente agregar un nuevo archivo en el cual ir guardando los datos que sea:
"Datos_Actividades_alfa=$_Cdelta=$_N=$_Iter=$". De esta manera puedo guardar paso a paso los valores
de actividades sin entrar en conflicto con la Variación Promedio. Y después, para no tener conflicto
con el grado medio, a ese sí le armo un vector aparte y listo. Bueno, ahora sí tengo organizado
el plan de trabajo. 

Ahora tengo que rearmar el main y pasarlo a Oporto. Luego subir cosas a Github, armar nuevas carpetas
en Oporto y luego ver de ir pasando las cosas necesarias a Algarve o Setubal para correr más cosas en
paralelo.

--------------------------------------------------------------------------------------------------------------

14/08/2021

Implementé el registro de las actividades de mis agentes, el registro de quienes se activan
y del grado medio de las redes de Adyacencia armadas. Con todo eso debería estar el programa 
en condiciones. Lo acabo de mandar a correr y parece funcionar.

Ahora, si esto funca bien, la pregunta es cómo mandar a correr esto. En principio se me ocurre
mandar a correr los datos en Oporto y en Setubal. Aunque para eso tendría que primero pasar
todo a Oporto, asegurarme que está todo en condiciones, luego copiar la carpeta de Oporto
a mi pc y de ahí pasarlo a la otra pc. Creo que es viable.

De paso, no estoy guardando los datos de Opiniones iniciales. Al parecer eso es algo
que tengo en la carpeta de Oporto, así que mejor lo agrego ahora y listo.

Hice el traspaso de los archivos que estaban en Oporto a la computadora de Algarve.
Ahora puedo correr cosas en Algarve, así que considerá que en general tenés 16
hilos disponibles, y no solo 8 o 10. Siempre considerando que no les querés robar
los 40 hilos a todos. 

IMPORTANTE: En Oporto están corriendo los datos con beta = 3, en Algarve están corriendo
los que tienen beta = 0. Igualmente se están guardando en carpetas diferenciadas, pero por
si acaso lo registro. Si mis cuentas son correctas entonces, esto va a estar hecho mañana.

--------------------------------------------------------------------------------------------------------------

15/08/2021

Bajé los datos de Oporto y Algarve y los pasé a mi PC. Empecé a armar el archivo de python
con el cual analizar los datos. Ya me encargué de que guarde los nombres de los archivos
de opiniones y actividades en el SuperDiccionario y me encargo de sólo tomar los de
Actividades al levantar datos.

También me encargué de armar los Arrays de Datos tales que guarde los valores
de actividades, los registros de activación y demás. Aunque creo que debería
encargarme de que los registros de activación se guarden todos como una única
matriz, eso haría más fácil el revisar los datos de ella. Ya corregí eso.
Queda mañana ir tomando los promedios, graficando cosas y ver cuál me falta
hacer a mi aparte. Y ver si de una vez puedo ponerme a hacer lo de los cuartiles
que charlamos con Sebas.

-----------------------------------------------------------------------------------------------------------

16/08/2021

Estoy armando el archivo de Python que debería hacer los gráficos para estudiar
las actividades, grados medios y demás. Acabo de terminar el código que levanta
las actividades y con eso me armo un gráfico que tiene la distribución de actividades,
la Función distribución de actividades por encima y los valores medios calculados
y empíricos graficados.

Hice el gráfico de probabilidad de activación en función de la actividad. La verdad
que dió bastante mal, porque esperaba una función identidad y estoy observando cualquier
cosa. Me parece muy raro esto. Charlarlo.

Tengo hecho el código que grafica la distribución de los grados medios de la red.
Este histograma está dando que el grado medio de la red de actividad se distribuye
como una gaussiana centrada en el valor teórico.

-------------------------------------------------------------------------------------------------------------

17/08/2021

Esto lo estoy escribiendo un día tarde. Ayer logré armar el gráfico que coloca
la fracción de agentes por debajo del umbral de consenso en vez de la fracción
de simulaciones que terminan en consenso.

Luego tuvimos una charla con Sebas y Pablo y hablamos sobre que quizás el cálculo
de las distancias o la probabilidad de conexión está mal. Revisar las pruebas que
hice y mostrárselas a Pablo y Sebas.

La otra cosa que quizás es el error puede ser el criterio de etiquetado. O el
criterio de finalización de corrida. Habría que investigar eso.

También anotamos algunas cosas para hacer para comprobar en los datos. 

--------------------------------------------------------------------------------------------------------

18/08/2021

Ya pasaron varios días así que no me acuerdo exactamente lo que iba a anotar acá.
Creo que por un lado iba a mencionar que mandé a correr nuevamente en Oporto y
Algarve las corridas de Regact y HomofiliaCero respectivamente. Lo hice porque
las corridas anteriores tenían el problema de que el guardado del registro de
agentes activados estaba mal hecho porque guardaba los agentes en 2 casilleros
más atrás de lo que debía, corriendo totalmente los registros de activación
por un lado, por no mencionar que eso a veces hacía que se sobreescribieran los
casilleros que guardaban el tamaño del vector como info. Ahora corregido
eso lo mandé a correr esperando que ahora sí el gráfico de probabilidad de activación
en función de la actividad estuviera funcionando correctamente.

Por otro lado, también armé una corrida llamada TiempoExtra, la cual tiene como objetivo
barrer alfa entre 0 y 1, no tan fino como lo que barrí antes. Tomé 21 valores de alfa en
ese intervalo, por lo que alfa varió de a 0,05. En esta corrida también extendí el tiempo
de simulación al aumentar la cantidad de veces que debo rearmar la red a 100. Por otro lado
reduje la cantidad de simulaciones por punto de ensamble a 50. La idea con esto es ver
si el gráfico de Consenso vs Alfa muestra más variación en los sujetos que quedan debajo
del umbral de consenso y si al aumentar el alfa hasta 1 esto empieza a caer fuerte.

------------------------------------------------------------------------------------------------------

21/08/2021

Ya tengo tres tiras de datos armados, debería pasarlos a mi pc y empezar a mirarlos. Vamos
a bajarlos y de ahí empezar a hacer los estudios.

Descargué los datos y los llevé a sus carpetas correspondientes. Ahora estoy armando los
gráficos de distribución de actividades, distribución de grados medios y probabilidad
de activación de agentes en función de su actividad. Por un lado tuve que corregir
el armado de la distribución de grado medio porque de la forma en que estaba hecho antes
esto pasaba que como buscaba los grados medios en una fila específica, entonces tenía
error al armar el gráfico debido a que TiempoExtra tenía más filas que Regact u 
HomofiliaCero.
Por el otro ahora que corregí el guardado de datos del registro de Actividades, el gráfico
ese da bárbaro. Es una alegría más eso.

Ahora queda entonces ir armando los otros gráficos que me pidieron Seba y Pablo. Voy a mandar
a armar el de Consenso vs Alfa ahora mismo, y ya el lunes haré los de distribución de
opiniones según tópico y varianza y correlación de opiniones. Seguro Numpy tiene formas
de calcular la correlación de forma fácil haciendo que eso sea una boludez.

Estaba pensando, se podría intentar hacer que todo el gráfico de fracción de agentes
en el consenso se arme en un sólo gráfico, colocando los datos de todos los sigmas en
el mismo lugar.

--------------------------------------------------------------------------------------------------

23/08/2021

Ya hice lo de que el gráfico de las fracciones de agentes en el consenso se coloque todo
en un solo gráfico. Va a ser mejor, y más colorido, para comparar mejor los datos.

También me armé los gráficos de distribución de opinión para cada tópico en función del
Alfa. Me parece que lo mejor sería cambiarlos para que la calificación del estado final sea
el título y no un cartelito que tapa el gráfico. Ahora mirando los resultados estamos creo
ante una muestra de qué está pasando. Son todas curvas centradas en cero, no veo cómo
esto pasa del consenso a la polarización.

Por último me armé los gráficos de Varianza en función del alfa. Por lo que se puede ver,
la Varianza está creciendo con el Alfa.

--------------------------------------------------------------------------------------------------

27/08/2021

Creo que me faltó anotar uno de los días de trabajo. Cuestión, ya me armé una parte del
código que grafica la varianza con sus errores para todas las redes.

Luego agregué una parte que grafica las distribuciones de las opiniones de todas las 
simulaciones de un ensamble y junta las curvas de un conjunto de alfas. Esa parte la coloqué
al final del código para que no me mantenga el archivo abierto eternamente.

Hecho esto, tengo que ver de pasar esto al código de redes fijas. Eso no suena muy difícil,
tengo que copiar lo que tengo al final, tener cuidado con el tema de que el SuperDiccionario se
arma usando GM en el caso de redes fijas y después darle para adelante. Ah, y mirar cómo están
hechos los datos para que se armen correctamente los Arrays de Opiniones.

---------------------------------------------------------------------------------------------------

28/08/2021

Pasé el código que arma las distribuciones de Opiniones al archivo de Redes estáticas, y con eso
me armé las distribuciones de Opiniones para Redes Estáticas. El código funca bien, aunque Pablo
me dijo que tendría que estudiar los alfas en el entorno del Alfa crítico, así que voy a tener
que volver a cargar el main de cálculo de Redes Estáticas, armarme nuevas redes estáticas y mandar
a correr toda la cosa. Mañana haré eso.

--------------------------------------------------------------------------------------------------

29/08/2021

Mandé a correr lo que me dijo Pablo sobre datos de redes Estáticas de Erdos-Renyi entorno al
alfa crítico. En particular fue para una red de GM=8, con dt de 0,001. Los datos los guardé
en una carpeta llamada ERZoom la cual recorre alfas entre 1 y 1,5 de a 0.005. El alfa crítico
es 0.125.

Cuando estén terminados los datos, me los paso a mi pc y desde ahí voy a armar las distribuciones.
Ahora voy a modificar el archivo de Graficos TDO RE.py para que levante esos datos y me arme
los gráficos correspondientes. Luego con el tiempo extra de hoy voy a ponerme a corregir tarea
de los chicos y ya mañana y el martes me dedicaré a lo del tema de releer los papers de Baumann
a ver si encuentro el motivo de que no esté funcando la cosa. Cierto que Pablo me dijo
de intentar armar datos de redes de Actividad donde la red se reconstruya en cada paso temporal,
para ver si ese cambio brusco en el armado de la red afecta al comportamiento.

Ya armé los gráficos de Distribución de Opiniones para la red de Erdos-Renyi Zoomeada. Aunque
el resultado no me convence, siento que necesito mirar algunos alfas más, así que mandaré a armar
esos datos extra. Los datos con Alfa hasta 0,15 no terminan de formar la bimodal. Voy a sacar datos
hasta 0,175.

Cabe mencionar que cambié el archivo de Python porque los datos que estoy armando en ERZoom
tienen agregada la fila de datos de Estado inicial de la Matriz de Opiniones, lo cual corrió
los valores de opiniones finales de la fila 3 a la 5. Atento a eso.

Hacer el gráfico de las distribuciones mañana, hoy no llego a eso.

Mañana lo que voy a hacer es a la mañana voy a corregir tareas de los pibes y a la tarde
ponerme con esto y con lo de tomar la mediana a los datos, así como corregir lo de la
Entropía que no daba del todo correcto. También tengo que empezar a buscarme momentos para hacer
cosas de la partida de rol. Eso me resulta importante.

--------------------------------------------------------------------------------------------------

30/08/2021

Me descargué los datos de Redes Estáticas así como el de la red de Actividad con red continuamente
cambiante. Los datos de redes estáticas los estoy llamando ERZoom, porque es una red de Erdos-Renyi en
la que estoy haciendo Zoom en una cierta región de Alfa para Cos(delta)=0 de manera de observar cómo
se da la transición de la distribución de opiniones unimodal a bimodal. Para esto uso alfa entre
0,1 y 0,2 barriendo de a 0,005, 100 simulaciones para cada alfa, dt=0,01 y red de Erdos-Renyi.
En cambio los datos de Redes de Actividades los llamé RCC por Redes Continuamente Cambiantes,
tienen dt=0,01, rearma la red 5000 veces, no guarda datos sobre las actividades, Alfa se mueve
entre 0 y 0,5 y guarda datos de los testigos.

Grafiqué los resultados de los datos de RCC, y lo que se observa es que al final no resultaron
como esperábamos. El rearmar las redes en cada interacción no sirvió para que el sistema
escapara el consenso, sino que por el contrario el constante cambio es tal que no permite
que el sistema logre despegar.

Ahora Pablo me dijo de armar esto de los gráficos de Proyección de las opiniones en una serie
de subplots. Para eso entonces voy a armar esto en un conjunto de subplots de 3x3.

-----------------------------------------------------------------------------------------------------

18/09/2021

Vamos a trabajar un poco los gráficos de Redes Estáticas para ir agregando eso a la Tesis.
Los datos de Redes estáticas fueron armados con una distribución aleatoria de los datos entre
-3 y 3. Además el valor de K usado es 1, por lo que al polarizar los agentes como mínimo van 
a tender a 1 ya que al polarizar tienden a Nro vecinos*K, y como la red es conexa tenen por
lo menos un vecino.

Entonces se me ocurre modificar el criterio para pedir que se considere al agente polarizado
si su opinión es mínimo 1. Le puse 0,95 por si tiende a 1 por abajo. Luego como están distribuidos
de manera homogénea entre -3 y 3 para ambas opiniones, eso significa que en un espacio de 6x6 tengo
1000 opiniones. Luego si la distribución es homogénea, en un área de 36 tengo mis 1000 opiniones.
Luego yo voy a asumir que si mis opiniones se encuentran en el cuadrado entre -1 y 1 no están
polarizadas. El cuadrado entre -1 y 1 tiene un área de 2x2, por lo que mide 4. Finalmente, si tengo
1000 opiniones en un área de 36, por regla de tres simple sé que en un área de 4 tengo una novena parte.
Es decir, 111 opiniones. Bueno, igual ahora que lo pienso lo importante no es la cantidad sino la
proporción. Es decir, 1/9 es la proporción que tengo ahí. Finalmente todo este análisis es para
hacer un criterio similar al que hicimos en Redes de Actividad. En Redes de Actividad decimos
que el sistema está en Consenso si la cantidad de agentes dentro del círculo de 2 Sigmas es
igual o mayor a la cantidad inicial. Por tanto, en este caso vamos a decir que si la proporción
de agentes en el cuadrado de [-1,1]x[-1,1] es mayor a 1/9 entonces el sistema está en Consenso.

Modificar el programa GraficosTDORE.py porque en los gráficos de Opiniones está graficando
cada simulación de un punto del Ensamble por separado, y en Redes Estáticas si quiero recuperar
el comportamiento de Polarización o Estado ideológico tengo que sumar TODAS las simulaciones
en un sólo gráfico.

---------------------------------------------------------------------------------------------------------

19/09/2021

Luego de varias iteraciones, logré que el programa corra y me arme los gráficos de opiniones y 
el de soluciones en el espacio de Parámetros. Queda discutir un poco con Pablo y Sebas el criterio,
pero creo que ahora el criterio resulta un poco más claro. Ahora mismo la línea del valor teórico
me queda claramente por abajo de la línea del valor experimental, pero las líneas son bastante parecidas,
por lo que pareciera ser un tema de que el criterio quizás es muy laxo.

Por otro lado, luego de un fuerte pero breve momento de crisis, creo que logré armar una subsección
de la tesis, la que corresponde a Redes de Erdos-Renyi. Ahora me queda entonces usar eso como base
para ir armando la de redes Scale-Free y Random Regulars. Luego sobre Redes de Actividad me conviene
basarme mucho en lo que dice Baumann en su paper, y lo que diga el paper original y así. Es decir,
buscá fuerte lo que los creadores de esto escribieron, no quieras hacerte el bardo de armar
por tu cuenta una descripción de esta red.

Fue una boludez, pero es un paso importante, ya tengo una parte. Ahora tengo que seguir.
Dios, que mes me espera.

De paso, también tengo que ponerme a trabajar en el armado de los gráficos de Entropía y el
de usar los cuartiles y la mediana. Hace tanto de eso, ya ni me acuerdo dónde es que Sebas
me pasó lo que charlamos. Muy mala mía

--------------------------------------------------------------------------------------------------------

20/09/2021

Viendo el largo tiempo que el programa tarda en recorrer y clasificar todos los archivos para
formar el SuperDiccionario, quizás sea conveniente hacerle caso a Pablo y dividir los archivos
según GM. Eso reduciría el trabajo. El super diccionario actualmente toma unos 24 minutos en armarse.
Es mucho para mis trabajos en pc, pero nada una vez que lo mande a correr en la pc de Oporto,
no vale la pena pelearse con esto.

Ya hablé con Sebas y corregí entonces el gráfico de la Varianza en función del tiempo para que
grafique los valores de Mediana de Tiempo de Simulación. Así que habría que mandarlo a correr
y ver que tal. Queda ver el tema del graficado de la entropía y para eso hay que mirar un poco
el cómo se define que un punto esté en el consenso. Se me ocurre finalmente usar esa definición
dada antes sobre que los sujetos polarizados son los que finalmente llegan a estar en un valor
1. También podríamos discutir que como la región de transición intermedia los sujetos NO van
a llegar a 1 como mínimo sino a un valor intermedio, quizás podemos hablar de que los sujetos
polarizan un poco antes. Como en 0,5 por ejemplo. Eso cambiaría un poco las cosas. Y definitivamente
bajaría la curva de alfa crítico experimental.

--------------------------------------------------------------------------------------------------------

21/09/2021

Ahí modifiqué el archivo para armar los gráficos de Promedios, el de Entropía y el de Cuantiles, anteriormente
el de Varianza. A los Promedios no los toqué, total ese tenía pinta de estar bien. El de Cuantiles en cambio
lo modifiqué de manera de que no tomo la varianza de los TideSi, sino que ahora excluyo los valores que se
encuentran en el 5% inferior y 95% superior de los datos, de manera de quedarme con el 90% de los datos que
están en el interior. A esos datos les calculo la longitud del intervalo y eso es lo que le tomo logaritmo
y luego grafico. La idea es que va a ser un valor mucho más robusto a outliers esto. Veremos que tal.

Por otro lado, la parte de la Entropía la modifiqué porque estaba arbitrariamente descartando los casos
en que los agentes caen al consenso, es decir la categoría cero del cálculo. Me parece que hacer eso
justamente es el problema de que la región de Polarización Descorrelacionada y la región de Consenso
terminen como una única gran cosa, y ahora deberían quedar claramente separadas. Espero, si todo sale
bien.

De paso, todavía no apliqué esto que mencioné ayer sobre que quizás un mejor criterio de polarización
sea 0,5 en vez de 1. HABLAR ESTO CON PABLO Y SEBA

Luego de hablar con Pablo y Seba quedamos en que sería una buena idea armar una neuva corrida de datos
haciendo zooom a la región de transición, cambiando un poco el criterio de corte, de manera de que

---------------------------------------------------------------------------------------------------------

23/09/2021

Hoy dejé pasar el día tranca, la verdad debería ponerme a hacer más cosas. Anotemos la lista de cosas
para hacer. Igual agradezco la sensación de no ser tan un fracaso, de estar bien hoy.

Por su puesto que tengo que seguir escribiendo la Tesis, eso queda para el sábado y domingo.
El viernes tengo que empezar a ver sobre la forma de ir graficando redes con gephi. Y lo otro
que tengo que hacer es mandar a correr datos de las regiones de transición. Para eso tengo que
ponerme a modificar el main de generación de Redes Estáticas. Pablo decía de hacer que el
sujeto mire a 20 pasos atrás y con eso me arme compare la variación. Si mantengo el mismo valor
de variación promedio como Criterio de Corte, pero cambio la cantidad de pasos en el pasado con
la que comparo, esto debería funcionar mejor. El tema es que eso me requiere tener un vector que
guarde datos de hace 20 pasos. Y que luego haga la diferencia como hasta ahora. Y tengo que
constantemente guardar esa info. Ok, habrá que ver de modificar el programa. Y quizás hacerlo como
un programa aparte.

Las carpetas donde guarde estos datos las debería llamar TransicionER, TransicionB y TransicionRR.
Podría diferenciar transicion del consenso con transición en región entre Descorrelacionada e
Ideológica. Ya fue, que sea Transicion1 y Transicion2. Y el tema de que para cada GM voy a
recorrer distintos alfas se me ocurre resolverlo mandando manualmente a correr las 4 cosas
y listo. Aunque atento con el tema de los archivos donde guardo el out. Hay que mirarlo bien esto.

-----------------------------------------------------------------------------------------------------------

26/09/2021

Armé la corrección para que el programa de Redes Estáticas corra comparando las opiniones del
sistema en el paso actual con lo visto en un paso previo 20 iteraciones atrás. Las cosas que
agregué son un array de opiniones, un int que lleva la cuenta del paso temporal con el que
hay que comparar y un free al final que se encarga de liberar todos los punteros. La idea de
ir ciclando entre las opiniones guardadas de pasos temporales previos es usar la función %
que se encarga de colocar como índice el valor que es el resto de la división.

También agregué unos pasos de "Termalización" que uso para guardar los primeros P pasos
tales que pueda empezar a comparar la evolución del sistema con esos pasos. Mañana lo mejor
sería ya poder probarlo, cosa del martes poder dedicarme a anotar cosas en la tesis y avanzar
con eso.

--------------------------------------------------------------------------------------------------------------

28/09/2021

Ya pasé el código que voy a usar para hacer Zoom en redes estáticas a la computadora de Algarve.
Así que ya estoy en condiciones de mandarlo a correr. El zoom lo voy a ahcer en la región de
transición. Cambié el criterio de corte para que el sistema corte comparando su estado actual con
su estado en un paso 20 iteraciones previas. Cada uno de los archivos creados va a una carpeta
correspondiente a su grado medio, es decir que para Erdos-Renyi tengo diferenciado los datos
obtenidos de las redes con GM=4, 8, 12 y 16 en 4 carpetas separadas.

La idea sería recorrer una región acotada donde se produce el cambio entre la zona de polarización
descorrelacionada, estado ideológico y consenso. Para eso voy a moverme en Cdelta entre -0,2 y 0,2
de a 0,01. (Por eso tengo el Cdelta divido 100). El alfa lo voy a mover entre -0,01 y 0,02 del alfa
crítico para Cdelta = 0. Me voy a mover de a 0,001 (Tengo que dividir el alfa por 1000).

Esto determina que para cada GM me tengo que mover en regiones de alfa diferentes. Esas regiones son:
.) GM=4: [0.24,0.27] (240,270)
.) GM=8: [0.135,0.155] (135,155)
.) GM=12: [0.073,0.103] (73, 103)
.) GM=16: [0.052,0.082] (52,82)

Ya mandé a correr en Algarve los programas, estoy consumiendo 20 hilos. Supongamos que en promedio
cada simulación tarda 90 segundos. Voy a tener 100 simulaciones, por cada uno de los 41 cos(delta)
por cada uno de los 6 alfas para cada una de las tres redes. 
Entonces eso significa que cada corrida tardará en promedio 76 días. Demasiado.

Hice las cuentas, pareciera que voy a tener que reducir el paso en el Cdelta y en la cantidad
de iteraciones, porque sino esto tardaría unos 75 días. No tengo ese tiempo, claramente.
Si reduzco el Cdelta a la mitad y las iteraciones también, ese tiempo se reduce en
4 veces, que sería 18 días. Es mucho, pero más aceptable. También puedo ayudar
a reducir esto usando las dos pcs, pero me da cosa ocuparme TODOS los hilos. Podría ocupar algunos hilos menos 
en Oporto, pero eso haría que divida las corridas de forma rara. No sé si me convence. Quizás podría
sino ocuparme de obtener datos en sólo 2 Gm primero, y de ahí si necesitamos más lo charlamos.

Maté todas las instanciaciones anteriores y lo volví a mandar esta vez para que genere los datos
más rápido. Reduje el recorrido de Cdelta a la mitad, ya no son 41 sino 21 Cdeltas. (Entre -0,2 y
0,2 de a 0,02). Reduje la cantidad de simulaciones por punto del ensamble a 50. Y como estoy sólo
armando datos para 2 de las redes, entonces ahora estoy usando la misma cantidad de hilos pero con
menor cantidad de alfas por cada hilo. Entonces esto ahora es 3 alfas por hilo. En conclusión,
cada corrida total va a tardar en promedio: 9,84 días. Esto es algo más aceptable. Después cualquier
cosa mando a correr los gm que faltan.

Mañana tengo dos cosas para hacer. Primero, revisar el tema de usar la varianza de las distribuciones de
opininiones como un criterio válido a la hora de determinar si el sistema se encuentra en consenso o en
polarización. También estaría bueno usar la correlación entre tópicos como forma de definir si está en
estado ideológico o polarización descorrelacionada. Luego de eso, continuar escribiendo la tesis.

------------------------------------------------------------------------------------------------------------------

29/09/2021

Acabo de modificar el armado del SuperDiccionario de forma tal que reduzca el tiempo que tarda en armarse.
Lo que hice fue saltear toda una sección del código que armaba el SuperDiccionario luego de haber recorrido
todos los archivos. Esa sección consumía mucho tiempo porque recorría TODA la lista de archivos montones de
veces y tenía muchos if en esos for. Lo reduje a recorrer una sola vez la lista de archivos y listo. Y comprobé
que el SuperDiccionario esté bien armado, así que eso soluciona el problema de que el código tardara 1 hora
en armar el SuperDiccionario. Ahora tardó sólo 100 segundos.

Ahora que terminé esto, conviene empezar con la parte de ver cómo varía la varianza de las opiniones para poder
usar eso como criterio de si el sistema está en consenso o polarización.

Bien, armé el código de esto, ahora sólo me queda ver que tal salen los gráficos. Dicho esto, quedaría lo de
mandar esto a Oporto y hacerlo correr y listo. Ya que estoy, voy a borrar los archivos no necesarios en Oporto
de Redes Estáticas, y subir todo a Github. Luego de eso, me pondré a preparar clases de Electro y después
seguiré con esto.